#!/usr/bin/env python3
"""
CodeLens åˆ›é€ æ¨¡å¼ - åˆ†æå®ç°å·¥å…· (create_analysis)
ç¬¬äºŒé˜¶æ®µï¼šåŸºäºæ¶æ„æ–‡æ¡£åˆ†æå®ç°æ–¹æ¡ˆå’Œå½±å“é“¾
"""

import sys
import json
import argparse
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
try:
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
    import os
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    sys.path.insert(0, project_root)
    from src.logging import get_logger
    from src.templates.document_templates import TemplateService
    from src.services.file_service import FileService
except ImportError:
    # å¦‚æœä¾èµ–ä¸å¯ç”¨ï¼Œåˆ›å»ºç©ºå®ç°
    class DummyLogger:
        def debug(self, msg, context=None): pass
        def info(self, msg, context=None): pass
        def warning(self, msg, context=None): pass
        def error(self, msg, context=None, exc_info=None): pass
        def log_operation_start(self, operation, *args, **kwargs): return "dummy_id"
        def log_operation_end(self, operation, operation_id, success=True, **kwargs): pass

    get_logger = lambda **kwargs: DummyLogger()
    
    class DummyTemplateService:
        def get_template_content(self, name): 
            return {"success": False, "error": "Template service not available"}
        def format_template(self, name, **kwargs):
            return {"success": False, "error": "Template service not available"}
            
    class DummyFileService:
        def __init__(self, *args, **kwargs): pass
        def scan_source_files(self, *args, **kwargs):
            return []
            
    TemplateService = DummyTemplateService
    FileService = DummyFileService

class CreateAnalysisCore:
    """åˆ›é€ æ¨¡å¼ç¬¬äºŒé˜¶æ®µï¼šåˆ†æå®ç°å·¥å…·"""
    
    def __init__(self, project_path: str):
        """åˆå§‹åŒ–"""
        self.logger = get_logger(component="CreateAnalysisCore", operation="init")
        self.project_path = Path(project_path).resolve()
        self.create_docs_path = self.project_path / "docs" / "project" / "create"
        self.requirements_dir = self.create_docs_path / "requirements"
        self.analysis_dir = self.create_docs_path / "analysis"
        self.architecture_docs_path = self.project_path / "docs" / "architecture"
        
        self.template_service = TemplateService()
        self.file_service = FileService()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info("CreateAnalysisCore åˆå§‹åŒ–", {
            "project_path": str(self.project_path),
            "analysis_dir": str(self.analysis_dir),
            "architecture_docs_path": str(self.architecture_docs_path)
        })
    
    def analyze_project_context(self) -> Dict[str, Any]:
        """åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œä¸ºAIåˆ†ææä¾›åŸºç¡€ä¿¡æ¯"""
        operation_id = self.logger.log_operation_start("analyze_project_context")
        
        try:
            context = {
                "project_name": self.project_path.name,
                "project_type": "Unknown",
                "tech_stack": [],
                "file_structure": {},
                "architecture_docs": [],
                "existing_services": [],
                "config_files": []
            }
            
            # æ‰«æé¡¹ç›®æ–‡ä»¶ç»“æ„
            try:
                scan_result = self.file_service.scan_source_files(str(self.project_path))
                context["file_structure"] = {"files": scan_result}
                
                # åˆ†ææŠ€æœ¯æ ˆ
                if scan_result:
                    tech_indicators = {
                        "Python": [".py", "requirements.txt", "setup.py", "pyproject.toml"],
                        "JavaScript/Node.js": ["package.json", ".js", ".ts", "node_modules"],
                        "Java": [".java", "pom.xml", "build.gradle"],
                        "Go": [".go", "go.mod", "go.sum"],
                        "Rust": [".rs", "Cargo.toml"],
                        "C/C++": [".c", ".cpp", ".h", "Makefile", "CMakeLists.txt"]
                    }
                    
                    for tech, indicators in tech_indicators.items():
                        for file_path in scan_result:
                            if any(indicator in file_path for indicator in indicators):
                                if tech not in context["tech_stack"]:
                                    context["tech_stack"].append(tech)
                                break
                
                # åˆ†æç°æœ‰æœåŠ¡å’Œæ¨¡å—
                for file_path in scan_result:
                    if "service" in file_path.lower() or "api" in file_path.lower():
                        context["existing_services"].append(file_path)
                        
            except Exception as e:
                self.logger.warning(f"Failed to scan project files: {e}")
            
            # æ£€æµ‹é¡¹ç›®ç±»å‹
            if "Python" in context["tech_stack"]:
                files = context["file_structure"].get("files", [])
                if any("flask" in str(f).lower() for f in files):
                    context["project_type"] = "Flask Web Application"
                elif any("django" in str(f).lower() for f in files):
                    context["project_type"] = "Django Web Application"
                elif any("fastapi" in str(f).lower() for f in files):
                    context["project_type"] = "FastAPI Application"
                else:
                    context["project_type"] = "Python Application"
            elif "JavaScript/Node.js" in context["tech_stack"]:
                context["project_type"] = "JavaScript/Node.js Application"
            elif context["tech_stack"]:
                context["project_type"] = f"{context['tech_stack'][0]} Application"
            
            # æ‰«ææ¶æ„æ–‡æ¡£
            if self.architecture_docs_path.exists():
                for arch_file in self.architecture_docs_path.glob("**/*.md"):
                    context["architecture_docs"].append(str(arch_file.relative_to(self.project_path)))
            
            self.logger.log_operation_end("analyze_project_context", operation_id, success=True)
            return context
            
        except Exception as e:
            self.logger.log_operation_end("analyze_project_context", operation_id, 
                success=False, error=str(e)
            )
            return {
                "project_name": self.project_path.name,
                "project_type": "Unknown",
                "tech_stack": [],
                "error": str(e)
            }
    
    def generate_ai_analysis_content(self, field_name: str, feature_name: str, context: Dict[str, Any], requirement_info: Dict[str, Any]) -> str:
        """åŸºäºé¡¹ç›®ä¸Šä¸‹æ–‡å’Œéœ€æ±‚ä¿¡æ¯ç”ŸæˆAIåˆ†æå†…å®¹"""
        project_type = context.get("project_type", "Application")
        tech_stack = context.get("tech_stack", [])
        existing_services = context.get("existing_services", [])
        architecture_docs = context.get("architecture_docs", [])
        
        if field_name == "architecture_impact":
            if "ç¼“å­˜" in feature_name:
                return f"å®ç°{feature_name}å°†åœ¨{project_type}ä¸­å¼•å…¥æ–°çš„ç¼“å­˜å±‚æ¶æ„ã€‚éœ€è¦è€ƒè™‘ç¼“å­˜ç­–ç•¥ã€æ•°æ®ä¸€è‡´æ€§ã€å¤±æ•ˆæœºåˆ¶ç­‰å…³é”®æ¶æ„å†³ç­–ã€‚å»ºè®®é‡‡ç”¨Redisæˆ–å†…å­˜ç¼“å­˜æ–¹æ¡ˆï¼Œç¡®ä¿ä¸ç°æœ‰æ•°æ®è®¿é—®å±‚çš„æ— ç¼é›†æˆã€‚"
            elif "è®¤è¯" in feature_name or "ç™»å½•" in feature_name:
                return f"{feature_name}å°†å½±å“{project_type}çš„å®‰å…¨æ¶æ„ã€‚éœ€è¦é›†æˆè®¤è¯ä¸­é—´ä»¶ã€ä¼šè¯ç®¡ç†ã€æƒé™æ§åˆ¶ç­‰ç»„ä»¶ã€‚å»ºè®®é‡‡ç”¨JWTæˆ–Session-basedè®¤è¯æœºåˆ¶ï¼Œç¡®ä¿ä¸ç°æœ‰APIç«¯ç‚¹çš„å®‰å…¨é›†æˆã€‚"
            elif "API" in feature_name:
                return f"{feature_name}å°†æ‰©å±•{project_type}çš„APIæ¶æ„ã€‚éœ€è¦è€ƒè™‘è·¯ç”±è®¾è®¡ã€è¯·æ±‚å¤„ç†ã€å“åº”æ ¼å¼ã€é”™è¯¯å¤„ç†ç­‰æ¶æ„å±‚é¢çš„å˜æ›´ã€‚å»ºè®®éµå¾ªRESTfulè®¾è®¡åŸåˆ™ï¼Œç¡®ä¿APIç‰ˆæœ¬å…¼å®¹æ€§ã€‚"
            else:
                return f"{feature_name}å°†å¯¹{project_type}äº§ç”Ÿæ¶æ„å±‚é¢çš„å½±å“ã€‚éœ€è¦åˆ†æç°æœ‰ç»„ä»¶ä¾èµ–å…³ç³»ï¼Œç¡®å®šæ–°å¢æ¨¡å—çš„æ¶æ„ä½ç½®ï¼Œè¯„ä¼°å¯¹æ•´ä½“ç³»ç»Ÿè®¾è®¡çš„å½±å“ã€‚"
                
        elif field_name == "existing_components":
            if existing_services:
                services_list = "ã€".join(existing_services[:3])
                return f"å½“å‰ç³»ç»ŸåŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š{services_list}ç­‰ã€‚{feature_name}éœ€è¦ä¸è¿™äº›ç°æœ‰ç»„ä»¶è¿›è¡Œé›†æˆï¼Œç¡®ä¿åŠŸèƒ½åè°ƒå’Œæ•°æ®ä¸€è‡´æ€§ã€‚"
            else:
                return f"éœ€è¦è¯¦ç»†åˆ†æ{project_type}çš„ç°æœ‰ç»„ä»¶ç»“æ„ï¼Œè¯†åˆ«ä¸{feature_name}ç›¸å…³çš„æ ¸å¿ƒæ¨¡å—ï¼Œè¯„ä¼°é›†æˆå¤æ‚åº¦ã€‚"
                
        elif field_name == "new_components":
            if "ç¼“å­˜" in feature_name:
                return f"éœ€è¦æ–°å¢ç¼“å­˜ç®¡ç†å™¨ã€ç¼“å­˜ç­–ç•¥é…ç½®ã€ç¼“å­˜ç›‘æ§ç­‰ç»„ä»¶ã€‚å»ºè®®åˆ›å»ºç‹¬ç«‹çš„ç¼“å­˜æœåŠ¡æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„ç¼“å­˜æ¥å£ã€‚"
            elif "è®¤è¯" in feature_name:
                return f"éœ€è¦æ–°å¢è®¤è¯æœåŠ¡ã€ä¼šè¯ç®¡ç†å™¨ã€æƒé™éªŒè¯ä¸­é—´ä»¶ç­‰ç»„ä»¶ã€‚å»ºè®®åˆ›å»ºç‹¬ç«‹çš„è®¤è¯æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„ç”¨æˆ·éªŒè¯æ¥å£ã€‚"
            else:
                return f"åŸºäº{feature_name}çš„åŠŸèƒ½éœ€æ±‚ï¼Œéœ€è¦æ–°å¢ç›¸åº”çš„æœåŠ¡ç»„ä»¶ã€æ•°æ®æ¨¡å‹ã€ä¸šåŠ¡é€»è¾‘å¤„ç†å™¨ç­‰æ ¸å¿ƒç»„ä»¶ã€‚"
                
        elif field_name == "data_structure_changes":
            if "ç¼“å­˜" in feature_name:
                return f"éœ€è¦æ–°å¢ç¼“å­˜é”®å€¼ç»“æ„è®¾è®¡ã€ç¼“å­˜æ•°æ®æ¨¡å‹ã€TTLé…ç½®ç­‰æ•°æ®ç»“æ„ã€‚å¯èƒ½éœ€è¦ä¿®æ”¹ç°æœ‰æ•°æ®è®¿é—®å±‚ä»¥æ”¯æŒç¼“å­˜æŸ¥è¯¢ã€‚"
            elif "è®¤è¯" in feature_name:
                return f"éœ€è¦æ–°å¢ç”¨æˆ·è®¤è¯è¡¨ã€ä¼šè¯å­˜å‚¨ç»“æ„ã€æƒé™é…ç½®è¡¨ç­‰ã€‚å¯èƒ½éœ€è¦ä¿®æ”¹ç°æœ‰ç”¨æˆ·ç›¸å…³æ•°æ®æ¨¡å‹ä»¥æ”¯æŒè®¤è¯åŠŸèƒ½ã€‚"
            else:
                return f"æ ¹æ®{feature_name}çš„ä¸šåŠ¡éœ€æ±‚ï¼Œéœ€è¦åˆ†æå¹¶è®¾è®¡ç›¸åº”çš„æ•°æ®ç»“æ„å˜æ›´ï¼ŒåŒ…æ‹¬æ–°å¢è¡¨/é›†åˆã€ä¿®æ”¹ç°æœ‰æ¨¡å‹ã€ç´¢å¼•ä¼˜åŒ–ç­‰ã€‚"
                
        elif field_name == "dependency_changes":
            if "Python" in tech_stack:
                if "ç¼“å­˜" in feature_name:
                    return "å¯èƒ½éœ€è¦æ–°å¢Rediså®¢æˆ·ç«¯åº“ï¼ˆå¦‚redis-pyï¼‰ã€å†…å­˜ç¼“å­˜åº“ç­‰ä¾èµ–ã€‚éœ€è¦æ›´æ–°requirements.txté…ç½®ã€‚"
                elif "è®¤è¯" in feature_name:
                    return "å¯èƒ½éœ€è¦æ–°å¢è®¤è¯åº“ï¼ˆå¦‚PyJWTã€Flask-Loginï¼‰ã€å¯†ç åŠ å¯†åº“ç­‰ä¾èµ–ã€‚éœ€è¦æ›´æ–°requirements.txté…ç½®ã€‚"
                else:
                    return f"æ ¹æ®{feature_name}çš„æŠ€æœ¯éœ€æ±‚ï¼Œå¯èƒ½éœ€è¦æ–°å¢ç›¸å…³Pythonåº“ä¾èµ–ï¼Œéœ€è¦è¯„ä¼°å¯¹ç°æœ‰ä¾èµ–æ ‘çš„å½±å“ã€‚"
            elif "JavaScript/Node.js" in tech_stack:
                return f"å¯èƒ½éœ€è¦æ–°å¢NPMåŒ…ä¾èµ–ï¼Œéœ€è¦æ›´æ–°package.jsoné…ç½®ï¼Œè¯„ä¼°ä¸ç°æœ‰Node.jsæ¨¡å—çš„å…¼å®¹æ€§ã€‚"
            else:
                return f"éœ€è¦åˆ†æ{feature_name}çš„æŠ€æœ¯ä¾èµ–éœ€æ±‚ï¼Œè¯„ä¼°æ–°å¢ä¾èµ–å¯¹ç°æœ‰æŠ€æœ¯æ ˆçš„å½±å“ã€‚"
                
        elif field_name == "interface_impact":
            if "API" in feature_name:
                return f"{feature_name}å°†æ–°å¢å¤šä¸ªAPIç«¯ç‚¹ï¼Œéœ€è¦ç¡®ä¿ä¸ç°æœ‰APIçš„ä¸€è‡´æ€§ã€‚å»ºè®®éµå¾ªç°æœ‰APIè®¾è®¡è§„èŒƒï¼Œä¿æŒå“åº”æ ¼å¼ç»Ÿä¸€ã€‚"
            elif "è®¤è¯" in feature_name:
                return f"{feature_name}å°†å½±å“æ‰€æœ‰éœ€è¦æƒé™æ§åˆ¶çš„APIæ¥å£ã€‚éœ€è¦åœ¨ç°æœ‰ç«¯ç‚¹ä¸­é›†æˆè®¤è¯ä¸­é—´ä»¶ï¼Œå¯èƒ½å¯¼è‡´æ¥å£è¡Œä¸ºå˜æ›´ã€‚"
            else:
                return f"{feature_name}å¯èƒ½å½±å“ç°æœ‰æ¥å£çš„è°ƒç”¨æ–¹å¼å’Œå“åº”æ ¼å¼ã€‚éœ€è¦è¯„ä¼°å¯¹å®¢æˆ·ç«¯å’Œå…¶ä»–æœåŠ¡è°ƒç”¨çš„å½±å“ã€‚"
                
        else:
            return f"åŸºäº{feature_name}çš„å…·ä½“éœ€æ±‚å’Œ{project_type}çš„æŠ€æœ¯ç‰¹ç‚¹ï¼Œéœ€è¦è¿›è¡Œè¯¦ç»†çš„æŠ€æœ¯åˆ†æå’Œè¯„ä¼°ã€‚"
        
    def generate_analysis_id(self, requirement_id: str) -> str:
        """ç”Ÿæˆåˆ†æID"""
        timestamp = int(time.time())
        return f"analysis_{requirement_id}_{timestamp}"
    
    def load_requirement_document(self, requirement_id: str) -> Dict[str, Any]:
        """åŠ è½½éœ€æ±‚æ–‡æ¡£"""
        operation_id = self.logger.log_operation_start("load_requirement_document", 
            requirement_id=requirement_id
        )
        
        try:
            req_file = self.requirements_dir / f"{requirement_id}.md"
            
            if not req_file.exists():
                return {
                    "success": False,
                    "error": f"Requirement document {requirement_id} not found"
                }
            
            # è¯»å–éœ€æ±‚æ–‡æ¡£å†…å®¹
            with open(req_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•è§£æéœ€æ±‚ä¿¡æ¯ï¼ˆå®é™…å®ç°ä¸­å¯ä»¥æ›´å¤æ‚ï¼‰
            requirement_info = {
                "requirement_id": requirement_id,
                "content": content,
                "file_path": str(req_file)
            }
            
            # å°è¯•ä»å†…å®¹ä¸­æå–åŠŸèƒ½åç§°
            lines = content.split('\n')
            feature_name = "æœªçŸ¥åŠŸèƒ½"
            for line in lines:
                if "åŠŸèƒ½åç§°" in line and "**" in line:
                    parts = line.split("**")
                    if len(parts) >= 3:
                        feature_name = parts[2].strip()
                        break
            
            requirement_info["feature_name"] = feature_name
            
            self.logger.log_operation_end("load_requirement_document", operation_id, 
                success=True, feature_name=feature_name
            )
            
            return {
                "success": True,
                "requirement_info": requirement_info
            }
            
        except Exception as e:
            self.logger.log_operation_end("load_requirement_document", operation_id, 
                success=False, error=str(e)
            )
            return {
                "success": False,
                "error": f"Failed to load requirement document: {str(e)}"
            }
    
    def detect_project_maturity(self) -> Dict[str, Any]:
        """æ£€æµ‹é¡¹ç›®æˆç†Ÿåº¦ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å®Œæ•´çš„æ¶æ„æ–‡æ¡£"""
        operation_id = self.logger.log_operation_start("detect_project_maturity")
        
        try:
            maturity_info = {
                "is_mature_project": False,
                "architecture_docs_exist": False,
                "architecture_docs_count": 0,
                "total_architecture_size": 0,
                "key_docs_found": [],
                "maturity_level": "new",  # new, developing, mature
                "analysis_strategy": "direct_design"  # direct_design, hybrid, architecture_based
            }
            
            # æ£€æŸ¥æ¶æ„æ–‡æ¡£ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.architecture_docs_path.exists():
                maturity_info["analysis_strategy"] = "direct_design"
                self.logger.log_operation_end("detect_project_maturity", operation_id, 
                    success=True, maturity_level="new"
                )
                return maturity_info
                
            maturity_info["architecture_docs_exist"] = True
            
            # æ‰«ææ¶æ„æ–‡æ¡£
            arch_files = list(self.architecture_docs_path.glob("*.md"))
            arch_files.extend(list(self.architecture_docs_path.glob("**/*.md")))
            
            maturity_info["architecture_docs_count"] = len(arch_files)
            
            # æ ¸å¿ƒæ¶æ„æ–‡æ¡£æ£€æŸ¥
            key_docs = ["overview.md", "tech-stack.md", "data-flow.md", "architecture.md", "design.md"]
            
            total_size = 0
            for arch_file in arch_files:
                file_size = arch_file.stat().st_size
                total_size += file_size
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®æ–‡æ¡£
                if arch_file.name.lower() in [doc.lower() for doc in key_docs]:
                    maturity_info["key_docs_found"].append({
                        "filename": arch_file.name,
                        "size": file_size,
                        "is_substantial": file_size > 1000  # å¤§äº1KBè®¤ä¸ºæ˜¯å®è´¨æ€§æ–‡æ¡£
                    })
            
            maturity_info["total_architecture_size"] = total_size
            
            # åˆ¤æ–­æˆç†Ÿåº¦çº§åˆ«
            substantial_key_docs = len([doc for doc in maturity_info["key_docs_found"] if doc["is_substantial"]])
            
            if maturity_info["architecture_docs_count"] == 0:
                maturity_info["maturity_level"] = "new"
                maturity_info["analysis_strategy"] = "direct_design"
            elif maturity_info["architecture_docs_count"] < 3 or substantial_key_docs == 0:
                maturity_info["maturity_level"] = "developing" 
                maturity_info["analysis_strategy"] = "hybrid"
            elif substantial_key_docs >= 2 and total_size > 5000:  # è‡³å°‘2ä¸ªå®è´¨æ€§å…³é”®æ–‡æ¡£ï¼Œæ€»å¤§å°>5KB
                maturity_info["maturity_level"] = "mature"
                maturity_info["analysis_strategy"] = "architecture_based"
                maturity_info["is_mature_project"] = True
            else:
                maturity_info["maturity_level"] = "developing"
                maturity_info["analysis_strategy"] = "hybrid"
            
            self.logger.log_operation_end("detect_project_maturity", operation_id, 
                success=True, 
                maturity_level=maturity_info["maturity_level"],
                docs_count=maturity_info["architecture_docs_count"],
                strategy=maturity_info["analysis_strategy"]
            )
            
            return maturity_info
            
        except Exception as e:
            self.logger.log_operation_end("detect_project_maturity", operation_id, 
                success=False, error=str(e)
            )
            # é»˜è®¤è¿”å›æ–°é¡¹ç›®çŠ¶æ€
            return {
                "is_mature_project": False,
                "architecture_docs_exist": False,
                "architecture_docs_count": 0,
                "total_architecture_size": 0,
                "key_docs_found": [],
                "maturity_level": "new",
                "analysis_strategy": "direct_design",
                "error_note": f"æ£€æµ‹å¤±è´¥: {str(e)}"
            }
    
    def analyze_architecture_impact_adaptive(self, requirement_info: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªé€‚åº”æ¶æ„å½±å“åˆ†æ - æ ¹æ®é¡¹ç›®æˆç†Ÿåº¦é‡‡ç”¨ä¸åŒç­–ç•¥"""
        operation_id = self.logger.log_operation_start("analyze_architecture_impact_adaptive", 
            requirement_id=requirement_info.get("requirement_id")
        )
        
        try:
            # æ£€æµ‹é¡¹ç›®æˆç†Ÿåº¦
            maturity_info = self.detect_project_maturity()
            strategy = maturity_info["analysis_strategy"]
            
            # è·å–é¡¹ç›®ä¸Šä¸‹æ–‡
            context = self.analyze_project_context()
            feature_name = requirement_info.get("feature_name", "")
            
            # åŸºç¡€åˆ†æç»“æ„
            architecture_analysis = {
                "project_maturity": maturity_info,
                "analysis_strategy": strategy,
                "architecture_docs_found": [],
                "architecture_impact": "",
                "existing_components": "", 
                "new_components": ""
            }
            
            if strategy == "direct_design":
                # æ–°é¡¹ç›®ç­–ç•¥ï¼šç›´æ¥åŸºäºéœ€æ±‚è®¾è®¡ï¼Œä¸åˆ†æç°æœ‰æ¶æ„
                architecture_analysis.update({
                    "architecture_impact": self.generate_direct_design_analysis(feature_name, requirement_info, "architecture_impact"),
                    "existing_components": "æ–°é¡¹ç›®æ— ç°æœ‰ç»„ä»¶ï¼Œå°†ä»é›¶å¼€å§‹è®¾è®¡",
                    "new_components": self.generate_direct_design_analysis(feature_name, requirement_info, "new_components")
                })
                
                self.logger.info("ä½¿ç”¨ç›´æ¥è®¾è®¡ç­–ç•¥", {
                    "strategy": "direct_design",
                    "reason": "é¡¹ç›®ç¼ºå°‘æ¶æ„æ–‡æ¡£ï¼Œé‡‡ç”¨ä»é›¶è®¾è®¡ç­–ç•¥"
                })
                
            elif strategy == "architecture_based":
                # æˆç†Ÿé¡¹ç›®ç­–ç•¥ï¼šåŸºäºç°æœ‰æ¶æ„æ–‡æ¡£è¿›è¡Œåˆ†æ
                architecture_analysis.update(self.analyze_existing_architecture(requirement_info, context))
                
                self.logger.info("ä½¿ç”¨æ¶æ„åˆ†æç­–ç•¥", {
                    "strategy": "architecture_based", 
                    "docs_count": maturity_info["architecture_docs_count"],
                    "key_docs": len(maturity_info["key_docs_found"])
                })
                
            else:  # hybrid strategy
                # æ··åˆç­–ç•¥ï¼šç»“åˆç°æœ‰æ–‡æ¡£å’Œç›´æ¥è®¾è®¡
                existing_analysis = self.analyze_existing_architecture(requirement_info, context)
                direct_analysis = {
                    "architecture_impact": self.generate_direct_design_analysis(feature_name, requirement_info, "architecture_impact"),
                    "new_components": self.generate_direct_design_analysis(feature_name, requirement_info, "new_components")
                }
                
                architecture_analysis.update({
                    "architecture_impact": f"åŸºäºæœ‰é™çš„æ¶æ„æ–‡æ¡£åˆ†æï¼š{existing_analysis['architecture_impact']}ã€‚è¡¥å……è®¾è®¡è€ƒè™‘ï¼š{direct_analysis['architecture_impact']}",
                    "existing_components": existing_analysis["existing_components"],
                    "new_components": f"ç»“åˆç°æœ‰æ¶æ„å’Œæ–°è®¾è®¡ï¼š{direct_analysis['new_components']}"
                })
                
                self.logger.info("ä½¿ç”¨æ··åˆåˆ†æç­–ç•¥", {
                    "strategy": "hybrid",
                    "reason": "é¡¹ç›®æœ‰éƒ¨åˆ†æ¶æ„æ–‡æ¡£ä½†ä¸å®Œæ•´"
                })
            
            self.logger.log_operation_end("analyze_architecture_impact_adaptive", operation_id, 
                success=True, 
                strategy=strategy,
                docs_found=len(architecture_analysis["architecture_docs_found"])
            )
            
            return architecture_analysis
            
        except Exception as e:
            self.logger.log_operation_end("analyze_architecture_impact_adaptive", operation_id, 
                success=False, error=str(e)
            )
            return {
                "project_maturity": {"maturity_level": "unknown", "analysis_strategy": "direct_design"},
                "analysis_strategy": "direct_design",
                "architecture_docs_found": [],
                "architecture_impact": "æ¶æ„å½±å“åˆ†æå¤±è´¥ï¼Œå»ºè®®åŸºäºéœ€æ±‚ç›´æ¥è®¾è®¡",
                "existing_components": "ç»„ä»¶åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š", 
                "new_components": "æ–°ç»„ä»¶è®¾è®¡å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "error_note": f"è‡ªé€‚åº”åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def generate_direct_design_analysis(self, feature_name: str, requirement_info: Dict[str, Any], field_type: str) -> str:
        """ä¸ºæ–°é¡¹ç›®ç”Ÿæˆç›´æ¥è®¾è®¡åˆ†æ"""
        user_description = requirement_info.get("user_description", "")
        
        if field_type == "architecture_impact":
            return f"æ–°é¡¹ç›®æ¶æ„è®¾è®¡ï¼š{feature_name}å°†ä½œä¸ºæ ¸å¿ƒåŠŸèƒ½æ¨¡å—å®ç°ã€‚å»ºè®®é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œç¡®ä¿åŠŸèƒ½çš„ç‹¬ç«‹æ€§å’Œå¯æ‰©å±•æ€§ã€‚åŸºäºéœ€æ±‚'{user_description[:100]}...'ï¼Œå»ºè®®é‡ç‚¹è€ƒè™‘åŠŸèƒ½çš„æ¥å£è®¾è®¡ã€æ•°æ®æµå‘å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚"
        elif field_type == "new_components":
            return f"æ–°é¡¹ç›®ç»„ä»¶è®¾è®¡ï¼šä¸ºå®ç°{feature_name}ï¼Œå»ºè®®åˆ›å»ºä»¥ä¸‹æ–°ç»„ä»¶ï¼š1) æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ç»„ä»¶ï¼›2) æ•°æ®å¤„ç†ç»„ä»¶ï¼›3) æ¥å£é€‚é…ç»„ä»¶ï¼›4) é…ç½®ç®¡ç†ç»„ä»¶ã€‚æ¯ä¸ªç»„ä»¶åº”æœ‰æ¸…æ™°çš„èŒè´£åˆ†å·¥å’Œæ¥å£å®šä¹‰ã€‚"
        else:
            return f"åŸºäºéœ€æ±‚ç›´æ¥è®¾è®¡{feature_name}ç›¸å…³çš„{field_type}ç»„ä»¶ã€‚"
    
    def analyze_existing_architecture(self, requirement_info: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç°æœ‰æ¶æ„ï¼ˆåŸæœ‰é€»è¾‘çš„é‡æ„ç‰ˆæœ¬ï¼‰"""
        feature_name = requirement_info.get("feature_name", "")
        
        analysis = {
            "architecture_docs_found": [],
            "architecture_impact": self.generate_ai_analysis_content("architecture_impact", feature_name, context, requirement_info),
            "existing_components": self.generate_ai_analysis_content("existing_components", feature_name, context, requirement_info),
            "new_components": self.generate_ai_analysis_content("new_components", feature_name, context, requirement_info)
        }
        
        # æ‰«ææ¶æ„æ–‡æ¡£ç›®å½•
        if self.architecture_docs_path.exists():
            arch_files = list(self.architecture_docs_path.glob("*.md"))
            arch_files.extend(list(self.architecture_docs_path.glob("**/*.md")))
            
            for arch_file in arch_files:
                analysis["architecture_docs_found"].append({
                    "filename": arch_file.name,
                    "path": str(arch_file.relative_to(self.project_path)),
                    "size": arch_file.stat().st_size
                })
        
        return analysis
    
    def analyze_architecture_impact(self, requirement_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ¶æ„å½±å“ï¼ˆä¿æŒå‘åå…¼å®¹çš„åŒ…è£…æ–¹æ³•ï¼‰"""
        # è°ƒç”¨æ–°çš„è‡ªé€‚åº”æ–¹æ³•
        return self.analyze_architecture_impact_adaptive(requirement_info)
    
    def analyze_code_impact(self, requirement_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æä»£ç å½±å“"""
        operation_id = self.logger.log_operation_start("analyze_code_impact", 
            requirement_id=requirement_info.get("requirement_id")
        )
        
        try:
            # æ‰«æé¡¹ç›®æ–‡ä»¶
            scan_result = self.file_service.scan_source_files(str(self.project_path))
            
            # è·å–é¡¹ç›®ä¸Šä¸‹æ–‡
            context = self.analyze_project_context()
            feature_name = requirement_info.get("feature_name", "")
            
            code_analysis = {
                "total_project_files": len(scan_result) if scan_result else 0,
                "main_files_to_modify": [],
                "core_functions_classes": [],
                "data_structure_changes": self.generate_ai_analysis_content("data_structure_changes", feature_name, context, requirement_info),
                "directly_affected_files": [],
                "indirectly_affected_files": [],
                "dependency_changes": self.generate_ai_analysis_content("dependency_changes", feature_name, context, requirement_info),
                "interface_impact": self.generate_ai_analysis_content("interface_impact", feature_name, context, requirement_info)
            }
            
            # åŸºäºåŠŸèƒ½åç§°æ¨æ–­å¯èƒ½å½±å“çš„æ–‡ä»¶ï¼ˆç®€åŒ–å®ç°ï¼‰
            feature_name_lower = feature_name.lower()
            all_files = scan_result if scan_result else []
            
            # åˆ†æä¸»è¦ä¿®æ”¹æ–‡ä»¶
            for file_path in all_files:
                if any(keyword in file_path.lower() for keyword in ["main", "core", "service", "api"]):
                    code_analysis["main_files_to_modify"].append(file_path)
            
            # åˆ†æç›´æ¥å½±å“çš„æ–‡ä»¶
            for file_info in all_files:
                file_path = file_info.get("relative_path", "")
                if any(keyword in feature_name for keyword in ["api", "service", "user", "data"] 
                       if keyword in file_path.lower()):
                    code_analysis["directly_affected_files"].append(file_path)
            
            # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            code_analysis["main_files_to_modify"] = code_analysis["main_files_to_modify"][:10]
            code_analysis["directly_affected_files"] = code_analysis["directly_affected_files"][:15]
            
            self.logger.log_operation_end("analyze_code_impact", operation_id, 
                success=True, files_analyzed=len(all_files)
            )
            
            return code_analysis
            
        except Exception as e:
            self.logger.log_operation_end("analyze_code_impact", operation_id, 
                success=False, error=str(e)
            )
            # è¿”å›åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µçš„é»˜è®¤å€¼
            return {
                "total_project_files": 0,
                "main_files_to_modify": "åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "core_functions_classes": "åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "data_structure_changes": "éœ€è¦åˆ†ææ•°æ®ç»“æ„å˜æ›´",
                "directly_affected_files": "åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "indirectly_affected_files": "åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "dependency_changes": "åˆ†æä¾èµ–å…³ç³»å˜æ›´",
                "interface_impact": "è¯„ä¼°æ¥å£å˜æ›´å½±å“",
                "error_note": f"ä»£ç å½±å“åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def generate_implementation_strategy(self, requirement_info: Dict[str, Any], 
                                       architecture_analysis: Dict[str, Any],
                                       code_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®ç°ç­–ç•¥"""
        operation_id = self.logger.log_operation_start("generate_implementation_strategy", 
            requirement_id=requirement_info.get("requirement_id")
        )
        
        try:
            # è·å–é¡¹ç›®ä¸Šä¸‹æ–‡
            context = self.analyze_project_context()
            feature_name = requirement_info.get("feature_name", "")
            project_type = context.get("project_type", "Application")
            tech_stack = context.get("tech_stack", [])
            
            # ç”ŸæˆAIé©±åŠ¨çš„å®ç°ç­–ç•¥
            if "ç¼“å­˜" in feature_name:
                tech_choices = f"åŸºäº{', '.join(tech_stack[:2]) if tech_stack else 'current tech stack'}å®ç°ç¼“å­˜å±‚ï¼Œæ¨èä½¿ç”¨Redisä½œä¸ºç¼“å­˜åç«¯ï¼Œå†…å­˜ç¼“å­˜ä½œä¸ºL1ç¼“å­˜"
                implementation_steps = [
                    "1. è®¾è®¡ç¼“å­˜é”®å€¼ç»“æ„å’ŒTTLç­–ç•¥",
                    "2. å®ç°ç¼“å­˜ç®¡ç†å™¨å’Œç¼“å­˜æ¥å£",
                    "3. é›†æˆç°æœ‰æ•°æ®è®¿é—®å±‚",
                    "4. å®ç°ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°æœºåˆ¶",
                    "5. æ·»åŠ ç¼“å­˜ç›‘æ§å’Œæ€§èƒ½æµ‹è¯•"
                ]
                key_points = ["ç¼“å­˜ä¸€è‡´æ€§ä¿è¯", "ç¼“å­˜ç©¿é€é˜²æŠ¤", "çƒ­ç‚¹æ•°æ®è¯†åˆ«", "å†…å­˜ä½¿ç”¨ä¼˜åŒ–"]
                risks = ["ç¼“å­˜é›ªå´©é£é™©", "æ•°æ®ä¸€è‡´æ€§æŒ‘æˆ˜", "å†…å­˜æ³„æ¼é£é™©", "ç¼“å­˜å‡»ç©¿é—®é¢˜"]
            elif "è®¤è¯" in feature_name or "ç™»å½•" in feature_name:
                tech_choices = f"åŸºäº{', '.join(tech_stack[:2]) if tech_stack else 'current tech stack'}å®ç°è®¤è¯ç³»ç»Ÿï¼Œæ¨èJWTæˆ–Session-basedè®¤è¯"
                implementation_steps = [
                    "1. è®¾è®¡ç”¨æˆ·è®¤è¯æ•°æ®æ¨¡å‹",
                    "2. å®ç°è®¤è¯æœåŠ¡å’Œä¸­é—´ä»¶",
                    "3. é›†æˆç°æœ‰APIç«¯ç‚¹å®‰å…¨æ§åˆ¶",
                    "4. å®ç°ä¼šè¯ç®¡ç†å’Œæƒé™æ§åˆ¶",
                    "5. æ·»åŠ å®‰å…¨æµ‹è¯•å’Œå®¡è®¡æ—¥å¿—"
                ]
                key_points = ["å¯†ç å®‰å…¨å­˜å‚¨", "ä¼šè¯ç®¡ç†ç­–ç•¥", "æƒé™æ§åˆ¶è®¾è®¡", "å®‰å…¨æ¼æ´é˜²æŠ¤"]
                risks = ["å®‰å…¨æ¼æ´é£é™©", "è®¤è¯æ€§èƒ½å½±å“", "ä¼šè¯å­˜å‚¨å‹åŠ›", "æƒé™è®¾è®¡å¤æ‚æ€§"]
            else:
                tech_choices = f"åŸºäºç°æœ‰{project_type}æŠ€æœ¯æ ˆå’Œ{', '.join(tech_stack[:2]) if tech_stack else 'current technologies'}è¿›è¡Œå®ç°"
                implementation_steps = [
                    "1. åˆ†æéœ€æ±‚å¹¶è®¾è®¡æŠ€æœ¯æ–¹æ¡ˆ",
                    "2. å®ç°æ ¸å¿ƒä¸šåŠ¡é€»è¾‘",
                    "3. é›†æˆç°æœ‰ç³»ç»Ÿç»„ä»¶",
                    "4. å®Œå–„æµ‹è¯•è¦†ç›–",
                    "5. éƒ¨ç½²éªŒè¯å’Œç›‘æ§"
                ]
                key_points = ["æ¶æ„ä¸€è‡´æ€§", "å‘åå…¼å®¹æ€§", "æ€§èƒ½ä¼˜åŒ–", "å¯ç»´æŠ¤æ€§"]
                risks = ["æŠ€æœ¯å®ç°å¤æ‚åº¦", "ç³»ç»Ÿé›†æˆæŒ‘æˆ˜", "æ€§èƒ½å½±å“è¯„ä¼°", "ç»´æŠ¤æˆæœ¬å¢åŠ "]
                
            strategy = {
                "technology_choices": tech_choices,
                "implementation_steps": implementation_steps,
                "key_technical_points": key_points,
                "potential_risks": risks,
                "unit_test_strategy": f"é’ˆå¯¹{feature_name}æ ¸å¿ƒåŠŸèƒ½ç¼–å†™å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿å…³é”®ä¸šåŠ¡é€»è¾‘æ­£ç¡®æ€§",
                "integration_test_strategy": "è®¾è®¡é›†æˆæµ‹è¯•éªŒè¯åŠŸèƒ½å®Œæ•´æ€§",
                "regression_test_strategy": "ç¡®ä¿ç°æœ‰åŠŸèƒ½ä¸å—å½±å“",
                "performance_impact": "è¯„ä¼°å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“",
                "optimization_suggestions": "æ€§èƒ½ä¼˜åŒ–å»ºè®®",
                "backward_compatibility": "ç¡®ä¿å‘åå…¼å®¹æ€§",
                "api_compatibility": "ä¿æŒAPIæ¥å£å…¼å®¹æ€§",
                "deployment_impact": "è¯„ä¼°éƒ¨ç½²ç›¸å…³å½±å“",
                "configuration_changes": "é…ç½®å˜æ›´è¯´æ˜",
                "data_migration": "æ•°æ®è¿ç§»æ–¹æ¡ˆ",
                "implementation_recommendations": "å®ç°å»ºè®®å’Œæœ€ä½³å®è·µ",
                "risk_mitigation": "é£é™©ç¼“è§£æªæ–½",
                "next_actions": "ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’",
                "pending_confirmations": "ç­‰å¾…ç¡®è®¤çš„å…³é”®å†³ç­–ç‚¹"
            }
            
            self.logger.log_operation_end("generate_implementation_strategy", operation_id, 
                success=True
            )
            
            return strategy
            
        except Exception as e:
            self.logger.log_operation_end("generate_implementation_strategy", operation_id, 
                success=False, error=str(e)
            )
            # è¿”å›åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µçš„é»˜è®¤å€¼
            return {
                "technology_choices": "æŠ€æœ¯é€‰å‹åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "implementation_steps": "å®ç°æ­¥éª¤åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "key_technical_points": "å…³é”®æŠ€æœ¯ç‚¹åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "potential_risks": "é£é™©è¯„ä¼°å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "unit_test_strategy": "å•å…ƒæµ‹è¯•ç­–ç•¥åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "integration_test_strategy": "é›†æˆæµ‹è¯•ç­–ç•¥åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "regression_test_strategy": "å›å½’æµ‹è¯•ç­–ç•¥åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "performance_impact": "æ€§èƒ½å½±å“è¯„ä¼°å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "optimization_suggestions": "ä¼˜åŒ–å»ºè®®åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "backward_compatibility": "å‘åå…¼å®¹æ€§åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "api_compatibility": "APIå…¼å®¹æ€§åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "deployment_impact": "éƒ¨ç½²å½±å“è¯„ä¼°å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "configuration_changes": "é…ç½®å˜æ›´åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "data_migration": "æ•°æ®è¿ç§»åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "implementation_recommendations": "å®ç°å»ºè®®åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "risk_mitigation": "é£é™©ç¼“è§£æªæ–½åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "next_actions": "ä¸‹ä¸€æ­¥è¡ŒåŠ¨åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "pending_confirmations": "å¾…ç¡®è®¤äº‹é¡¹åˆ†æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨ç¡®å®š",
                "error_note": f"å®ç°ç­–ç•¥åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def create_analysis_report(self, requirement_id: str, analysis_depth: str = "detailed", 
                             include_tests: bool = True) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
        operation_id = self.logger.log_operation_start("create_analysis_report", 
            requirement_id=requirement_id, analysis_depth=analysis_depth
        )
        
        try:
            # 1. åŠ è½½éœ€æ±‚æ–‡æ¡£
            requirement_result = self.load_requirement_document(requirement_id)
            if not requirement_result.get("success"):
                return requirement_result
            
            requirement_info = requirement_result["requirement_info"]
            
            # 2. åˆ†ææ¶æ„å½±å“
            architecture_analysis = self.analyze_architecture_impact(requirement_info)
            
            # 3. åˆ†æä»£ç å½±å“
            code_analysis = self.analyze_code_impact(requirement_info)
            
            # 4. ç”Ÿæˆå®ç°ç­–ç•¥
            implementation_strategy = self.generate_implementation_strategy(
                requirement_info, architecture_analysis, code_analysis
            )
            
            # 5. ç”Ÿæˆåˆ†æID
            analysis_id = self.generate_analysis_id(requirement_id)
            
            # 6. å‡†å¤‡æ¨¡æ¿æ•°æ®
            template_data = {
                "feature_name": requirement_info.get("feature_name"),
                "requirement_id": requirement_id,
                "analysis_id": analysis_id,
                "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "analyzer": "CodeLens åˆ›é€ æ¨¡å¼åˆ†æå·¥å…·",
                
                # æ¶æ„åˆ†ææ•°æ®
                **architecture_analysis,
                
                # ä»£ç åˆ†ææ•°æ®
                **code_analysis,
                
                # å®ç°ç­–ç•¥æ•°æ®
                **implementation_strategy
            }
            
            # å¤„ç†åˆ—è¡¨å­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for key, value in template_data.items():
                if isinstance(value, list):
                    if key.endswith('_files') or 'files' in key:
                        template_data[key] = '\n'.join(f"- {item}" for item in value)
                    else:
                        template_data[key] = '\n'.join(f"- {item}" for item in value)
            
            # 7. ç”Ÿæˆåˆ†ææ–‡æ¡£
            doc_result = self.generate_analysis_document(analysis_id, template_data)
            
            if doc_result.get("success", True):
                result = {
                    "tool": "create_analysis",
                    "mode": "create_report",
                    "analysis_id": analysis_id,
                    "requirement_id": requirement_id,
                    "status": "completed",
                    
                    "analysis_summary": {
                        "feature_name": requirement_info.get("feature_name"),
                        "analysis_depth": analysis_depth,
                        "include_tests": include_tests,
                        "total_files_analyzed": code_analysis.get("total_project_files", 0),
                        "architecture_docs_found": len(architecture_analysis.get("architecture_docs_found", []))
                    },
                    
                    "document_info": doc_result.get("document_info", {}),
                    
                    "next_stage": {
                        "stage": 3,
                        "tool": "create_todo",
                        "command": f"python src/mcp_tools/create_todo.py {self.project_path} --analysis-id {analysis_id}",
                        "description": "ç”Ÿæˆè¯¦ç»†å®ç°è®¡åˆ’",
                        "note": "è¯·å…ˆç¡®è®¤åˆ†æç»“æœå†è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"
                    },
                    
                    "timestamp": datetime.now().isoformat()
                }
            else:
                result = doc_result
            
            self.logger.log_operation_end("create_analysis_report", operation_id, 
                success=True, analysis_id=analysis_id
            )
            
            return result
            
        except Exception as e:
            self.logger.log_operation_end("create_analysis_report", operation_id, 
                success=False, error=str(e)
            )
            return {
                "success": False,
                "error": f"Failed to create analysis report: {str(e)}"
            }
    
    def generate_analysis_document(self, analysis_id: str, template_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ–‡æ¡£"""
        operation_id = self.logger.log_operation_start("generate_analysis_document", 
            analysis_id=analysis_id
        )
        
        try:
            # è·å–åˆ†ææ¨¡æ¿
            template_result = self.template_service.get_template_content("create_analysis")
            if not template_result.get("success", True):
                return {
                    "success": False,
                    "error": f"Failed to get analysis template: {template_result.get('error', 'Unknown error')}"
                }
            
            # æ ¼å¼åŒ–æ¨¡æ¿
            format_result = self.template_service.format_template("create_analysis", **template_data)
            if not format_result.get("success", True):
                return {
                    "success": False,
                    "error": f"Failed to format analysis template: {format_result.get('error', 'Unknown error')}"
                }
            
            # ç”Ÿæˆæ–‡æ¡£æ–‡ä»¶
            doc_filename = f"{analysis_id}.md"
            doc_filepath = self.analysis_dir / doc_filename
            
            # å†™å…¥æ–‡æ¡£
            with open(doc_filepath, 'w', encoding='utf-8') as f:
                f.write(format_result["formatted_content"])
            
            result = {
                "success": True,
                "document_info": {
                    "filename": doc_filename,
                    "filepath": str(doc_filepath),
                    "file_size": doc_filepath.stat().st_size,
                    "created_time": datetime.now().isoformat()
                }
            }
            
            self.logger.log_operation_end("generate_analysis_document", operation_id, 
                success=True, filepath=str(doc_filepath)
            )
            
            return result
            
        except Exception as e:
            self.logger.log_operation_end("generate_analysis_document", operation_id, 
                success=False, error=str(e)
            )
            return {
                "success": False,
                "error": f"Failed to generate analysis document: {str(e)}"
            }
    
    def list_analysis_reports(self) -> Dict[str, Any]:
        """åˆ—å‡ºåˆ†ææŠ¥å‘Š"""
        operation_id = self.logger.log_operation_start("list_analysis_reports")
        
        try:
            reports = []
            
            if self.analysis_dir.exists():
                for analysis_file in self.analysis_dir.glob("*.md"):
                    file_stat = analysis_file.stat()
                    reports.append({
                        "filename": analysis_file.name,
                        "analysis_id": analysis_file.stem,
                        "filepath": str(analysis_file),
                        "created_time": datetime.fromtimestamp(file_stat.st_ctime).isoformat(),
                        "modified_time": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                        "file_size": file_stat.st_size
                    })
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            reports.sort(key=lambda x: x["created_time"], reverse=True)
            
            result = {
                "tool": "create_analysis",
                "mode": "list_reports",
                "total_count": len(reports),
                "analysis_reports": reports,
                "analysis_dir": str(self.analysis_dir),
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.log_operation_end("list_analysis_reports", operation_id, 
                success=True, count=len(reports)
            )
            
            return result
            
        except Exception as e:
            self.logger.log_operation_end("list_analysis_reports", operation_id, 
                success=False, error=str(e)
            )
            return {
                "success": False,
                "error": f"Failed to list analysis reports: {str(e)}"
            }

class CreateAnalysisTool:
    """åˆ›é€ æ¨¡å¼åˆ†æå®ç°å·¥å…· - MCPæ¥å£"""
    
    def __init__(self):
        self.tool_name = "create_analysis"
        
    def get_tool_definition(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å®šä¹‰"""
        return {
            "name": self.tool_name,
            "description": "ğŸ” CodeLensåˆ›é€ æ¨¡å¼ç¬¬äºŒé˜¶æ®µ - åŸºäºæ¶æ„æ–‡æ¡£åˆ†æå®ç°æ–¹æ¡ˆå’Œå½±å“é“¾",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "project_path": {
                        "type": "string",
                        "description": "é¡¹ç›®æ ¹ç›®å½•è·¯å¾„",
                        "default": "."
                    },
                    "mode": {
                        "type": "string",
                        "enum": ["create", "list"],
                        "description": "æ‰§è¡Œæ¨¡å¼: create=åˆ›å»ºåˆ†ææŠ¥å‘Š, list=åˆ—å‡ºåˆ†ææŠ¥å‘Š",
                        "default": "create"
                    },
                    "requirement_id": {
                        "type": "string",
                        "description": "éœ€æ±‚æ–‡æ¡£ID (createæ¨¡å¼å¿…éœ€)"
                    },
                    "analysis_depth": {
                        "type": "string",
                        "enum": ["basic", "detailed", "comprehensive"],
                        "description": "åˆ†ææ·±åº¦",
                        "default": "detailed"
                    },
                    "include_tests": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«æµ‹è¯•åˆ†æ",
                        "default": True
                    }
                },
                "required": ["project_path"]
            }
        }
    
    def execute(self, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·"""
        try:
            project_path = arguments.get("project_path", ".")
            mode = arguments.get("mode", "create")
            requirement_id = arguments.get("requirement_id")
            analysis_depth = arguments.get("analysis_depth", "detailed")
            include_tests = arguments.get("include_tests", True)
            
            # åˆå§‹åŒ–åˆ†æå·¥å…·
            analysis_tool = CreateAnalysisCore(project_path)
            
            if mode == "list":
                # åˆ—å‡ºåˆ†ææŠ¥å‘Š
                result = analysis_tool.list_analysis_reports()
            elif mode == "create" and requirement_id:
                # åˆ›å»ºåˆ†ææŠ¥å‘Š
                result = analysis_tool.create_analysis_report(
                    requirement_id, analysis_depth, include_tests
                )
            else:
                result = {
                    "success": False,
                    "error": "è¯·æä¾›å¿…éœ€çš„å‚æ•°ï¼šcreateæ¨¡å¼éœ€è¦requirement_id"
                }
            
            return {
                "success": True,
                "data": result
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"CreateAnalysis tool execution failed: {str(e)}"
            }

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="CodeLens åˆ›é€ æ¨¡å¼ - åˆ†æå®ç°å·¥å…·")
    parser.add_argument("project_path", nargs="?", default=".", 
                        help="é¡¹ç›®è·¯å¾„ (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--requirement-id", required=False,
                        help="éœ€æ±‚æ–‡æ¡£ID")
    parser.add_argument("--analysis-depth", choices=["basic", "detailed", "comprehensive"], 
                        default="detailed", help="åˆ†ææ·±åº¦ (é»˜è®¤: detailed)")
    parser.add_argument("--include-tests", action="store_true", default=True,
                        help="æ˜¯å¦åŒ…å«æµ‹è¯•åˆ†æ (é»˜è®¤: True)")
    parser.add_argument("--list", action="store_true",
                        help="åˆ—å‡ºç°æœ‰åˆ†ææŠ¥å‘Š")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–åˆ†æå·¥å…·
        analysis_tool = CreateAnalysisCore(args.project_path)
        
        if args.list:
            # åˆ—å‡ºåˆ†ææŠ¥å‘Š
            result = analysis_tool.list_analysis_reports()
        elif args.requirement_id:
            # åˆ›å»ºåˆ†ææŠ¥å‘Š
            result = analysis_tool.create_analysis_report(
                args.requirement_id, args.analysis_depth, args.include_tests
            )
        else:
            # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            result = {
                "tool": "create_analysis",
                "mode": "help",
                "message": "è¯·æä¾› --requirement-id å‚æ•°æˆ–ä½¿ç”¨ --list æŸ¥çœ‹ç°æœ‰åˆ†ææŠ¥å‘Š",
                "usage_examples": [
                    f"python {sys.argv[0]} /path/to/project --requirement-id req_login_1234567890",
                    f"python {sys.argv[0]} /path/to/project --list",
                    f"python {sys.argv[0]} /path/to/project --requirement-id req_login_1234567890 --analysis-depth comprehensive"
                ]
            }
        
        # è¾“å‡ºç»“æœ
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "tool": "create_analysis"
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()