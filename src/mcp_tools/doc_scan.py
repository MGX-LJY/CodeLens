"""
MCP doc_scan å·¥å…·å®ç°
æ‰«æé¡¹ç›®æ–‡ä»¶å¹¶è¿”å›ç»“æ„åŒ–ä¿¡æ¯ä¾›Claude Codeä½¿ç”¨
"""
import sys
import os
import json
import time
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°pathä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

from src.services.file_service import FileService
from src.logging import get_logger

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from src.config import get_tool_config
    HAS_CONFIG_MANAGER = True
except ImportError:
    HAS_CONFIG_MANAGER = False
    get_tool_config = lambda x: {}


class DocScanTool:
    """MCP doc_scan å·¥å…·ç±» - ä¸ºClaude Codeæä¾›é¡¹ç›®æ–‡ä»¶ä¿¡æ¯"""

    def __init__(self):
        self.tool_name = "doc_scan"
        self.description = "æ‰«æé¡¹ç›®æ–‡ä»¶å¹¶è¿”å›ç»“æ„åŒ–ä¿¡æ¯ä¾›Claude Codeä½¿ç”¨"
        self.file_service = FileService()
        self.logger = get_logger(component="DocScanTool", operation="init")
        self.logger.info("DocScanTool åˆå§‹åŒ–å®Œæˆ")

    def _get_include_extensions(self):
        """ä»é…ç½®æ–‡ä»¶è·å–è¦åŒ…å«çš„æ–‡ä»¶æ‰©å±•å"""
        try:
            config_path = Path(__file__).parent.parent / "config" / "default_config.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                return config.get("file_filtering", {}).get("include_extensions", [".py"])
        except Exception as e:
            self.logger.warning(f"æ— æ³•è¯»å–é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ‰©å±•å: {e}")
        return [".py"]

    def get_tool_definition(self) -> Dict[str, Any]:
        """è·å–MCPå·¥å…·å®šä¹‰"""
        return {
            "name": self.tool_name,
            "description": self.description,
            "inputSchema": {
                "type": "object",
                "properties": {
                    "project_path": {
                        "type": "string",
                        "description": "è¦æ‰«æçš„é¡¹ç›®è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰"
                    },
                    "include_content": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«æ–‡ä»¶å†…å®¹"
                    },
                    "config": {
                        "type": "object",
                        "properties": {
                            "file_extensions": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "è¦åˆ†æçš„æ–‡ä»¶æ‰©å±•å"
                            },
                            "exclude_patterns": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "æ’é™¤çš„ç›®å½•æˆ–æ–‡ä»¶æ¨¡å¼"
                            },
                            "max_file_size": {
                                "type": "number",
                                "description": "å•ä¸ªæ–‡ä»¶æœ€å¤§å­—ç¬¦æ•°é™åˆ¶"
                            },
                            "max_depth": {
                                "type": "number",
                                "description": "ç›®å½•æ ‘æ‰«ææœ€å¤§æ·±åº¦"
                            },
                            "ignore_patterns": {
                                "type": "object",
                                "properties": {
                                    "files": {
                                        "type": "array",
                                        "items": {"type": "string"},
                                        "description": "å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼"
                                    },
                                    "directories": {
                                        "type": "array",
                                        "items": {"type": "string"},
                                        "description": "å¿½ç•¥çš„ç›®å½•æ¨¡å¼"
                                    },
                                    "content_based": {
                                        "type": "array",
                                        "items": {"type": "string"},
                                        "description": "åŸºäºå†…å®¹çš„è¿‡æ»¤è§„åˆ™"
                                    }
                                }
                            },
                            "smart_filtering": {
                                "type": "object",
                                "properties": {
                                    "enabled": {
                                        "type": "boolean",
                                        "description": "å¯ç”¨æ™ºèƒ½è¿‡æ»¤"
                                    },
                                    "project_type": {
                                        "type": "string",
                                        "enum": ["auto_detect", "python", "javascript", "java", "go", "rust"],
                                        "description": "é¡¹ç›®ç±»å‹"
                                    },
                                    "keep_config_files": {
                                        "type": "boolean",
                                        "description": "ä¿ç•™é…ç½®æ–‡ä»¶"
                                    },
                                    "keep_test_files": {
                                        "type": "boolean",
                                        "description": "ä¿ç•™æµ‹è¯•æ–‡ä»¶"
                                    }
                                }
                            },
                            "analysis_focus": {
                                "type": "object",
                                "properties": {
                                    "main_source_only": {
                                        "type": "boolean",
                                        "description": "ä»…åˆ†æä¸»è¦æºä»£ç æ–‡ä»¶"
                                    },
                                    "exclude_examples": {
                                        "type": "boolean",
                                        "description": "æ’é™¤ç¤ºä¾‹æ–‡ä»¶"
                                    },
                                    "exclude_migrations": {
                                        "type": "boolean",
                                        "description": "æ’é™¤æ•°æ®åº“è¿ç§»æ–‡ä»¶"
                                    }
                                }
                            }
                        }
                    }
                },
                "required": []
            }
        }

    def execute(self, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œdoc_scanå·¥å…·"""
        operation_id = self.logger.log_operation_start("execute_doc_scan", 
                                                       project_path=arguments.get("project_path"),
                                                       include_content=arguments.get("include_content", False),
                                                       max_depth=arguments.get("config", {}).get("max_depth", 3))
        start_time = time.time()
        
        self.logger.info("å¼€å§‹doc_scanæ“ä½œ", {
            "arguments": arguments, 
            "operation_id": operation_id
        })

        try:
            # å‚æ•°éªŒè¯ - å¦‚æœæ²¡æœ‰æä¾›project_pathï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
            project_path = arguments.get("project_path")
            if not project_path:
                project_path = os.getcwd()
                self.logger.info("æœªæä¾›project_pathï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•", {
                    "current_working_directory": project_path
                })
            
            self.logger.debug("éªŒè¯project_pathå‚æ•°", {"project_path": project_path})

            if not os.path.exists(project_path):
                error_msg = f"Project path does not exist: {project_path}"
                self.logger.error(error_msg)
                return self._error_response(error_msg)

            if not os.path.isdir(project_path):
                error_msg = f"Project path is not a directory: {project_path}"
                self.logger.error(error_msg)
                return self._error_response(error_msg)

            # è®°å½•å¼€å§‹æ‰«æ
            self.logger.info("å¼€å§‹æ‰«æé¡¹ç›®", {"project_path": project_path})

            # è·å–å‚æ•°
            include_content = arguments.get("include_content", True)
            config = arguments.get("config", {})
            
            self.logger.debug("æ‰«æé…ç½®å‚æ•°", {
                "include_content": include_content,
                "config": config
            })

            # æå–é…ç½®å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä»default_config.jsonè¯»å–
            file_extensions = config.get("file_extensions")
            if not file_extensions:
                file_extensions = self._get_include_extensions()
            exclude_patterns = config.get("exclude_patterns",
                                          ["__pycache__", ".git", "node_modules", ".idea", ".vscode"])
            max_file_size = config.get("max_file_size", 122880)
            max_depth = config.get("max_depth", 3)

            # æå–æ–°çš„è¿‡æ»¤é…ç½®
            ignore_patterns = config.get("ignore_patterns", {})
            smart_filtering = config.get("smart_filtering", {})
            analysis_focus = config.get("analysis_focus", {})

            # åº”ç”¨æ™ºèƒ½è¿‡æ»¤é€»è¾‘
            self.logger.debug("å¼€å§‹åº”ç”¨æ™ºèƒ½è¿‡æ»¤é€»è¾‘")
            enhanced_exclude_patterns = self._apply_smart_filtering(
                project_path, exclude_patterns, ignore_patterns, smart_filtering, analysis_focus
            )
            self.logger.debug("æ™ºèƒ½è¿‡æ»¤é€»è¾‘åº”ç”¨å®Œæˆ", {
                "original_patterns_count": len(exclude_patterns),
                "enhanced_patterns_count": len(enhanced_exclude_patterns)
            })

            # ğŸ”§ æ™ºèƒ½æ‰«æï¼šä¼˜å…ˆä½¿ç”¨ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨ï¼Œé¿å…å…¨é‡æ‰«æ
            task_files = self._get_target_files_from_tasks(project_path)
            if task_files:
                self.logger.info(f"å‘ç°ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨ï¼Œä½¿ç”¨ç²¾å‡†æ‰«ææ¨¡å¼ ({len(task_files)}ä¸ªæ–‡ä»¶)")
                project_info = self._scan_targeted_files(
                    project_path, task_files, include_content, max_file_size
                )
            else:
                self.logger.info("æœªå‘ç°ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå…¨é‡æ‰«ææ¨¡å¼")
                project_info = self.file_service.get_project_files_info(
                    project_path=project_path,
                    include_content=include_content,
                    extensions=file_extensions,
                    exclude_patterns=enhanced_exclude_patterns,
                    max_file_size=max_file_size
                )
            
            self.logger.info("FileServiceé¡¹ç›®ä¿¡æ¯è·å–å®Œæˆ", {
                "files_found": project_info.get('statistics', {}).get('total_files', 0),
                "total_size": project_info.get('statistics', {}).get('total_size', 0)
            })

            # åº”ç”¨å†…å®¹è¿‡æ»¤
            if smart_filtering.get("enabled", True):
                self.logger.debug("å¼€å§‹åº”ç”¨å†…å®¹è¿‡æ»¤")
                original_file_count = len(project_info.get('files', []))
                project_info = self._apply_content_filtering(project_info, ignore_patterns, analysis_focus)
                filtered_file_count = len(project_info.get('files', []))
                self.logger.debug("å†…å®¹è¿‡æ»¤å®Œæˆ", {
                    "original_files": original_file_count,
                    "filtered_files": filtered_file_count,
                    "filtered_out": original_file_count - filtered_file_count
                })

            # è·å–ç›®å½•æ ‘ï¼ˆä½¿ç”¨æŒ‡å®šçš„æ·±åº¦ï¼‰
            self.logger.debug("å¼€å§‹è·å–ç›®å½•æ ‘", {"max_depth": max_depth})
            directory_tree = self.file_service.get_directory_tree(project_path, max_depth)
            self.logger.debug("ç›®å½•æ ‘è·å–å®Œæˆ")

            # æ›´æ–°directory_treeåˆ°project_infoä¸­
            project_info['directory_tree'] = directory_tree

            # ğŸ”§ è¾“å‡ºä¼˜åŒ–ï¼šé™åˆ¶æ˜¾ç¤ºæ–‡ä»¶æ•°é‡ï¼Œå®Œæ•´ç»“æœä¿å­˜åˆ°æ–‡ä»¶
            display_limit = config.get("display_limit", 10)  # é»˜è®¤æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
            full_scan_result = project_info.copy()  # å®Œæ•´ç»“æœå¤‡ä»½
            
            if "files" in project_info and len(project_info["files"]) > display_limit:
                # ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶
                full_result_path = os.path.join(project_path, ".codelens", "full_scan_result.json")
                try:
                    os.makedirs(os.path.dirname(full_result_path), exist_ok=True)
                    with open(full_result_path, 'w', encoding='utf-8') as f:
                        json.dump(full_scan_result, f, indent=2, ensure_ascii=False)
                    self.logger.info(f"å®Œæ•´æ‰«æç»“æœå·²ä¿å­˜åˆ°: {full_result_path}")
                except Exception as e:
                    self.logger.warning(f"ä¿å­˜å®Œæ•´ç»“æœå¤±è´¥: {e}")
                
                # é™åˆ¶è¾“å‡ºæ˜¾ç¤ºçš„æ–‡ä»¶
                project_info["files"] = project_info["files"][:display_limit]
                project_info["display_info"] = {
                    "displayed_files": display_limit,
                    "total_files": full_scan_result['statistics']['total_files'],
                    "full_result_saved": full_result_path if os.path.exists(full_result_path) else None
                }

            # è®°å½•æˆåŠŸå®Œæˆ
            duration_ms = (time.time() - start_time) * 1000
            summary = {
                "total_files": full_scan_result['statistics']['total_files'],
                "displayed_files": len(project_info.get("files", [])),
                "total_size": full_scan_result['statistics']['total_size'],
                "file_types": full_scan_result['statistics']['file_types']
            }

            self.logger.log_operation_end("execute_doc_scan", operation_id, duration_ms, True,
                                        project_path=project_path,
                                        total_files=summary['total_files'],
                                        displayed_files=summary['displayed_files'],
                                        total_size=summary['total_size'],
                                        file_types_count=len(summary['file_types']))

            return self._success_response({
                "scan_result": project_info,
                "scan_config": {
                    "project_path": project_path,
                    "include_content": include_content,
                    "file_extensions": file_extensions,
                    "exclude_patterns": exclude_patterns,
                    "max_file_size": max_file_size,
                    "max_depth": max_depth,
                    "display_limit": display_limit
                },
                "message": f"æˆåŠŸæ‰«æé¡¹ç›®ï¼š{project_path} (æ˜¾ç¤º{summary['displayed_files']}/{summary['total_files']}ä¸ªæ–‡ä»¶)",
                "summary": summary
            })

        except Exception as e:
            # è®°å½•å¤±è´¥å®Œæˆ
            duration_ms = (time.time() - start_time) * 1000
            self.logger.log_operation_end("execute_doc_scan", operation_id, duration_ms, False, error=str(e))
            self.logger.error(f"é¡¹ç›®æ‰«æå¤±è´¥: {arguments.get('project_path')}, "
                              f"ç”¨æ—¶: {duration_ms}ms, é”™è¯¯: {str(e)}", exc_info=e)

            return self._error_response(f"æ‰«æå¤±è´¥: {str(e)}")

    def _get_target_files_from_tasks(self, project_path: str) -> list:
        """ä».codelens/tasks.jsonè¯»å–target_fileåˆ—è¡¨"""
        tasks_file = os.path.join(project_path, ".codelens", "tasks.json")
        if not os.path.exists(tasks_file):
            self.logger.debug("tasks.jsonæ–‡ä»¶ä¸å­˜åœ¨", {"path": tasks_file})
            return []
        
        try:
            with open(tasks_file, 'r', encoding='utf-8') as f:
                tasks_data = json.load(f)
            
            target_files = []
            for task_id, task_info in tasks_data.items():
                target_file = task_info.get("target_file")
                if target_file and target_file not in target_files:
                    target_files.append(target_file)
            
            self.logger.info(f"ä»tasks.jsonæå–target_fileåˆ—è¡¨", {
                "tasks_count": len(tasks_data),
                "target_files_count": len(target_files)
            })
            return target_files
            
        except Exception as e:
            self.logger.error(f"è¯»å–tasks.jsonå¤±è´¥: {e}")
            return []

    def _scan_targeted_files(self, project_path: str, target_files: list, 
                           include_content: bool, max_file_size: int) -> Dict[str, Any]:
        """ç²¾å‡†æ‰«ææŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨"""
        self.logger.info(f"å¼€å§‹ç²¾å‡†æ‰«æ {len(target_files)} ä¸ªæŒ‡å®šæ–‡ä»¶")
        
        # è·å–é¡¹ç›®åŸºç¡€ä¿¡æ¯
        project_info = self.file_service.get_project_info(project_path)
        
        # æ„å»ºæ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        files_info = []
        processed_files = 0
        skipped_files = 0
        
        for target_file in target_files:
            file_path = os.path.join(project_path, target_file)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                self.logger.warning(f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
                skipped_files += 1
                continue
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                'path': file_path,
                'relative_path': target_file
            }
            
            # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
            metadata = self.file_service.get_file_metadata(file_path)
            if metadata:
                file_info.update(metadata)
            
            # æ·»åŠ æ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if include_content:
                content = self.file_service.read_file_safe(file_path, max_file_size)
                file_info['content'] = content
                file_info['content_available'] = content is not None
            
            files_info.append(file_info)
            processed_files += 1
        
        # è·å–ç›®å½•æ ‘ï¼ˆç®€åŒ–ç‰ˆï¼Œé™ä½tokenæ¶ˆè€—ï¼‰
        directory_tree = self.file_service.get_directory_tree(project_path, max_depth=2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        statistics = {
            'total_files': len(files_info),
            'processed_files': processed_files,
            'skipped_files': skipped_files,
            'total_size': sum(f.get('size', 0) for f in files_info),
            'file_types': {},
            'scan_mode': 'targeted'
        }
        
        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
        for file_info in files_info:
            ext = file_info.get('extension', 'no_extension')
            statistics['file_types'][ext] = statistics['file_types'].get(ext, 0) + 1
        
        result = {
            'project_info': project_info,
            'files': files_info,
            'directory_tree': directory_tree,
            'statistics': statistics
        }
        
        self.logger.info("ç²¾å‡†æ‰«æå®Œæˆ", {
            "processed": processed_files,
            "skipped": skipped_files,
            "total_size": statistics['total_size']
        })
        
        return result

    def _success_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æˆåŠŸå“åº”"""
        self.logger.debug("ç”ŸæˆæˆåŠŸå“åº”", {"data_keys": list(data.keys()) if data else []})
        return {
            "success": True,
            "tool": self.tool_name,
            "data": data
        }

    def _apply_smart_filtering(self, project_path: str, base_exclude_patterns: list,
                               ignore_patterns: Dict[str, Any], smart_filtering: Dict[str, Any],
                               analysis_focus: Dict[str, Any]) -> list:
        """åº”ç”¨æ™ºèƒ½è¿‡æ»¤é€»è¾‘"""
        self.logger.debug("å¼€å§‹æ™ºèƒ½è¿‡æ»¤å¤„ç†", {
            "base_patterns_count": len(base_exclude_patterns),
            "smart_filtering_enabled": smart_filtering.get("enabled", True)
        })
        
        enhanced_patterns = base_exclude_patterns.copy()

        # æ·»åŠ åŸºæœ¬å¿½ç•¥æ¨¡å¼
        ignore_dirs = ignore_patterns.get("directories", ["__pycache__", ".git", "node_modules", ".idea", "venv"])

        enhanced_patterns.extend(ignore_dirs)

        # é¡¹ç›®ç±»å‹ç‰¹å®šè¿‡æ»¤
        project_type = smart_filtering.get("project_type", "auto_detect")
        if project_type == "auto_detect":
            self.logger.debug("è‡ªåŠ¨æ£€æµ‹é¡¹ç›®ç±»å‹")
            project_type = self._detect_project_type(project_path)
            self.logger.debug("é¡¹ç›®ç±»å‹æ£€æµ‹å®Œæˆ", {"detected_type": project_type})

        type_specific_patterns = self._get_type_specific_excludes(project_type)
        enhanced_patterns.extend(type_specific_patterns)
        self.logger.debug("é¡¹ç›®ç±»å‹ç‰¹å®šæ¨¡å¼æ·»åŠ å®Œæˆ", {
            "project_type": project_type,
            "added_patterns_count": len(type_specific_patterns)
        })

        # åˆ†æç„¦ç‚¹è¿‡æ»¤
        if analysis_focus.get("exclude_examples", True):
            enhanced_patterns.extend(["examples", "example", "demo", "demos", "samples"])

        if analysis_focus.get("exclude_migrations", True):
            enhanced_patterns.extend(["migrations", "migrate", "db/migrate"])

        return list(set(enhanced_patterns))  # å»é‡

    def _apply_content_filtering(self, project_info: Dict[str, Any],
                                 ignore_patterns: Dict[str, Any],
                                 analysis_focus: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨åŸºäºå†…å®¹çš„è¿‡æ»¤"""
        content_rules = ignore_patterns.get("content_based", ["empty_files", "generated_files", "binary_files"])

        filtered_files = []
        for file_info in project_info.get("files", []):

            # æ£€æŸ¥ç©ºæ–‡ä»¶
            if "empty_files" in content_rules and file_info.get("size", 0) == 0:
                continue

            # æ£€æŸ¥ç”Ÿæˆæ–‡ä»¶
            if "generated_files" in content_rules and self._is_generated_file(file_info):
                continue

            # æ£€æŸ¥äºŒè¿›åˆ¶æ–‡ä»¶
            if "binary_files" in content_rules and self._is_binary_file(file_info):
                continue

            # ä¸»è¦æºç è¿‡æ»¤
            if analysis_focus.get("main_source_only", False) and not self._is_main_source(file_info):
                continue

            filtered_files.append(file_info)

        project_info["files"] = filtered_files

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if "statistics" in project_info:
            project_info["statistics"]["total_files"] = len(filtered_files)
            project_info["statistics"]["total_size"] = sum(f.get("size", 0) for f in filtered_files)

        return project_info

    def _detect_project_type(self, project_path: str) -> str:
        """æ£€æµ‹é¡¹ç›®ç±»å‹"""
        self.logger.debug("å¼€å§‹æ£€æµ‹é¡¹ç›®ç±»å‹", {"project_path": project_path})
        project_path = Path(project_path)

        # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶
        if (project_path / "requirements.txt").exists() or (project_path / "setup.py").exists():
            return "python"
        elif (project_path / "package.json").exists():
            return "javascript"
        elif (project_path / "pom.xml").exists() or (project_path / "build.gradle").exists():
            return "java"
        elif (project_path / "go.mod").exists():
            return "go"
        elif (project_path / "Cargo.toml").exists():
            return "rust"

        detected_type = "unknown"
        self.logger.debug("é¡¹ç›®ç±»å‹æ£€æµ‹ç»“æœ", {"type": detected_type})
        return detected_type

    def _get_type_specific_excludes(self, project_type: str) -> list:
        """è·å–é¡¹ç›®ç±»å‹ç‰¹å®šçš„æ’é™¤æ¨¡å¼"""
        # é€šç”¨ç³»ç»Ÿæ–‡ä»¶æ’é™¤ï¼ˆæ‰€æœ‰é¡¹ç›®ç±»å‹éƒ½é€‚ç”¨ï¼‰
        common_excludes = [".DS_Store", "Thumbs.db", "*.log", ".gitignore", ".gitkeep", "*.tmp", "*.temp"]
        
        type_patterns = {
            "python": ["*.pyc", "*.pyo", "*.pyd", "__pycache__", ".pytest_cache", "venv", "env", ".venv", ".coverage"],
            "javascript": ["node_modules", "dist", "build", "*.min.js", ".next", ".nuxt"],
            "java": ["target", "build", "*.class", "*.jar", "*.war"],
            "go": ["vendor", "*.exe"],
            "rust": ["target", "Cargo.lock"]
        }

        # åˆå¹¶é€šç”¨æ’é™¤å’Œé¡¹ç›®ç‰¹å®šæ’é™¤
        project_specific = type_patterns.get(project_type, [])
        return common_excludes + project_specific

    def _is_generated_file(self, file_info: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç”Ÿæˆæ–‡ä»¶"""
        file_path = file_info.get("path", "")
        content = file_info.get("content", "")

        # æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼
        generated_patterns = [
            "generated", "gen_", "_gen", ".generated",
            "dist/", "build/", "out/", ".min.", "bundle."
        ]

        for pattern in generated_patterns:
            if pattern in file_path.lower():
                return True

        # æ£€æŸ¥å†…å®¹ç‰¹å¾ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
        if content:
            generated_markers = [
                "// This file was automatically generated",
                "# This file is automatically generated",
                "/* Auto-generated",
                "DO NOT EDIT"
            ]

            content_start = content[:500].upper()
            for marker in generated_markers:
                if marker.upper() in content_start:
                    return True

        return False

    def _is_binary_file(self, file_info: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶"""
        file_path = file_info.get("path", "")

        binary_extensions = [
            ".exe", ".dll", ".so", ".dylib", ".a", ".lib",
            ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".ico",
            ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
            ".zip", ".tar", ".gz", ".rar", ".7z",
            ".mp3", ".mp4", ".avi", ".mov", ".wmv"
        ]

        file_ext = Path(file_path).suffix.lower()
        return file_ext in binary_extensions

    def _is_main_source(self, file_info: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¦æºç æ–‡ä»¶"""
        file_path = file_info.get("path", "")

        # æ’é™¤æµ‹è¯•æ–‡ä»¶
        if any(test_pattern in file_path.lower() for test_pattern in ["test", "spec", "__test__"]):
            return False

        # æ’é™¤é…ç½®æ–‡ä»¶
        config_patterns = ["config", "setting", ".env", "Dockerfile"]
        if any(config_pattern in file_path.lower() for config_pattern in config_patterns):
            return False

        # æ’é™¤æ–‡æ¡£æ–‡ä»¶  
        if any(file_path.lower().endswith(ext) for ext in [".md", ".txt", ".rst"]):
            return False

        return True

    def _error_response(self, message: str) -> Dict[str, Any]:
        """é”™è¯¯å“åº”"""
        self.logger.error("ç”Ÿæˆé”™è¯¯å“åº”", {"error_message": message})
        return {
            "success": False,
            "tool": self.tool_name,
            "error": message
        }


def create_mcp_tool() -> DocScanTool:
    """åˆ›å»ºMCPå·¥å…·å®ä¾‹"""
    return DocScanTool()


# å‘½ä»¤è¡Œæ¥å£ï¼Œç”¨äºæµ‹è¯•
def main():
    """å‘½ä»¤è¡Œæµ‹è¯•æ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="MCP doc_scan tool")
    parser.add_argument("project_path", help="Project path to scan")
    parser.add_argument("--no-content", action="store_true",
                        help="Don't include file content in results")
    parser.add_argument("--extensions", nargs="+", default=[".py"],
                        help="File extensions to include")
    parser.add_argument("--max-size", type=int, default=122880,
                        help="Maximum file size in characters")
    parser.add_argument("--max-depth", type=int, default=3,
                        help="Maximum directory depth for tree scan")

    args = parser.parse_args()

    # æ„å»ºå‚æ•°
    arguments = {
        "project_path": args.project_path,
        "include_content": not args.no_content,
        "config": {
            "file_extensions": args.extensions,
            "max_file_size": args.max_size,
            "max_depth": args.max_depth
        }
    }

    # æ‰§è¡Œå·¥å…·
    tool = create_mcp_tool()
    result = tool.execute(arguments)

    # è¾“å‡ºç»“æœ
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
