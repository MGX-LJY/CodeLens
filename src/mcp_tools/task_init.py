"""
MCP task_init å·¥å…·å®ç°
åŸºäºé¡¹ç›®åˆ†æç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„é˜¶æ®µæ€§ä»»åŠ¡åˆ—è¡¨
"""
import sys
import os
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°pathä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

from src.task_engine.task_manager import TaskManager, TaskType, TaskStatus
from src.task_engine.phase_controller import PhaseController, Phase
from src.logging import get_logger

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from src.config import get_file_filtering_config, get_file_size_limits_config, get_tool_config

    HAS_CONFIG_MANAGER = True
except ImportError:
    HAS_CONFIG_MANAGER = False
    get_file_filtering_config = lambda: None
    get_file_size_limits_config = lambda: None
    get_tool_config = lambda x: {}


class TaskPlanGenerator:
    """ä»»åŠ¡è®¡åˆ’ç”Ÿæˆå™¨"""

    def __init__(self):
        self.logger = get_logger(component="TaskPlanGenerator", operation="init")
        self.logger.info("TaskPlanGenerator åˆå§‹åŒ–å¼€å§‹")

        # æ¨¡æ¿æ˜ å°„å…³ç³»
        self.template_mapping = {
            TaskType.FILE_SUMMARY: "file_summary",
            TaskType.ARCHITECTURE: "architecture",
            TaskType.TECH_STACK: "tech_stack",
            TaskType.DATA_FLOW: "data_flow",
            TaskType.SYSTEM_ARCHITECTURE: "system_architecture",
            TaskType.COMPONENT_DIAGRAM: "component_diagram",
            TaskType.DEPLOYMENT_DIAGRAM: "deployment_diagram",
            TaskType.PROJECT_README: "project_readme"
        }

        # åŠ è½½é…ç½®
        self._load_config()

        self.logger.info("TaskPlanGenerator åˆå§‹åŒ–å®Œæˆ", {
            "template_mapping_count": len(self.template_mapping),
            "priority_levels": len(self.priority_mapping),
            "filter_rules": len(self.file_filters["exclude_patterns"]),
            "config_manager_available": HAS_CONFIG_MANAGER
        })

    def _load_config(self):
        """åŠ è½½é…ç½®"""
        if HAS_CONFIG_MANAGER:
            try:
                # ä»é…ç½®ç®¡ç†å™¨è·å–é…ç½®
                filtering_config = get_file_filtering_config()
                file_size_config = get_file_size_limits_config()
                tool_config = get_tool_config("task_init")

                # ä¼˜å…ˆçº§æ˜ å°„ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„smart_filtering.priority_patternsæˆ–é»˜è®¤å€¼ï¼‰
                if filtering_config and hasattr(filtering_config, 'smart_filtering'):
                    priority_patterns = filtering_config.smart_filtering.get('priority_patterns', {})
                    self.priority_mapping = {
                        "high": priority_patterns.get("high", ["main.py", "app.py", "index.js", "server.js", "main.go",
                                                               "main.rs"]),
                        "normal": priority_patterns.get("normal",
                                                        ["config", "model", "service", "controller", "handler"]),
                        "low": priority_patterns.get("low", ["util", "helper", "test", "spec"])
                    }
                else:
                    self._use_default_priority_mapping()

                # æ–‡ä»¶è¿‡æ»¤è§„åˆ™ï¼ˆä»é…ç½®è·å–ï¼‰
                if filtering_config:
                    self.file_filters = {
                        "exclude_patterns": filtering_config.exclude_patterns,
                        "exclude_directories": filtering_config.exclude_directories,
                        "min_file_size": file_size_config.min_file_size if file_size_config else 50,
                        "max_files_per_project": filtering_config.smart_filtering.get('max_files_per_project',
                                                                                      25) if hasattr(filtering_config,
                                                                                                     'smart_filtering') else 25
                    }
                else:
                    self._use_default_file_filters()

                self.logger.info("é…ç½®åŠ è½½æˆåŠŸ", {
                    "exclude_patterns_count": len(self.file_filters["exclude_patterns"]),
                    "exclude_directories_count": len(self.file_filters["exclude_directories"]),
                    "min_file_size": self.file_filters["min_file_size"],
                    "max_files_per_project": self.file_filters["max_files_per_project"]
                })

            except Exception as e:
                self.logger.warning(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                self._use_default_config()
        else:
            self.logger.warning("é…ç½®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self._use_default_config()

    def _use_default_config(self):
        """ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        self._use_default_priority_mapping()
        self._use_default_file_filters()

    def _use_default_priority_mapping(self):
        """ä½¿ç”¨é»˜è®¤ä¼˜å…ˆçº§æ˜ å°„"""
        self.priority_mapping = {
            "high": ["main.py", "app.py", "index.js", "server.js", "main.go", "main.rs"],
            "normal": ["config", "model", "service", "controller", "handler"],
            "low": ["util", "helper", "test", "spec"]
        }

    def _use_default_file_filters(self):
        """ä½¿ç”¨é»˜è®¤æ–‡ä»¶è¿‡æ»¤è§„åˆ™ - ä¼˜å…ˆä½¿ç”¨é…ç½®ç³»ç»Ÿ"""
        # é¦–å…ˆå°è¯•ä½¿ç”¨é…ç½®ç³»ç»Ÿ
        if HAS_CONFIG_MANAGER:
            try:
                filtering_config = get_file_filtering_config()
                file_size_config = get_file_size_limits_config()
                
                if filtering_config and file_size_config:
                    self.file_filters = {
                        "exclude_patterns": filtering_config.exclude_patterns,
                        "exclude_directories": filtering_config.exclude_directories,
                        "min_file_size": file_size_config.min_file_size,
                        "max_files_per_project": filtering_config.smart_filtering.get('max_files_per_project', 25) if hasattr(filtering_config, 'smart_filtering') else 25
                    }
                    
                    self.logger.info("ä½¿ç”¨é…ç½®ç³»ç»Ÿçš„è¿‡æ»¤è§„åˆ™", {
                        "exclude_patterns_count": len(self.file_filters["exclude_patterns"]),
                        "exclude_directories_count": len(self.file_filters["exclude_directories"]),
                        "min_file_size": self.file_filters["min_file_size"],
                        "max_files_per_project": self.file_filters["max_files_per_project"]
                    })
                    return
                else:
                    self.logger.warning("é…ç½®ç³»ç»Ÿè¿”å›ç©ºé…ç½®ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
            except Exception as e:
                self.logger.warning(f"é…ç½®ç³»ç»Ÿè®¿é—®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™: {e}")
        else:
            self.logger.debug("é…ç½®ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å±è”½è§„åˆ™")
        
        # ä½¿ç”¨é»˜è®¤è§„åˆ™ä½œä¸ºåå¤‡
        self.file_filters = {
            "exclude_patterns": [
                "__init__.py",  # ç©ºçš„åˆå§‹åŒ–æ–‡ä»¶
                "__pycache__",
                ".pyc",
                ".git",
                "node_modules",
                ".venv",
                "venv",
                "test_",
                "_test.py",
                "conftest.py",
                ".env",
                ".example",
                "requirements.txt",
                "package.json",
                "Dockerfile",
                ".yml",
                ".yaml",
                ".json",
                ".md",
                ".txt",
                ".log"
            ],
            "exclude_directories": [
                "tests",
                "test",
                "__pycache__",
                ".git",
                "node_modules",
                ".venv",
                "venv",
                "build",
                "dist",
                "logs",
                "temp",
                "tmp",
                "src/config",  # æ’é™¤é…ç½®ç›®å½•
                "config"  # æ’é™¤é…ç½®ç›®å½•
            ],
            "min_file_size": 50,  # æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            "max_files_per_project": 25  # æ¯ä¸ªé¡¹ç›®æœ€å¤§æ–‡ä»¶æ•°
        }
        
        self.logger.info("ä½¿ç”¨é»˜è®¤è¿‡æ»¤è§„åˆ™", {
            "exclude_patterns_count": len(self.file_filters["exclude_patterns"]),
            "exclude_directories_count": len(self.file_filters["exclude_directories"])
        })

    def _get_include_extensions(self) -> List[str]:
        """è·å–è¦åŒ…å«çš„æ–‡ä»¶æ‰©å±•å"""
        if HAS_CONFIG_MANAGER:
            try:
                filtering_config = get_file_filtering_config()
                if filtering_config and hasattr(filtering_config, 'include_extensions'):
                    return filtering_config.include_extensions
            except Exception as e:
                self.logger.warning(f"è·å–é…ç½®æ‰©å±•åå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")

        # é»˜è®¤æ‰©å±•ååˆ—è¡¨ï¼ˆä¸é…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´ï¼‰
        return [
            ".py", ".pyw", ".pyi",
            ".js", ".mjs", ".cjs", ".jsx",
            ".ts", ".tsx",
            ".html", ".htm", ".xhtml",
            ".css", ".scss", ".sass", ".less", ".styl",
            ".sh", ".bash", ".zsh", ".fish",
            ".ps1", ".psm1",
            ".bat", ".cmd",
            ".json", ".yml", ".yaml",
            ".md", ".txt"
        ]

    def generate_tasks_auto(self, project_path: str,
                            task_granularity: str = "file",
                            max_files: int = None) -> Dict[str, Any]:
        """æ™ºèƒ½ç”Ÿæˆä»»åŠ¡è®¡åˆ’ - ç®€åŒ–ç‰ˆAPI"""
        operation_id = self.logger.log_operation_start("generate_tasks_auto",
                                                       project_path=project_path,
                                                       task_granularity=task_granularity,
                                                       max_files=max_files)

        try:
            # 1. è‡ªåŠ¨åŠ è½½åˆ†ææ•°æ®
            self.logger.info("è‡ªåŠ¨åŠ è½½é¡¹ç›®åˆ†ææ•°æ®")
            analysis_result = self._auto_load_analysis_data(project_path)
            if not analysis_result:
                raise ValueError("æ— æ³•åŠ è½½é¡¹ç›®åˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ doc_guide")

            # 2. æ™ºèƒ½è¿‡æ»¤æ–‡ä»¶
            self.logger.info("å¼€å§‹æ™ºèƒ½æ–‡ä»¶è¿‡æ»¤")
            filtered_files = self._smart_filter_files(project_path, analysis_result, max_files or 999999)
            self.logger.info(f"æ–‡ä»¶è¿‡æ»¤å®Œæˆï¼Œä»åŸå§‹æ–‡ä»¶ä¸­ç­›é€‰å‡º {len(filtered_files)} ä¸ªé‡è¦æ–‡ä»¶")

            # 3. æ›´æ–°åˆ†æç»“æœä¸­çš„æ–‡ä»¶åˆ—è¡¨
            if "generation_plan" not in analysis_result:
                analysis_result["generation_plan"] = {}
            analysis_result["generation_plan"]["phase_1_files"] = filtered_files

            # 4. è°ƒç”¨åŸå§‹çš„generate_tasksæ–¹æ³•
            return self.generate_tasks(project_path, analysis_result, task_granularity, False, None)

        except Exception as e:
            self.logger.log_operation_end("generate_tasks_auto", operation_id, success=False, error=str(e))
            raise e
        finally:
            self.logger.log_operation_end("generate_tasks_auto", operation_id, success=True)

    def _auto_load_analysis_data(self, project_path: str) -> Dict[str, Any]:
        """è‡ªåŠ¨åŠ è½½é¡¹ç›®åˆ†ææ•°æ®"""
        analysis_file = Path(project_path) / ".codelens" / "analysis.json"

        if not analysis_file.exists():
            self.logger.warning("åˆ†ææ–‡ä»¶ä¸å­˜åœ¨", {"analysis_file": str(analysis_file)})
            return None

        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info("æˆåŠŸåŠ è½½åˆ†ææ•°æ®", {"file_path": str(analysis_file)})
                return data
        except Exception as e:
            self.logger.error("åŠ è½½åˆ†ææ•°æ®å¤±è´¥", {"error": str(e), "file_path": str(analysis_file)})
            return None

    def _smart_filter_files(self, project_path: str, analysis_result: Dict[str, Any], max_files: int = 20) -> List[str]:
        """æ™ºèƒ½è¿‡æ»¤æ–‡ä»¶"""
        # è·å–é…ç½®ä¸­çš„åŒ…å«æ–‡ä»¶æ‰©å±•å
        all_files = []
        project_root = Path(project_path)

        # ä»é…ç½®è·å–è¦åŒ…å«çš„æ–‡ä»¶æ‰©å±•å
        include_extensions = self._get_include_extensions()

        # æŒ‰æ‰©å±•åæ‰«ææ‰€æœ‰ç›®æ ‡æ–‡ä»¶
        for extension in include_extensions:
            # ç§»é™¤æ‰©å±•åå‰çš„ç‚¹å·ï¼Œç”¨äºglobæ¨¡å¼
            glob_pattern = f"*{extension}"
            for file_path in project_root.rglob(glob_pattern):
                try:
                    rel_path = file_path.relative_to(project_root)
                    all_files.append(str(rel_path))
                except ValueError:
                    # è·³è¿‡æ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„çš„æ–‡ä»¶
                    continue

        self.logger.info(f"æ‰«æåˆ° {len(all_files)} ä¸ªæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰é…ç½®çš„æ‰©å±•åï¼‰")

        # åº”ç”¨è¿‡æ»¤è§„åˆ™
        filtered_files = []
        for file_path in all_files:
            if self._should_include_file(file_path, project_root):
                filtered_files.append(file_path)

        self.logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_files)} ä¸ªæ–‡ä»¶")

        # æŒ‰é‡è¦æ€§æ’åº
        prioritized_files = self._prioritize_files(filtered_files, project_root)

        # ä¸å†é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œå¤„ç†æ‰€æœ‰æ‰«æåˆ°çš„æ–‡ä»¶
        self.logger.info(f"å°†å¤„ç†æ‰€æœ‰ {len(prioritized_files)} ä¸ªæ–‡ä»¶ï¼Œä¸å†é™åˆ¶æ•°é‡")

        return prioritized_files

    def _should_include_file(self, file_path: str, project_root: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«æŸä¸ªæ–‡ä»¶"""
        file_path_lower = file_path.lower()

        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        for pattern in self.file_filters["exclude_patterns"]:
            if pattern in file_path_lower:
                return False

        # æ£€æŸ¥æ’é™¤ç›®å½•
        for exclude_dir in self.file_filters["exclude_directories"]:
            if exclude_dir in file_path_lower:
                return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        full_path = project_root / file_path
        try:
            if full_path.stat().st_size < self.file_filters["min_file_size"]:
                return False
        except:
            return False

        # ç‰¹æ®Šå¤„ç†ï¼šæ’é™¤ç©ºçš„__init__.pyæ–‡ä»¶
        if file_path.endswith("__init__.py"):
            try:
                if full_path.stat().st_size < 100:  # å°äº100å­—èŠ‚è®¤ä¸ºæ˜¯ç©ºæ–‡ä»¶
                    return False
            except:
                return False

        return True

    def _prioritize_files(self, files: List[str], project_root: Path) -> List[str]:
        """æŒ‰é‡è¦æ€§å¯¹æ–‡ä»¶è¿›è¡Œæ’åº"""
        file_scores = []

        for file_path in files:
            score = self._calculate_file_importance(file_path, project_root)
            file_scores.append((file_path, score))

        # æŒ‰åˆ†æ•°é™åºæ’åº
        file_scores.sort(key=lambda x: x[1], reverse=True)

        return [file_path for file_path, score in file_scores]

    def _calculate_file_importance(self, file_path: str, project_root: Path) -> int:
        """è®¡ç®—æ–‡ä»¶é‡è¦æ€§åˆ†æ•°"""
        score = 0
        file_path_lower = file_path.lower()
        file_name = Path(file_path).name.lower()

        # ä¸»è¦å…¥å£æ–‡ä»¶ (+100)
        if file_name in ["main.py", "app.py", "server.py", "index.py"]:
            score += 100

        # æ¨¡å‹å’Œæ ¸å¿ƒä¸šåŠ¡æ–‡ä»¶ (+50)
        if "model" in file_path_lower or "service" in file_path_lower:
            score += 50

        # è·¯ç”±å’Œæ§åˆ¶å™¨ (+30)
        if "route" in file_path_lower or "controller" in file_path_lower or "handler" in file_path_lower:
            score += 30

        # é…ç½®æ–‡ä»¶ (+20)
        if "config" in file_path_lower or "setting" in file_path_lower:
            score += 20

        # æ•°æ®åº“ç›¸å…³ (+25)
        if "db.py" in file_path_lower or "database" in file_path_lower:
            score += 25

        # æ–‡ä»¶å¤§å°åŠ åˆ†
        try:
            full_path = project_root / file_path
            file_size = full_path.stat().st_size
            if file_size > 5000:  # å¤§äº5KB
                score += 10
            if file_size > 15000:  # å¤§äº15KB  
                score += 10
        except:
            pass

        # ç›®å½•æ·±åº¦å‡åˆ†ï¼ˆè¶Šæ·±è¶Šä¸é‡è¦ï¼‰
        depth = len(Path(file_path).parts) - 1
        score -= depth * 5

        return score

    def generate_tasks(self, project_path: str, analysis_result: Dict[str, Any],
                       task_granularity: str = "file", parallel_tasks: bool = False,
                       custom_priorities: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„ä»»åŠ¡è®¡åˆ’"""
        operation_id = self.logger.log_operation_start("generate_tasks",
                                                       project_path=project_path,
                                                       task_granularity=task_granularity,
                                                       parallel_tasks=parallel_tasks)

        self.logger.info("å¼€å§‹ç”Ÿæˆä»»åŠ¡è®¡åˆ’", {
            "project_path": project_path,
            "task_granularity": task_granularity,
            "parallel_tasks": parallel_tasks,
            "has_custom_priorities": custom_priorities is not None,
            "operation_id": operation_id
        })

        # æå–åˆ†æç»“æœ - ä¿®å¤åµŒå¥—JSONç»“æ„è§£æ
        self.logger.debug("è§£æåˆ†æç»“æœç»“æ„")
        if "data" in analysis_result:
            # å¦‚æœæ˜¯MCPå·¥å…·çš„è¾“å‡ºæ ¼å¼
            data = analysis_result["data"]
            project_analysis = data.get("project_analysis", {})
            plan = data.get("generation_plan", {})
        else:
            # å¦‚æœæ˜¯ç›´æ¥çš„åˆ†æç»“æœæ ¼å¼
            project_analysis = analysis_result.get("project_analysis", {})
            plan = analysis_result.get("generation_plan", {})

        # ç”Ÿæˆå„é˜¶æ®µä»»åŠ¡ (3é˜¶æ®µæ¶æ„)
        self.logger.info("å¼€å§‹ç”Ÿæˆå„é˜¶æ®µä»»åŠ¡")

        self.logger.debug("ç”ŸæˆPhase 1ä»»åŠ¡ï¼ˆæ–‡ä»¶å±‚ï¼‰")
        phase_1_tasks = self._generate_phase_1_tasks(project_path, plan, custom_priorities)
        self.logger.info("Phase 1ä»»åŠ¡ç”Ÿæˆå®Œæˆ", {"task_count": len(phase_1_tasks)})

        self.logger.debug("ç”ŸæˆPhase 2ä»»åŠ¡ï¼ˆæ¶æ„å±‚ï¼‰")
        phase_2_tasks = self._generate_phase_2_tasks(project_path, project_analysis, phase_1_tasks)  # æ¶æ„å±‚
        self.logger.info("Phase 2ä»»åŠ¡ç”Ÿæˆå®Œæˆ", {"task_count": len(phase_2_tasks)})

        self.logger.debug("ç”ŸæˆPhase 3ä»»åŠ¡ï¼ˆé¡¹ç›®å±‚ï¼‰")
        phase_3_tasks = self._generate_phase_3_tasks(project_path, project_analysis, phase_2_tasks)  # é¡¹ç›®å±‚
        self.logger.info("Phase 3ä»»åŠ¡ç”Ÿæˆå®Œæˆ", {"task_count": len(phase_3_tasks)})

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        all_tasks = phase_1_tasks + phase_2_tasks + phase_3_tasks
        self.logger.info("ä»»åŠ¡è®¡åˆ’ç”Ÿæˆå®Œæˆ", {
            "total_tasks": len(all_tasks),
            "phase_breakdown": {
                "phase_1": len(phase_1_tasks),
                "phase_2": len(phase_2_tasks),
                "phase_3": len(phase_3_tasks)
            }
        })

        task_plan = {
            "total_phases": 3,
            "total_tasks": len(all_tasks),
            "estimated_duration": plan.get("estimated_duration", "Unknown"),
            "dependencies_graph": self._build_dependency_graph(all_tasks),
            "task_distribution": {
                "phase_1_files": len(phase_1_tasks),
                "phase_2_architecture": len(phase_2_tasks),
                "phase_3_project": len(phase_3_tasks)
            }
        }

        # æ„å»ºå®Œæ•´å“åº”
        result = {
            "task_plan": task_plan,
            "phase_1_files": {
                "description": f"æ–‡ä»¶å±‚æ–‡æ¡£ç”Ÿæˆï¼ˆ{len(phase_1_tasks)}ä¸ªæ–‡ä»¶ï¼‰",
                "dependencies": [],
                "estimated_time": f"{len(phase_1_tasks) * 3} minutes",
                "tasks": phase_1_tasks
            },
            "phase_2_architecture": {
                "description": f"æ¶æ„å±‚æ–‡æ¡£ç”Ÿæˆï¼ˆ{len(phase_2_tasks)}ä¸ªæ¨¡æ¿ï¼‰",
                "dependencies": ["phase_1_complete"],
                "estimated_time": f"{len(phase_2_tasks) * 10} minutes",
                "tasks": phase_2_tasks
            },
            "phase_3_project": {
                "description": "é¡¹ç›®å±‚æ–‡æ¡£ç”Ÿæˆï¼ˆä»…README.mdï¼‰",
                "dependencies": ["phase_2_complete"],
                "estimated_time": "10 minutes",
                "tasks": phase_3_tasks
            }
        }

        self.logger.log_operation_end("generate_tasks", operation_id, success=True)
        return result

    def _generate_phase_1_tasks(self, project_path: str, plan: Dict[str, Any],
                                custom_priorities: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¬¬ä¸€é˜¶æ®µä»»åŠ¡ï¼ˆæ–‡ä»¶å±‚ï¼‰"""
        tasks = []
        files_to_process = plan.get("phase_1_files", [])

        for i, file_path in enumerate(files_to_process):
            task_id = f"file_summary_{int(time.time() * 1000)}_{i}"

            # ç¡®å®šä¼˜å…ˆçº§
            priority = self._get_file_priority(file_path, custom_priorities)

            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_path = f"docs/files/summaries/{file_path}.md"

            task = {
                "id": task_id,
                "type": "file_summary",
                "description": f"ç”Ÿæˆ{file_path}æ–‡ä»¶æ‘˜è¦",
                "phase": "phase_1_files",
                "target_file": file_path,
                "template": "file_summary",
                "output_path": output_path,
                "dependencies": [],  # æ–‡ä»¶ä»»åŠ¡ä¸å†ä¾èµ–scanä»»åŠ¡
                "priority": priority,
                "estimated_time": "3 minutes",
                "status": "pending",
                "metadata": {
                    "file_type": Path(file_path).suffix,
                    "file_size_category": "unknown"
                }
            }

            tasks.append(task)

        return tasks

    def _generate_phase_2_tasks(self, project_path: str, analysis: Dict[str, Any],
                                phase_1_tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¬¬äºŒé˜¶æ®µä»»åŠ¡ï¼ˆæ¶æ„å±‚ï¼‰"""
        tasks = []

        # æ‰€æœ‰æ–‡ä»¶å±‚ä»»åŠ¡ä½œä¸ºä¾èµ–
        file_task_ids = [task["id"] for task in phase_1_tasks]

        # 1. æ¶æ„æ¦‚è¿°
        task_id = f"architecture_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "architecture",
            "description": "ç”Ÿæˆç³»ç»Ÿæ¶æ„æ¦‚è¿°",
            "phase": "phase_2_architecture",
            "template": "architecture",
            "output_path": "docs/architecture/overview.md",
            "dependencies": file_task_ids,
            "priority": "high",
            "estimated_time": "12 minutes",
            "status": "pending",
            "metadata": {
                "project_type": analysis.get("project_type", "unknown"),
                "complexity": analysis.get("code_complexity", "unknown")
            }
        })

        architecture_id = task_id

        # 2. æŠ€æœ¯æ ˆåˆ†æ
        task_id = f"tech_stack_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "tech_stack",
            "description": "åˆ†ææŠ€æœ¯æ ˆå’Œæ¶æ„åŸåˆ™",
            "phase": "phase_2_architecture",
            "template": "tech_stack",
            "output_path": "docs/architecture/tech-stack.md",
            "dependencies": [architecture_id],
            "priority": "high",
            "estimated_time": "10 minutes",
            "status": "pending",
            "metadata": {}
        })

        # 3. æ•°æ®æµè®¾è®¡
        task_id = f"data_flow_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "data_flow",
            "description": "è®¾è®¡ç³»ç»Ÿæ•°æ®æµ",
            "phase": "phase_2_architecture",
            "template": "data_flow",
            "output_path": "docs/architecture/data-flow.md",
            "dependencies": [architecture_id],
            "priority": "normal",
            "estimated_time": "8 minutes",
            "status": "pending",
            "metadata": {}
        })

        # 4. ç³»ç»Ÿæ¶æ„å›¾
        task_id = f"system_architecture_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "system_architecture",
            "description": "ç»˜åˆ¶ç³»ç»Ÿæ¶æ„å›¾",
            "phase": "phase_2_architecture",
            "template": "system_architecture",
            "output_path": "docs/architecture/diagrams/system-architecture.md",
            "dependencies": [architecture_id],
            "priority": "normal",
            "estimated_time": "6 minutes",
            "status": "pending",
            "metadata": {}
        })

        # 5. ç»„ä»¶å…³ç³»å›¾
        task_id = f"component_diagram_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "component_diagram",
            "description": "ç»˜åˆ¶ç»„ä»¶å…³ç³»å›¾",
            "phase": "phase_2_architecture",
            "template": "component_diagram",
            "output_path": "docs/architecture/diagrams/component-diagram.md",
            "dependencies": [architecture_id],
            "priority": "normal",
            "estimated_time": "6 minutes",
            "status": "pending",
            "metadata": {}
        })

        # 6. éƒ¨ç½²æ¶æ„å›¾
        task_id = f"deployment_diagram_{int(time.time() * 1000)}"
        tasks.append({
            "id": task_id,
            "type": "deployment_diagram",
            "description": "è®¾è®¡éƒ¨ç½²æ¶æ„",
            "phase": "phase_2_architecture",
            "template": "deployment_diagram",
            "output_path": "docs/architecture/diagrams/deployment-diagram.md",
            "dependencies": [architecture_id],
            "priority": "low",
            "estimated_time": "5 minutes",
            "status": "pending",
            "metadata": {}
        })

        return tasks

    def _generate_phase_3_tasks(self, project_path: str, analysis: Dict[str, Any],
                                phase_2_tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¬¬ä¸‰é˜¶æ®µä»»åŠ¡ï¼ˆé¡¹ç›®å±‚ï¼‰"""
        tasks = []

        # æ‰€æœ‰æ¶æ„å±‚ä»»åŠ¡ä½œä¸ºä¾èµ–
        arch_task_ids = [task["id"] for task in phase_2_tasks]

        # 1. ç”ŸæˆREADME.md
        readme_task_id = f"project_readme_{int(time.time() * 1000)}"
        tasks.append({
            "id": readme_task_id,
            "type": "project_readme",
            "description": "ç”Ÿæˆé¡¹ç›®READMEæ–‡æ¡£",
            "phase": "phase_3_project",
            "template": "project_readme",
            "output_path": "docs/project/README.md",
            "dependencies": arch_task_ids,
            "priority": "high",
            "estimated_time": "10 minutes",
            "status": "pending",
            "metadata": {
                "project_type": analysis.get("project_type", "unknown"),
                "framework": analysis.get("main_framework", "custom"),
                "complexity": analysis.get("code_complexity", "unknown")
            }
        })

        # 2. ç”ŸæˆCHANGELOG.md
        changelog_task_id = f"changelog_{int(time.time() * 1000)}"
        tasks.append({
            "id": changelog_task_id,
            "type": "changelog",
            "description": "ç”Ÿæˆé¡¹ç›®å˜æ›´æ—¥å¿—æ–‡æ¡£",
            "phase": "phase_3_project",
            "template": "changelog",
            "output_path": "docs/project/CHANGELOG.md",
            "dependencies": arch_task_ids,
            "priority": "normal",
            "estimated_time": "5 minutes",
            "status": "pending",
            "metadata": {
                "project_type": analysis.get("project_type", "unknown"),
                "framework": analysis.get("main_framework", "custom")
            }
        })

        return tasks

    def _get_file_priority(self, file_path: str, custom_priorities: Dict[str, Any] = None) -> str:
        """ç¡®å®šæ–‡ä»¶ä¼˜å…ˆçº§"""
        if custom_priorities and file_path in custom_priorities:
            return custom_priorities[file_path]

        file_lower = file_path.lower()

        # æ£€æŸ¥é«˜ä¼˜å…ˆçº§æ¨¡å¼
        for pattern in self.priority_mapping["high"]:
            if pattern in file_lower:
                return "high"

        # æ£€æŸ¥æ™®é€šä¼˜å…ˆçº§æ¨¡å¼
        for pattern in self.priority_mapping["normal"]:
            if pattern in file_lower:
                return "normal"

        # æ£€æŸ¥ä½ä¼˜å…ˆçº§æ¨¡å¼
        for pattern in self.priority_mapping["low"]:
            if pattern in file_lower:
                return "low"

        return "normal"

    def _build_dependency_graph(self, all_tasks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """æ„å»ºä¾èµ–å…³ç³»å›¾"""
        graph = {}

        for task in all_tasks:
            task_id = task["id"]
            dependencies = task.get("dependencies", [])
            graph[task_id] = dependencies

        return graph

    def create_tasks_in_manager(self, task_manager: TaskManager, task_plan: Dict[str, Any]) -> int:
        """åœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ›å»ºæ‰€æœ‰ä»»åŠ¡"""
        operation_id = self.logger.log_operation_start("create_tasks_in_manager")

        # æ£€æŸ¥task_planæ˜¯å¦æœ‰æ•ˆ
        if task_plan is None:
            self.logger.error("task_planä¸ºNoneï¼Œæ— æ³•åˆ›å»ºä»»åŠ¡")
            self.logger.log_operation_end("create_tasks_in_manager", operation_id, success=False,
                                          error="task_plan is None")
            return 0

        self.logger.info("å¼€å§‹åœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ›å»ºä»»åŠ¡", {
            "operation_id": operation_id,
            "total_phases": len([p for p in task_plan.keys() if p.startswith("phase_")])
        })

        created_count = 0
        skipped_count = 0
        error_count = 0

        # æŒ‰é˜¶æ®µé¡ºåºåˆ›å»ºä»»åŠ¡
        phases = ["phase_1_files", "phase_2_architecture", "phase_3_project"]

        for phase in phases:
            if phase in task_plan:
                phase_data = task_plan[phase]
                tasks = phase_data.get("tasks", [])
                self.logger.info(f"å¤„ç†é˜¶æ®µ {phase}", {"task_count": len(tasks)})

                for task_data in tasks:
                    # è½¬æ¢ä»»åŠ¡ç±»å‹
                    task_type_str = task_data["type"]
                    self.logger.debug("å¤„ç†ä»»åŠ¡", {"task_type": task_type_str, "task_id": task_data.get("id")})

                    try:
                        task_type = TaskType(task_type_str)
                    except ValueError as e:
                        # å¦‚æœæ— æ³•è½¬æ¢ï¼Œè·³è¿‡æ­¤ä»»åŠ¡
                        self.logger.warning(f"è·³è¿‡æ— æ•ˆä»»åŠ¡ç±»å‹: {task_type_str}", {"error": str(e)})
                        skipped_count += 1
                        continue

                    try:
                        # ä¼ å…¥é¢„å®šä¹‰task_idç¡®ä¿ä¾èµ–å…³ç³»ä¸€è‡´æ€§
                        task_id = task_manager.create_task(
                            task_type=task_type,
                            description=task_data["description"],
                            phase=task_data["phase"],
                            target_file=task_data.get("target_file"),
                            target_module=task_data.get("target_module"),
                            template_name=task_data.get("template"),
                            output_path=task_data.get("output_path"),
                            dependencies=task_data.get("dependencies", []),
                            priority=task_data.get("priority", "normal"),
                            estimated_time=task_data.get("estimated_time"),
                            metadata=task_data.get("metadata", {}),
                            task_id=task_data["id"]  # ä½¿ç”¨é¢„å®šä¹‰çš„task_id
                        )

                        created_count += 1
                        self.logger.debug("ä»»åŠ¡åˆ›å»ºæˆåŠŸ", {"task_id": task_id, "type": task_type_str})

                    except Exception as e:
                        self.logger.error("åˆ›å»ºä»»åŠ¡å¤±è´¥", {
                            "task_description": task_data.get('description', 'Unknown'),
                            "error": str(e)
                        })
                        error_count += 1
                        continue

        self.logger.log_operation_end("create_tasks_in_manager", operation_id, success=True,
                                      created_count=created_count,
                                      skipped_count=skipped_count,
                                      error_count=error_count)

        self.logger.info("ä»»åŠ¡åˆ›å»ºå®Œæˆ", {
            "created": created_count,
            "skipped": skipped_count,
            "errors": error_count,
            "total_processed": created_count + skipped_count + error_count
        })

        return created_count


class TaskInitTool:
    """MCP task_init å·¥å…·ç±»"""

    def __init__(self):
        self.tool_name = "task_init"
        self.description = "åŸºäºé¡¹ç›®åˆ†æç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„é˜¶æ®µæ€§ä»»åŠ¡åˆ—è¡¨"
        self.generator = TaskPlanGenerator()
        self.logger = get_logger(component="TaskInitTool", operation="init")
        self.logger.info("TaskInitTool åˆå§‹åŒ–å®Œæˆ")

    def get_tool_definition(self) -> Dict[str, Any]:
        """è·å–MCPå·¥å…·å®šä¹‰"""
        return {
            "name": self.tool_name,
            "description": self.description,
            "inputSchema": {
                "type": "object",
                "properties": {
                    "project_path": {
                        "type": "string",
                        "description": "é¡¹ç›®è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰"
                    },
                    "analysis_result": {
                        "type": "object",
                        "description": "doc_guideçš„åˆ†æç»“æœï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨åŠ è½½ï¼‰"
                    },
                    "task_granularity": {
                        "type": "string",
                        "enum": ["file", "batch", "module"],
                        "description": "ä»»åŠ¡ç²’åº¦"
                    },
                    "max_files": {
                        "type": "number",
                        "description": "æœ€å¤§æ–‡ä»¶æ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤20ä¸ªï¼‰"
                    },
                    "create_in_manager": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ›å»ºä»»åŠ¡"
                    },
                    "auto_mode": {
                        "type": "boolean",
                        "description": "æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ¨¡å¼ï¼ˆè‡ªåŠ¨è¿‡æ»¤æ–‡ä»¶ï¼Œç®€åŒ–å‚æ•°ï¼‰"
                    }
                },
                "required": ["project_path"]
            }
        }

    def execute(self, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œtask_initå·¥å…·"""
        operation_id = self.logger.log_operation_start("execute_task_init",
                                                       project_path=arguments.get("project_path"),
                                                       auto_mode=arguments.get("auto_mode", True))

        try:
            self.logger.info("å¼€å§‹æ‰§è¡Œtask_initå·¥å…·", {"arguments": arguments, "operation_id": operation_id})

            # å‚æ•°éªŒè¯
            project_path = arguments.get("project_path")
            if not project_path:
                project_path = os.getcwd()  # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•

            if not os.path.exists(project_path):
                error_msg = "Invalid project path"
                self.logger.error(error_msg, {"project_path": project_path})
                return self._error_response(error_msg)

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ¨¡å¼ (é»˜è®¤å¯ç”¨)
            auto_mode = arguments.get("auto_mode", True)
            analysis_result = arguments.get("analysis_result")

            self.logger.info("æ¨¡å¼æ£€æŸ¥", {
                "auto_mode": auto_mode,
                "has_analysis_result": analysis_result is not None
            })

            # è·å–å‚æ•°
            task_granularity = arguments.get("task_granularity", "file")
            max_files = arguments.get("max_files", 20)
            create_in_manager = arguments.get("create_in_manager", False)

            # æ™ºèƒ½æ¨¡å¼ï¼šè‡ªåŠ¨åŠ è½½æ•°æ®å¹¶è¿‡æ»¤æ–‡ä»¶
            if auto_mode:
                self.logger.info("ä½¿ç”¨æ™ºèƒ½æ¨¡å¼ç”Ÿæˆä»»åŠ¡è®¡åˆ’", {
                    "project_path": project_path,
                    "task_granularity": task_granularity,
                    "max_files": max_files,
                    "create_in_manager": create_in_manager
                })

                # ç”Ÿæˆä»»åŠ¡è®¡åˆ’ - ä½¿ç”¨æ™ºèƒ½API
                self.logger.debug("è°ƒç”¨TaskPlanGeneratoræ™ºèƒ½æ¨¡å¼")
                task_plan = self.generator.generate_tasks_auto(
                    project_path=project_path,
                    task_granularity=task_granularity,
                    max_files=max_files
                )
                self.logger.debug("æ™ºèƒ½ä»»åŠ¡è®¡åˆ’ç”Ÿæˆå®Œæˆ")

            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šéœ€è¦æ‰‹åŠ¨æä¾›analysis_result
                if not analysis_result:
                    error_msg = "Analysis result is required when auto_mode is disabled"
                    self.logger.error(error_msg)
                    return self._error_response(error_msg)

                parallel_tasks = arguments.get("parallel_tasks", False)
                custom_priorities = arguments.get("custom_priorities", {})

                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼ç”Ÿæˆä»»åŠ¡è®¡åˆ’", {
                    "project_path": project_path,
                    "task_granularity": task_granularity,
                    "parallel_tasks": parallel_tasks,
                    "has_custom_priorities": bool(custom_priorities),
                    "create_in_manager": create_in_manager
                })

                # ç”Ÿæˆä»»åŠ¡è®¡åˆ’ - ä½¿ç”¨ä¼ ç»ŸAPI
                self.logger.debug("è°ƒç”¨TaskPlanGeneratorä¼ ç»Ÿæ¨¡å¼")
                task_plan = self.generator.generate_tasks(
                    project_path=project_path,
                    analysis_result=analysis_result,
                    task_granularity=task_granularity,
                    parallel_tasks=parallel_tasks,
                    custom_priorities=custom_priorities
                )
                self.logger.debug("ä¼ ç»Ÿä»»åŠ¡è®¡åˆ’ç”Ÿæˆå®Œæˆ")

            # æ£€æŸ¥ä»»åŠ¡è®¡åˆ’æ˜¯å¦ç”ŸæˆæˆåŠŸ
            if task_plan is None:
                self.logger.error("ä»»åŠ¡è®¡åˆ’ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›None")
                self.logger.log_operation_end("execute_task_init", operation_id, success=False,
                                              error="Task plan generation failed")
                return self._error_response("Task plan generation failed: generate_tasks returned None")

            # å¦‚æœéœ€è¦ï¼Œåœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ›å»ºä»»åŠ¡
            created_count = 0
            if create_in_manager:
                self.logger.info("å¼€å§‹åœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­åˆ›å»ºä»»åŠ¡")
                task_manager = TaskManager(project_path)
                created_count = self.generator.create_tasks_in_manager(task_manager, task_plan)
                self.logger.info("ä»»åŠ¡å·²åˆ›å»ºåœ¨ç®¡ç†å™¨ä¸­", {"created_count": created_count})

            # å®‰å…¨åœ°è®¿é—®task_planæ•°æ®
            total_tasks = task_plan.get('task_plan', {}).get('total_tasks', 0) if task_plan else 0
            total_phases = task_plan.get('task_plan', {}).get('total_phases', 0) if task_plan else 0

            self.logger.log_operation_end("execute_task_init", operation_id, success=True,
                                          total_tasks=total_tasks,
                                          total_phases=total_phases,
                                          created_in_manager=create_in_manager,
                                          created_count=created_count)

            # ä¼˜åŒ–å“åº”ï¼šåªè¿”å›å‰10ä¸ªä»»åŠ¡è¯¦æƒ…ï¼Œé˜²æ­¢tokenè¿‡å¤§
            response_data = self._optimize_response_for_display(task_plan)
            if create_in_manager:
                response_data["manager_info"] = {
                    "tasks_created": created_count,
                    "creation_successful": True
                }

            return self._success_response(response_data)

        except Exception as e:
            self.logger.log_operation_end("execute_task_init", operation_id, success=False, error=str(e))
            self.logger.error(f"ä»»åŠ¡è®¡åˆ’ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=e)
            return self._error_response(f"Task initialization failed: {str(e)}")

    def _optimize_response_for_display(self, task_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–å“åº”æ˜¾ç¤ºï¼Œåªè¿”å›å‰10ä¸ªä»»åŠ¡è¯¦æƒ…ï¼Œé˜²æ­¢tokenè¿‡å¤§"""
        optimized = task_plan.copy()
        
        # ä¿ç•™å®Œæ•´çš„æ‘˜è¦ä¿¡æ¯
        task_plan_summary = optimized.get("task_plan", {})
        
        # å¯¹æ¯ä¸ªé˜¶æ®µåªæ˜¾ç¤ºå‰10ä¸ªä»»åŠ¡
        for phase_key in ["phase_1_files", "phase_2_architecture", "phase_3_project"]:
            if phase_key in optimized:
                phase_data = optimized[phase_key]
                tasks = phase_data.get("tasks", [])
                
                if len(tasks) > 10:
                    # åªä¿ç•™å‰10ä¸ªä»»åŠ¡è¯¦æƒ…
                    limited_tasks = tasks[:10]
                    phase_data["tasks"] = limited_tasks
                    
                    # æ·»åŠ è¯´æ˜ä¿¡æ¯
                    if "description" in phase_data:
                        total_count = len(tasks)
                        phase_data["description"] = f"{phase_data['description']} [æ˜¾ç¤ºå‰10ä¸ªï¼Œå…±{total_count}ä¸ªä»»åŠ¡]"
        
        # æ·»åŠ å®Œæ•´ä»»åŠ¡ä¿å­˜æç¤º
        optimized["display_notice"] = {
            "message": "ä¸ºé˜²æ­¢å“åº”è¿‡å¤§ï¼Œæ­¤å¤„åªæ˜¾ç¤ºå‰10ä¸ªä»»åŠ¡è¯¦æƒ…",
            "full_tasks_location": "å®Œæ•´ä»»åŠ¡åˆ—è¡¨å·²ä¿å­˜åˆ° .codelens/tasks.json",
            "total_tasks": task_plan_summary.get("total_tasks", 0)
        }
        
        return optimized

    def _success_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æˆåŠŸå“åº”"""
        self.logger.debug("ç”ŸæˆæˆåŠŸå“åº”", {"data_keys": list(data.keys()) if data else []})
        return {
            "success": True,
            "tool": self.tool_name,
            "data": data
        }

    def _error_response(self, message: str) -> Dict[str, Any]:
        """é”™è¯¯å“åº”"""
        self.logger.error("ç”Ÿæˆé”™è¯¯å“åº”", {"error_message": message})
        return {
            "success": False,
            "tool": self.tool_name,
            "error": message
        }


def create_mcp_tool() -> TaskInitTool:
    """åˆ›å»ºMCPå·¥å…·å®ä¾‹"""
    return TaskInitTool()


# å‘½ä»¤è¡Œæ¥å£ï¼Œç”¨äºæµ‹è¯•
def main():
    """å‘½ä»¤è¡Œæµ‹è¯•æ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="MCP task_init tool")
    parser.add_argument("project_path", help="Project path")
    parser.add_argument("--analysis-file", 
                        help="JSON file with analysis result from doc_guide (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾ .codelens/analysis.json)")
    parser.add_argument("--granularity", choices=["file", "batch", "module"],
                        default="file", help="Task granularity")
    parser.add_argument("--create-tasks", action="store_true",
                        help="Create tasks in task manager")

    args = parser.parse_args()

    # è‡ªåŠ¨æŸ¥æ‰¾åˆ†ææ–‡ä»¶
    analysis_file = args.analysis_file
    if not analysis_file:
        # å°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸‹æŸ¥æ‰¾ .codelens/analysis.json
        project_path = Path(args.project_path)
        auto_analysis_file = project_path / ".codelens" / "analysis.json"
        
        if auto_analysis_file.exists():
            analysis_file = str(auto_analysis_file)
            print(f"âœ… è‡ªåŠ¨æ‰¾åˆ°åˆ†ææ–‡ä»¶: {analysis_file}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°åˆ†ææ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ: python src/mcp_tools/doc_guide.py {args.project_path}")
            return

    # è¯»å–åˆ†æç»“æœ
    try:
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_result = json.load(f)
        print(f"ğŸ“Š æˆåŠŸè¯»å–åˆ†ææ–‡ä»¶: {analysis_file}")
    except Exception as e:
        print(f"âŒ è¯»å–åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
        return

    # æ„å»ºå‚æ•°
    arguments = {
        "project_path": args.project_path,
        "analysis_result": analysis_result,
        "task_granularity": args.granularity,
        "create_in_manager": args.create_tasks
    }

    # æ‰§è¡Œå·¥å…·
    tool = create_mcp_tool()
    result = tool.execute(arguments)

    # è¾“å‡ºç»“æœ
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
