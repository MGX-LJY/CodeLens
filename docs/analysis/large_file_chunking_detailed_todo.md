# CodeLenså¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†æ·±åº¦ç ”ç©¶Todoåˆ—è¡¨

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£åŒ…å«äº†CodeLenså¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆçš„30ä¸ªæ·±åº¦ç ”ç©¶ç‚¹ï¼Œæ¶µç›–æŠ€æœ¯å®ç°ã€æ€§èƒ½ä¼˜åŒ–ã€é”™è¯¯å¤„ç†ã€ç³»ç»Ÿé›†æˆç­‰å…³é”®æ–¹é¢ã€‚æ¯ä¸ªtodoé¡¹ç›®éƒ½éœ€è¦æ·±å…¥çš„æŠ€æœ¯åˆ†æå’Œè®¾è®¡æ€è€ƒã€‚

---

## ğŸ”¬ æŠ€æœ¯æ ¸å¿ƒæ·±åº¦åˆ†æ

### 1. æ·±åº¦åˆ†æASTè§£æåœ¨ä¸åŒPythonè¯­æ³•ä¸‹çš„è¾¹ç•Œæƒ…å†µ
**ç ”ç©¶é‡ç‚¹ï¼š**
- Python 3.8+ çš„æµ·è±¡æ“ä½œç¬¦ `:=` å¤„ç†
- f-stringå¤æ‚è¡¨è¾¾å¼çš„ASTèŠ‚ç‚¹åˆ†æ
- async/awaitè¯­æ³•ç³–åœ¨ç±»æ–¹æ³•ä¸­çš„åµŒå¥—å¤„ç†
- åŠ¨æ€å±æ€§è®¿é—® `getattr/setattr` çš„é™æ€åˆ†æé™åˆ¶
- ç±»å‹æ³¨è§£ï¼ˆType Hintsï¼‰çš„ASTèŠ‚ç‚¹æå–å’Œä¿ç•™

**æŠ€æœ¯æŒ‘æˆ˜ï¼š**
```python
# å¤æ‚caseç¤ºä¾‹
class DataProcessor:
    async def process(self, data: List[Dict[str, Any]]) -> AsyncIterator[Result]:
        if (n := len(data)) > 1000:  # æµ·è±¡æ“ä½œç¬¦
            async for item in self.batch_process(data):
                yield f"Processed: {item.name!r}"  # f-string with repr
```

**é¢„æœŸäº§å‡ºï¼š** ASTè§£æå™¨å¢å¼ºæ–¹æ¡ˆå’Œè¾¹ç•Œæƒ…å†µå¤„ç†ç­–ç•¥

---

### 2. è®¾è®¡åˆ†ç‰‡ä¾èµ–å…³ç³»æ£€æµ‹æœºåˆ¶ï¼ˆç±»ç»§æ‰¿ã€æ–¹æ³•è°ƒç”¨ç­‰ï¼‰
**ç ”ç©¶é‡ç‚¹ï¼š**
- ç±»ç»§æ‰¿å…³ç³»çš„è·¨åˆ†ç‰‡è¿½è¸ª
- æ–¹æ³•è°ƒç”¨é“¾çš„ä¾èµ–åˆ†æ
- å¾ªç¯ä¾èµ–çš„æ£€æµ‹å’Œå¤„ç†
- åŠ¨æ€å¯¼å…¥æ¨¡å—çš„ä¾èµ–è§£æ
- Mixinæ¨¡å¼å’Œå¤šé‡ç»§æ‰¿çš„å¤æ‚æ€§åˆ†æ

**æŠ€æœ¯æ–¹æ¡ˆï¼š**
```python
class DependencyTracker:
    def analyze_cross_chunk_dependencies(self, chunks: List[CodeChunk]) -> Dict[str, Set[str]]:
        """åˆ†æåˆ†ç‰‡é—´çš„ä¾èµ–å…³ç³»"""
        dependencies = {}
        
        # 1. ç±»ç»§æ‰¿å…³ç³»åˆ†æ
        inheritance_map = self._build_inheritance_map(chunks)
        
        # 2. æ–¹æ³•è°ƒç”¨åˆ†æ  
        call_graph = self._build_call_graph(chunks)
        
        # 3. å˜é‡å¼•ç”¨åˆ†æ
        reference_map = self._analyze_variable_references(chunks)
        
        return self._merge_dependency_maps(inheritance_map, call_graph, reference_map)
```

**é¢„æœŸäº§å‡ºï¼š** ä¾èµ–å…³ç³»æ£€æµ‹å¼•æ“å’Œåˆ†ç‰‡æ’åºç®—æ³•

---

### 3. ç ”ç©¶è£…é¥°å™¨ã€å…ƒç±»ç­‰é«˜çº§Pythonç‰¹æ€§çš„åˆ†ç‰‡å¤„ç†
**ç ”ç©¶é‡ç‚¹ï¼š**
- è£…é¥°å™¨çš„è¯­ä¹‰å®Œæ•´æ€§ä¿æŒ
- å…ƒç±»å®šä¹‰çš„åˆ†ç‰‡è¾¹ç•Œç¡®å®š
- propertyè£…é¥°å™¨çš„getter/setteråˆ†ç»„
- ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„`__enter__/__exit__`ä¿æŒ
- æè¿°ç¬¦åè®®çš„å®Œæ•´æ€§å¤„ç†

**å¤æ‚ç¤ºä¾‹ï¼š**
```python
class MetaClass(type):
    def __new__(cls, name, bases, attrs):
        # å…ƒç±»é€»è¾‘...
        return super().__new__(cls, name, bases, attrs)

@dataclass
@total_ordering  
class ComplexClass(metaclass=MetaClass):
    @property
    @lru_cache(maxsize=128)
    def expensive_property(self) -> Any:
        return self._compute_value()
    
    @expensive_property.setter
    def expensive_property(self, value: Any) -> None:
        self._cached_value = value
```

**é¢„æœŸäº§å‡ºï¼š** é«˜çº§ç‰¹æ€§æ„ŸçŸ¥çš„åˆ†ç‰‡ç­–ç•¥å’Œè¯­ä¹‰ä¿æŒæ–¹æ¡ˆ

---

### 4. åˆ†æä»£ç æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²çš„åˆ†é…ç­–ç•¥
**ç ”ç©¶é‡ç‚¹ï¼š**
- docstringä¸å…¶å¯¹åº”å‡½æ•°/ç±»çš„ç»‘å®šä¿æŒ
- è¡Œå†…æ³¨é‡Šçš„å½’å±åˆ¤æ–­é€»è¾‘
- å¤šè¡Œæ³¨é‡Šå—çš„åˆ†ç‰‡è¾¹ç•Œå¤„ç†
- TODO/FIXME/NOTEç­‰ç‰¹æ®Šæ³¨é‡Šçš„æå–
- æ³¨é‡Šçš„è¯­ä¹‰åˆ†æå’Œé‡è¦æ€§è¯„çº§

**åˆ†é…ç­–ç•¥ï¼š**
```python
def analyze_comment_allocation(self, code_chunk: str, ast_nodes: List[ast.AST]) -> CommentAllocation:
    """
    æ³¨é‡Šåˆ†é…ç­–ç•¥ï¼š
    1. å‡½æ•°/ç±»ä¸Šæ–¹çš„æ³¨é‡Š -> åˆ†é…ç»™è¯¥å‡½æ•°/ç±»
    2. è¡Œå°¾æ³¨é‡Š -> åˆ†é…ç»™è¯¥è¡Œä»£ç 
    3. ç‹¬ç«‹æ³¨é‡Šå— -> åˆ†é…ç»™æœ€è¿‘çš„ä»£ç å—
    4. æ¨¡å—çº§docstring -> ä¿ç•™åœ¨æ¨¡å—çº§åˆ†ç‰‡ä¸­
    5. é‡è¦æ³¨é‡Šï¼ˆTODOç­‰ï¼‰ -> åœ¨å¤šä¸ªç›¸å…³åˆ†ç‰‡ä¸­é‡å¤
    """
```

**é¢„æœŸäº§å‡ºï¼š** æ™ºèƒ½æ³¨é‡Šåˆ†é…ç®—æ³•å’Œæ–‡æ¡£å®Œæ•´æ€§ä¿è¯æœºåˆ¶

---

### 5. è®¾è®¡åˆ†ç‰‡å¤§å°åŠ¨æ€è°ƒæ•´ç®—æ³•ï¼ˆåŸºäºå¤æ‚åº¦è¯„ä¼°ï¼‰
**ç ”ç©¶é‡ç‚¹ï¼š**
- ä»£ç å¤æ‚åº¦åº¦é‡æ ‡å‡†ï¼ˆåœˆå¤æ‚åº¦ã€è®¤çŸ¥å¤æ‚åº¦ï¼‰
- åˆ†ç‰‡å¤§å°çš„åŠ¨æ€è°ƒæ•´é˜ˆå€¼
- åŸºäºClaude Codeç†è§£èƒ½åŠ›çš„æœ€ä¼˜åˆ†ç‰‡å¤§å°
- ä¸åŒè¯­è¨€çš„å¤æ‚åº¦æƒé‡è°ƒæ•´
- å¤æ‚åº¦ä¸æ–‡æ¡£è´¨é‡çš„ç›¸å…³æ€§åˆ†æ

**ç®—æ³•è®¾è®¡ï¼š**
```python
class ComplexityBasedChunker:
    def calculate_chunk_complexity(self, chunk: CodeChunk) -> float:
        """
        å¤æ‚åº¦è¯„ä¼°å› å­ï¼š
        - åµŒå¥—å±‚æ•°: weight=2.0
        - å¾ªç¯ç»“æ„: weight=1.5  
        - æ¡ä»¶åˆ†æ”¯: weight=1.2
        - å‡½æ•°è°ƒç”¨é“¾: weight=1.0
        - ç±»ç»§æ‰¿æ·±åº¦: weight=1.8
        """
        complexity_score = 0.0
        
        # ASTéå†è®¡ç®—å„ç§å¤æ‚åº¦å› å­
        for node in ast.walk(ast.parse(chunk.content)):
            if isinstance(node, ast.For):
                complexity_score += 1.5 * self._get_nesting_depth(node)
            elif isinstance(node, ast.If):
                complexity_score += 1.2 * len(node.orelse)
            # ... æ›´å¤šå¤æ‚åº¦è®¡ç®—
            
        return complexity_score
    
    def adjust_chunk_size(self, base_chunk: CodeChunk) -> List[CodeChunk]:
        """åŸºäºå¤æ‚åº¦åŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°"""
        complexity = self.calculate_chunk_complexity(base_chunk)
        
        if complexity > self.HIGH_COMPLEXITY_THRESHOLD:
            return self._split_chunk_further(base_chunk)
        elif complexity < self.LOW_COMPLEXITY_THRESHOLD:
            return self._try_merge_with_neighbors(base_chunk)
        else:
            return [base_chunk]
```

**é¢„æœŸäº§å‡ºï¼š** è‡ªé€‚åº”åˆ†ç‰‡å¤§å°ç®—æ³•å’Œå¤æ‚åº¦è¯„ä¼°ä½“ç³»

---

### 6. å®ç°æ™ºèƒ½å¯¼å…¥è¯­å¥ä¼˜åŒ–å’Œå»é‡æœºåˆ¶
**ç ”ç©¶é‡ç‚¹ï¼š**
- è·¨åˆ†ç‰‡çš„é‡å¤importæ£€æµ‹
- unused importçš„æ™ºèƒ½æ¸…ç†
- ç›¸å¯¹å¯¼å…¥vsç»å¯¹å¯¼å…¥çš„ç»Ÿä¸€åŒ–
- å¾ªç¯å¯¼å…¥çš„æ£€æµ‹å’Œè­¦å‘Š
- å¯¼å…¥è¯­å¥çš„æœ€ä¼˜æ’åºç­–ç•¥

**ä¼˜åŒ–æœºåˆ¶ï¼š**
```python
class ImportOptimizer:
    def optimize_imports_across_chunks(self, chunks: List[CodeChunk]) -> List[CodeChunk]:
        """è·¨åˆ†ç‰‡å¯¼å…¥ä¼˜åŒ–"""
        
        # 1. æ”¶é›†æ‰€æœ‰å¯¼å…¥è¯­å¥
        all_imports = self._collect_all_imports(chunks)
        
        # 2. åˆ†æå®é™…ä½¿ç”¨æƒ…å†µ  
        usage_analysis = self._analyze_import_usage(chunks)
        
        # 3. æ£€æµ‹å¾ªç¯å¯¼å…¥
        circular_imports = self._detect_circular_imports(all_imports)
        
        # 4. ç”Ÿæˆæœ€ä¼˜å¯¼å…¥é›†åˆ
        optimized_imports = self._generate_optimal_imports(
            all_imports, usage_analysis, circular_imports
        )
        
        # 5. æ›´æ–°å„åˆ†ç‰‡çš„å¯¼å…¥è¯­å¥
        return self._update_chunk_imports(chunks, optimized_imports)
```

**é¢„æœŸäº§å‡ºï¼š** å¯¼å…¥è¯­å¥æ™ºèƒ½ä¼˜åŒ–å¼•æ“å’Œå»é‡ç®—æ³•

---

### 7. è®¾è®¡å†…å­˜æ•ˆç‡çš„æµå¼ASTè§£ææ–¹æ¡ˆ
**ç ”ç©¶é‡ç‚¹ï¼š**
- å¤§æ–‡ä»¶çš„åˆ†å—è¯»å–å’Œè§£æç­–ç•¥
- ASTèŠ‚ç‚¹çš„å»¶è¿ŸåŠ è½½æœºåˆ¶
- å†…å­˜ä½¿ç”¨é‡çš„å®æ—¶ç›‘æ§
- åƒåœ¾å›æ”¶çš„ä¸»åŠ¨è§¦å‘æ—¶æœº
- è§£æä¸­æ–­å’Œæ¢å¤çš„çŠ¶æ€ä¿å­˜

**æµå¼è§£ææ¶æ„ï¼š**
```python
class StreamingASTParser:
    def __init__(self, memory_limit_mb: int = 512):
        self.memory_limit = memory_limit_mb * 1024 * 1024
        self.current_memory_usage = 0
        self.parse_state = ParseState()
    
    def parse_file_streaming(self, file_path: str) -> Iterator[ast.AST]:
        """æµå¼è§£æå¤§æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            buffer = ""
            line_count = 0
            
            for line in f:
                buffer += line
                line_count += 1
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨é‡
                if self._check_memory_usage():
                    # å°è¯•è§£æå½“å‰buffer
                    if self._can_parse_safely(buffer):
                        yield from self._parse_buffer_chunk(buffer)
                        buffer = self._get_continuation_context()
                        self._trigger_gc_if_needed()
                
                # æŒ‰è¯­æ³•è¾¹ç•Œåˆ†å—
                if self._is_syntax_boundary(line) and len(buffer) > 8192:
                    yield from self._parse_buffer_chunk(buffer)
                    buffer = ""
```

**é¢„æœŸäº§å‡ºï¼š** å†…å­˜é«˜æ•ˆçš„æµå¼è§£æå¼•æ“å’Œç›‘æ§ç³»ç»Ÿ

---

### 8. ç ”ç©¶TypeScriptå¤æ‚ç±»å‹ç³»ç»Ÿçš„åˆ†ç‰‡å¤„ç†ç­–ç•¥
**ç ”ç©¶é‡ç‚¹ï¼š**
- æ³›å‹çº¦æŸçš„è·¨åˆ†ç‰‡ä¿æŒ
- è”åˆç±»å‹å’Œäº¤å‰ç±»å‹çš„å®Œæ•´æ€§
- æ¡ä»¶ç±»å‹å’Œæ˜ å°„ç±»å‹çš„å¤„ç†
- æ¨¡å—å£°æ˜åˆå¹¶çš„åˆ†ç‰‡è¾¹ç•Œ
- è£…é¥°å™¨å…ƒæ•°æ®çš„ä¿ç•™ç­–ç•¥

**TypeScriptç‰¹æ®Šå¤„ç†ï¼š**
```typescript
// å¤æ‚TypeScriptç¤ºä¾‹
interface BaseConfig<T extends Record<string, any>> {
  data: T;
  transform: <U>(input: T) => U;
}

type ConditionalType<T> = T extends string 
  ? StringProcessor<T>
  : T extends number 
  ? NumberProcessor<T> 
  : DefaultProcessor<T>;

class GenericProcessor<
  T extends BaseConfig<any>,
  U = ConditionalType<T['data']>
> implements Processor<T, U> {
  // å®ç°...
}
```

**å¤„ç†ç­–ç•¥ï¼š**
```python
class TypeScriptChunker(BaseChunker):
    def handle_complex_types(self, content: str) -> List[TypeDefinition]:
        """å¤„ç†å¤æ‚TypeScriptç±»å‹å®šä¹‰"""
        
        # 1. è§£æç±»å‹å®šä¹‰å’Œçº¦æŸ
        type_definitions = self._parse_type_definitions(content)
        
        # 2. åˆ†æç±»å‹ä¾èµ–å…³ç³»
        type_dependencies = self._analyze_type_dependencies(type_definitions)
        
        # 3. ç¡®ä¿ç±»å‹å®Œæ•´æ€§
        complete_type_chunks = self._ensure_type_completeness(
            type_definitions, type_dependencies
        )
        
        return complete_type_chunks
```

**é¢„æœŸäº§å‡ºï¼š** TypeScriptç±»å‹ç³»ç»Ÿæ„ŸçŸ¥çš„åˆ†ç‰‡å¤„ç†å¼•æ“

---

### 9. åˆ†æJavaæ³›å‹ã€åå°„ã€æ³¨è§£å¯¹åˆ†ç‰‡çš„å½±å“
**ç ”ç©¶é‡ç‚¹ï¼š**
- æ³›å‹è¾¹ç•Œçš„å®Œæ•´æ€§ä¿æŒ
- æ³¨è§£å¤„ç†å™¨ç›¸å…³ä»£ç çš„åˆ†ç»„
- åå°„APIè°ƒç”¨çš„ä¸Šä¸‹æ–‡ä¿æŒ
- Springç­‰æ¡†æ¶æ³¨è§£çš„è¯­ä¹‰åˆ†æ
- è¿è¡Œæ—¶ç±»å‹ä¿¡æ¯çš„ä¿ç•™ç­–ç•¥

**Javaå¤æ‚ç‰¹æ€§å¤„ç†ï¼š**
```java
@Entity
@Table(name = "users")
@JsonIgnoreProperties(ignoreUnknown = true)
public class User<T extends Serializable & Comparable<T>> {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    @NotNull
    @Size(min = 2, max = 50)
    private String name;
    
    // æ³›å‹æ–¹æ³•withå¤æ‚çº¦æŸ
    public <U extends T & Cloneable> List<U> processData(
        Class<U> clazz, 
        Function<T, U> transformer
    ) throws ReflectiveOperationException {
        // åå°„é€»è¾‘...
    }
}
```

**åˆ†ç‰‡å¤„ç†ç­–ç•¥ï¼š**
```python
class JavaChunker(BaseChunker):
    def analyze_annotation_groups(self, class_node: JavaClass) -> List[AnnotationGroup]:
        """åˆ†ææ³¨è§£åˆ†ç»„"""
        groups = []
        
        # JPAç›¸å…³æ³¨è§£åˆ†ç»„
        jpa_annotations = self._extract_jpa_annotations(class_node)
        if jpa_annotations:
            groups.append(AnnotationGroup("jpa", jpa_annotations))
        
        # éªŒè¯ç›¸å…³æ³¨è§£åˆ†ç»„  
        validation_annotations = self._extract_validation_annotations(class_node)
        if validation_annotations:
            groups.append(AnnotationGroup("validation", validation_annotations))
            
        return groups
```

**é¢„æœŸäº§å‡ºï¼š** Javaä¼ä¸šçº§ç‰¹æ€§æ„ŸçŸ¥çš„åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆ

---

### 10. è®¾è®¡C++æ¨¡æ¿ç‰¹åŒ–å’Œå‘½åç©ºé—´çš„å¤„ç†æ–¹æ¡ˆ
**ç ”ç©¶é‡ç‚¹ï¼š**
- æ¨¡æ¿ç‰¹åŒ–çš„å®Œæ•´å®šä¹‰ä¿æŒ
- å‘½åç©ºé—´çš„å±‚æ¬¡ç»“æ„ç»´æŠ¤
- SFINAEæŠ€æœ¯çš„ä¸Šä¸‹æ–‡ä¿æŒ
- å¤´æ–‡ä»¶å’Œå®ç°æ–‡ä»¶çš„å…³è”å¤„ç†
- é¢„å¤„ç†å™¨å®çš„å±•å¼€å’Œä¿ç•™

**C++å¤æ‚ç‰¹æ€§ï¼š**
```cpp
namespace detail {
    template<typename T, typename = void>
    struct has_iterator : std::false_type {};
    
    template<typename T>
    struct has_iterator<T, std::void_t<typename T::iterator>> 
        : std::true_type {};
}

template<typename Container>
class DataProcessor {
public:
    template<typename T = Container>
    std::enable_if_t<detail::has_iterator<T>::value, void>
    process(const T& container) {
        // å¤„ç†å¯è¿­ä»£å®¹å™¨
    }
    
    template<typename T = Container>
    std::enable_if_t<!detail::has_iterator<T>::value, void>
    process(const T& item) {
        // å¤„ç†å•ä¸ªå…ƒç´ 
    }
};
```

**é¢„æœŸäº§å‡ºï¼š** C++æ¨¡æ¿å…ƒç¼–ç¨‹æ„ŸçŸ¥çš„åˆ†ç‰‡å¤„ç†å¼•æ“

---

### 11. ç ”ç©¶Go interfaceå’Œgoroutineç›¸å…³ä»£ç çš„åˆ†ç‰‡ç­–ç•¥
**ç ”ç©¶é‡ç‚¹ï¼š**
- interfaceå®šä¹‰å’Œå®ç°çš„å…³è”ä¿æŒ
- goroutineç›¸å…³ä»£ç çš„ä¸Šä¸‹æ–‡åˆ†ç»„
- channelæ“ä½œçš„è¯­ä¹‰å®Œæ•´æ€§
- context.Contextä¼ æ’­çš„åˆ†æ
- å¹¶å‘å®‰å…¨æ€§ç›¸å…³ä»£ç çš„æ ‡è¯†

**Goå¹¶å‘æ¨¡å¼å¤„ç†ï¼š**
```go
type DataProcessor interface {
    Process(ctx context.Context, data <-chan Data) <-chan Result
    Close() error
}

type processor struct {
    workers   int
    workChan  chan work
    resultChan chan Result
}

func (p *processor) Process(ctx context.Context, data <-chan Data) <-chan Result {
    result := make(chan Result, p.workers)
    
    // å¯åŠ¨å·¥ä½œgoroutine
    for i := 0; i < p.workers; i++ {
        go p.worker(ctx, data, result)
    }
    
    // ç›‘æ§goroutine
    go func() {
        defer close(result)
        // ç­‰å¾…å¤„ç†å®Œæˆ...
    }()
    
    return result
}
```

**åˆ†ç‰‡ç­–ç•¥ï¼š**
```python
class GoChunker(BaseChunker):
    def identify_concurrency_groups(self, content: str) -> List[ConcurrencyGroup]:
        """è¯†åˆ«å¹¶å‘ç›¸å…³çš„ä»£ç ç»„"""
        groups = []
        
        # 1. è¯†åˆ«interfaceå’Œå®ç°
        interfaces = self._find_interfaces(content)
        implementations = self._find_implementations(content, interfaces)
        
        # 2. è¯†åˆ«goroutineæ¨¡å¼
        goroutine_patterns = self._find_goroutine_patterns(content)
        
        # 3. è¯†åˆ«channelæ“ä½œ
        channel_operations = self._find_channel_operations(content)
        
        return self._group_related_concurrent_code(
            interfaces, implementations, goroutine_patterns, channel_operations
        )
```

**é¢„æœŸäº§å‡ºï¼š** Goå¹¶å‘ç¼–ç¨‹æ„ŸçŸ¥çš„åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆ

---

### 12. è®¾è®¡Rustæ‰€æœ‰æƒç³»ç»Ÿå’Œç”Ÿå‘½å‘¨æœŸçš„åˆ†ææ–¹æ³•
**ç ”ç©¶é‡ç‚¹ï¼š**
- ç”Ÿå‘½å‘¨æœŸæ ‡æ³¨çš„å®Œæ•´æ€§ä¿æŒ
- å€Ÿç”¨æ£€æŸ¥å™¨ç›¸å…³ä»£ç çš„åˆ†ç»„
- unsafeå—çš„ä¸Šä¸‹æ–‡ä¿æŒ
- traitå¯¹è±¡çš„åˆ†ç‰‡è¾¹ç•Œç¡®å®š
- å®å®šä¹‰å’Œä½¿ç”¨çš„å…³è”ç»´æŠ¤

**Rustå¤æ‚æ‰€æœ‰æƒæ¨¡å¼ï¼š**
```rust
trait DataProcessor<'a> {
    type Output: Send + 'a;
    
    fn process<'b>(&'b self, data: &'a [u8]) -> Self::Output 
    where 'a: 'b;
}

struct AsyncProcessor<'a, T> 
where 
    T: Send + Sync + 'static 
{
    buffer: &'a mut [u8],
    callback: Box<dyn Fn(&T) -> bool + Send + 'a>,
}

impl<'a, T> AsyncProcessor<'a, T> 
where 
    T: Send + Sync + 'static 
{
    unsafe fn process_unchecked(&mut self, data: *const T) -> Result<(), Error> {
        // unsafeæ“ä½œéœ€è¦ç‰¹æ®Šå¤„ç†
    }
}
```

**åˆ†ææ–¹æ¡ˆï¼š**
```python
class RustChunker(BaseChunker):
    def analyze_ownership_patterns(self, content: str) -> OwnershipAnalysis:
        """åˆ†æRustæ‰€æœ‰æƒæ¨¡å¼"""
        
        # 1. ç”Ÿå‘½å‘¨æœŸå‚æ•°åˆ†æ
        lifetime_params = self._extract_lifetime_parameters(content)
        
        # 2. å€Ÿç”¨å…³ç³»åˆ†æ
        borrow_relationships = self._analyze_borrow_relationships(content)
        
        # 3. unsafeå—è¯†åˆ«
        unsafe_blocks = self._identify_unsafe_blocks(content)
        
        # 4. trait boundsåˆ†æ
        trait_bounds = self._analyze_trait_bounds(content)
        
        return OwnershipAnalysis(
            lifetime_params, borrow_relationships, 
            unsafe_blocks, trait_bounds
        )
```

**é¢„æœŸäº§å‡ºï¼š** Rustæ‰€æœ‰æƒç³»ç»Ÿæ„ŸçŸ¥çš„åˆ†ç‰‡å¤„ç†å¼•æ“

---

### 13. å®ç°è·¨è¯­è¨€ä»£ç æ··åˆæ–‡ä»¶çš„æ™ºèƒ½è¯†åˆ«å’Œå¤„ç†
**ç ”ç©¶é‡ç‚¹ï¼š**
- HTMLä¸­åµŒå…¥çš„JavaScript/CSSä»£ç æå–
- Jupyter Notebookçš„å¤šè¯­è¨€cellå¤„ç†
- Markdownä¸­çš„ä»£ç å—è¯†åˆ«å’Œåˆ†ç±»
- æ¨¡æ¿å¼•æ“ä¸­çš„ä»£ç ç‰‡æ®µå¤„ç†
- é…ç½®æ–‡ä»¶ä¸­çš„è„šæœ¬ä»£ç è¯†åˆ«

**æ··åˆæ–‡ä»¶å¤„ç†ï¼š**
```python
class MultiLanguageFileHandler:
    def detect_embedded_languages(self, file_path: str) -> List[EmbeddedLanguage]:
        """æ£€æµ‹æ–‡ä»¶ä¸­çš„åµŒå…¥è¯­è¨€"""
        
        file_ext = Path(file_path).suffix
        content = self._read_file_content(file_path)
        
        if file_ext == '.html':
            return self._extract_from_html(content)
        elif file_ext == '.ipynb':
            return self._extract_from_jupyter(content) 
        elif file_ext == '.md':
            return self._extract_from_markdown(content)
        elif file_ext in ['.vue', '.svelte']:
            return self._extract_from_component(content)
        else:
            return []
    
    def _extract_from_html(self, html_content: str) -> List[EmbeddedLanguage]:
        """ä»HTMLä¸­æå–JavaScriptå’ŒCSS"""
        languages = []
        
        # æå–<script>æ ‡ç­¾ä¸­çš„JavaScript
        js_blocks = self._extract_script_tags(html_content)
        for block in js_blocks:
            languages.append(EmbeddedLanguage(
                language='javascript',
                content=block.content,
                start_line=block.start_line,
                end_line=block.end_line,
                context='html_script'
            ))
        
        # æå–<style>æ ‡ç­¾ä¸­çš„CSS
        css_blocks = self._extract_style_tags(html_content)
        for block in css_blocks:
            languages.append(EmbeddedLanguage(
                language='css',
                content=block.content,
                start_line=block.start_line,
                end_line=block.end_line,
                context='html_style'
            ))
            
        return languages
```

**é¢„æœŸäº§å‡ºï¼š** å¤šè¯­è¨€æ··åˆæ–‡ä»¶çš„æ™ºèƒ½å¤„ç†å¼•æ“

---

## âš ï¸ é”™è¯¯å¤„ç†ä¸å®¹é”™æ€§

### 14. è®¾è®¡åˆ†ç‰‡å¤„ç†å¤±è´¥æ—¶çš„é™çº§å’Œæ¢å¤æœºåˆ¶
**ç ”ç©¶é‡ç‚¹ï¼š**
- éƒ¨åˆ†åˆ†ç‰‡å¤±è´¥æ—¶çš„å¤„ç†ç­–ç•¥
- è‡ªåŠ¨é™çº§åˆ°ç®€å•åˆ†ç‰‡æ¨¡å¼
- å¤„ç†å¤±è´¥çš„è¯¦ç»†é”™è¯¯æŠ¥å‘Š
- ç”¨æˆ·æ‰‹åŠ¨å¹²é¢„çš„æ¥å£è®¾è®¡
- å¤±è´¥æ¢å¤çš„çŠ¶æ€ä¿å­˜å’Œæ¢å¤

**é™çº§æœºåˆ¶ï¼š**
```python
class ChunkingFailureHandler:
    def __init__(self):
        self.fallback_strategies = [
            self._try_ast_based_chunking,      # ç­–ç•¥1: ASTåˆ†æ
            self._try_regex_based_chunking,    # ç­–ç•¥2: æ­£åˆ™è¡¨è¾¾å¼
            self._try_line_based_chunking,     # ç­–ç•¥3: åŸºäºè¡Œæ•°
            self._try_size_based_chunking      # ç­–ç•¥4: åŸºäºæ–‡ä»¶å¤§å°
        ]
    
    def handle_chunking_failure(self, file_path: str, error: Exception) -> ChunkingResult:
        """å¤„ç†åˆ†ç‰‡å¤±è´¥çš„é™çº§æœºåˆ¶"""
        
        self._log_failure(file_path, error)
        
        for i, strategy in enumerate(self.fallback_strategies):
            try:
                result = strategy(file_path)
                if result.is_successful():
                    self._log_successful_fallback(file_path, strategy_level=i)
                    return result
            except Exception as fallback_error:
                self._log_fallback_failure(file_path, i, fallback_error)
                continue
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åŸºæœ¬çš„å¤„ç†ç»“æœ
        return self._create_minimal_result(file_path)
    
    def _create_minimal_result(self, file_path: str) -> ChunkingResult:
        """åˆ›å»ºæœ€å°åŒ–çš„å¤„ç†ç»“æœ"""
        return ChunkingResult(
            chunks=[CodeChunk.create_whole_file_chunk(file_path)],
            processing_method="minimal_fallback",
            success=True,
            warnings=["Advanced chunking failed, using whole file as single chunk"]
        )
```

**é¢„æœŸäº§å‡ºï¼š** å¤šå±‚æ¬¡é™çº§æœºåˆ¶å’Œæ•…éšœæ¢å¤ç³»ç»Ÿ

---

### 15. åˆ†æè¯­æ³•é”™è¯¯ä»£ç çš„éƒ¨åˆ†è§£æå’Œå®¹é”™å¤„ç†
**ç ”ç©¶é‡ç‚¹ï¼š**
- è¯­æ³•é”™è¯¯çš„ç²¾ç¡®å®šä½å’Œéš”ç¦»
- å¯è§£æéƒ¨åˆ†çš„æå–å’Œå¤„ç†
- é”™è¯¯ä¿®å¤å»ºè®®çš„ç”Ÿæˆ
- éƒ¨åˆ†ASTçš„æ„å»ºç­–ç•¥
- é”™è¯¯ä»£ç çš„æ–‡æ¡£ç”Ÿæˆæ–¹æ¡ˆ

**å®¹é”™è§£æå™¨ï¼š**
```python
class FaultTolerantParser:
    def parse_with_errors(self, content: str) -> PartialParseResult:
        """å®¹é”™è§£æåŒ…å«è¯­æ³•é”™è¯¯çš„ä»£ç """
        
        try:
            # å°è¯•å®Œæ•´è§£æ
            ast_tree = ast.parse(content)
            return PartialParseResult(ast_tree, errors=[])
            
        except SyntaxError as e:
            # è¯­æ³•é”™è¯¯æ—¶ï¼Œå°è¯•éƒ¨åˆ†è§£æ
            return self._parse_partially(content, e)
    
    def _parse_partially(self, content: str, syntax_error: SyntaxError) -> PartialParseResult:
        """éƒ¨åˆ†è§£æç­–ç•¥"""
        
        lines = content.split('\n')
        error_line = syntax_error.lineno - 1
        
        # ç­–ç•¥1: è·³è¿‡é”™è¯¯è¡Œï¼Œè§£æå…¶ä½™éƒ¨åˆ†
        clean_lines = lines[:error_line] + ['# SYNTAX ERROR SKIPPED'] + lines[error_line + 1:]
        try:
            partial_ast = ast.parse('\n'.join(clean_lines))
            return PartialParseResult(
                partial_ast, 
                errors=[syntax_error],
                skipped_lines=[error_line]
            )
        except:
            # ç­–ç•¥2: é€å‡½æ•°/é€ç±»å°è¯•è§£æ
            return self._parse_function_by_function(lines, syntax_error)
    
    def _parse_function_by_function(self, lines: List[str], original_error: SyntaxError) -> PartialParseResult:
        """é€å‡½æ•°è§£æç­–ç•¥"""
        parsed_functions = []
        errors = [original_error]
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°å‡½æ•°å®šä¹‰
        function_ranges = self._find_function_ranges(lines)
        
        for start, end in function_ranges:
            function_code = '\n'.join(lines[start:end])
            try:
                func_ast = ast.parse(function_code)
                parsed_functions.append(func_ast)
            except SyntaxError as func_error:
                errors.append(func_error)
                # è®°å½•æ— æ³•è§£æçš„å‡½æ•°
        
        # æ„å»ºéƒ¨åˆ†AST
        partial_ast = self._build_partial_ast(parsed_functions)
        return PartialParseResult(partial_ast, errors)
```

**é¢„æœŸäº§å‡ºï¼š** è¯­æ³•é”™è¯¯å®¹é”™çš„éƒ¨åˆ†è§£æå¼•æ“

---

### 16. è®¾è®¡åˆ†ç‰‡åˆå¹¶æ—¶çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ£€æŸ¥
**ç ”ç©¶é‡ç‚¹ï¼š**
- åˆ†ç‰‡é—´å¼•ç”¨çš„å®Œæ•´æ€§éªŒè¯
- æ–‡æ¡£ç»“æ„çš„é€»è¾‘è¿è´¯æ€§æ£€æŸ¥
- é‡å¤å†…å®¹çš„æ£€æµ‹å’Œå¤„ç†
- ç¼ºå¤±ä¾èµ–çš„è‡ªåŠ¨è¡¥å…¨
- åˆå¹¶ç»“æœçš„è´¨é‡è¯„ä¼°

**ä¸€è‡´æ€§æ£€æŸ¥å™¨ï¼š**
```python
class ConsistencyChecker:
    def check_merge_consistency(self, chunk_docs: List[ChunkDocument]) -> ConsistencyReport:
        """æ£€æŸ¥åˆå¹¶æ–‡æ¡£çš„ä¸€è‡´æ€§"""
        
        report = ConsistencyReport()
        
        # 1. æ£€æŸ¥å¼•ç”¨å®Œæ•´æ€§
        reference_issues = self._check_reference_completeness(chunk_docs)
        report.add_issues("references", reference_issues)
        
        # 2. æ£€æŸ¥æ–‡æ¡£ç»“æ„
        structure_issues = self._check_document_structure(chunk_docs)
        report.add_issues("structure", structure_issues)
        
        # 3. æ£€æŸ¥å†…å®¹é‡å¤
        duplication_issues = self._check_content_duplication(chunk_docs)
        report.add_issues("duplication", duplication_issues)
        
        # 4. æ£€æŸ¥ç¼ºå¤±ä¾èµ–
        missing_deps = self._check_missing_dependencies(chunk_docs)
        report.add_issues("missing_deps", missing_deps)
        
        return report
    
    def _check_reference_completeness(self, chunk_docs: List[ChunkDocument]) -> List[ReferenceIssue]:
        """æ£€æŸ¥å¼•ç”¨å®Œæ•´æ€§"""
        issues = []
        all_definitions = set()
        all_references = set()
        
        # æ”¶é›†æ‰€æœ‰å®šä¹‰å’Œå¼•ç”¨
        for doc in chunk_docs:
            all_definitions.update(doc.definitions)
            all_references.update(doc.references)
        
        # æ‰¾åˆ°æœªå®šä¹‰çš„å¼•ç”¨
        undefined_refs = all_references - all_definitions
        for ref in undefined_refs:
            issues.append(ReferenceIssue(
                type="undefined_reference",
                reference=ref,
                message=f"Reference '{ref}' is used but not defined in any chunk"
            ))
        
        return issues
```

**é¢„æœŸäº§å‡ºï¼š** æ–‡æ¡£åˆå¹¶ä¸€è‡´æ€§æ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤ç³»ç»Ÿ

---

## âš¡ æ€§èƒ½ä¸ç”¨æˆ·ä½“éªŒ

### 17. å®ç°åˆ†ç‰‡å¤„ç†è¿›åº¦çš„å®æ—¶åé¦ˆå’Œå¯è§†åŒ–
**ç ”ç©¶é‡ç‚¹ï¼š**
- å¤„ç†è¿›åº¦çš„ç²¾ç¡®è®¡ç®—å’ŒæŠ¥å‘Š
- å®æ—¶è¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
- å¤„ç†æ—¶é—´çš„é¢„ä¼°ç®—æ³•
- ç”¨æˆ·å‹å¥½çš„è¿›åº¦ä¿¡æ¯å±•ç¤º
- å¤„ç†ä¸­æ–­å’Œæ¢å¤çš„ç”¨æˆ·ç•Œé¢

**è¿›åº¦åé¦ˆç³»ç»Ÿï¼š**
```python
class ChunkingProgressTracker:
    def __init__(self, total_files: int):
        self.total_files = total_files
        self.processed_files = 0
        self.current_file = ""
        self.current_file_progress = 0.0
        self.start_time = time.time()
        self.callbacks = []
    
    def update_file_progress(self, file_path: str, chunks_processed: int, total_chunks: int):
        """æ›´æ–°å½“å‰æ–‡ä»¶çš„å¤„ç†è¿›åº¦"""
        self.current_file = file_path
        self.current_file_progress = chunks_processed / total_chunks if total_chunks > 0 else 0
        
        overall_progress = (self.processed_files + self.current_file_progress) / self.total_files
        
        # é¢„ä¼°å‰©ä½™æ—¶é—´
        elapsed = time.time() - self.start_time
        if overall_progress > 0:
            estimated_total = elapsed / overall_progress
            remaining_time = estimated_total - elapsed
        else:
            remaining_time = None
        
        progress_info = ProgressInfo(
            overall_progress=overall_progress,
            current_file=file_path,
            file_progress=self.current_file_progress,
            processed_files=self.processed_files,
            total_files=self.total_files,
            elapsed_time=elapsed,
            estimated_remaining=remaining_time,
            chunks_info={
                'processed': chunks_processed,
                'total': total_chunks,
                'current_chunk': chunks_processed + 1 if chunks_processed < total_chunks else total_chunks
            }
        )
        
        # é€šçŸ¥æ‰€æœ‰å›è°ƒ
        for callback in self.callbacks:
            callback(progress_info)
    
    def add_progress_callback(self, callback: Callable[[ProgressInfo], None]):
        """æ·»åŠ è¿›åº¦å›è°ƒ"""
        self.callbacks.append(callback)

class ProgressVisualizer:
    def create_progress_bar(self, progress_info: ProgressInfo) -> str:
        """åˆ›å»ºæ–‡æœ¬è¿›åº¦æ¡"""
        bar_length = 50
        filled_length = int(bar_length * progress_info.overall_progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        percentage = progress_info.overall_progress * 100
        
        time_info = ""
        if progress_info.estimated_remaining:
            time_info = f" | ETA: {self._format_time(progress_info.estimated_remaining)}"
        
        return f"""
â•­â”€ Chunking Progress â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ {bar} {percentage:.1f}%{time_info:<20} â”‚
â”‚ File: {progress_info.current_file:<45} â”‚
â”‚ Chunks: {progress_info.chunks_info['current_chunk']}/{progress_info.chunks_info['total']:<10} Files: {progress_info.processed_files}/{progress_info.total_files} â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
"""
```

**é¢„æœŸäº§å‡ºï¼š** å®æ—¶è¿›åº¦è¿½è¸ªå’Œå¯è§†åŒ–åé¦ˆç³»ç»Ÿ

---

### 18. ç ”ç©¶LRUç¼“å­˜ç­–ç•¥åœ¨åˆ†ç‰‡æ–‡æ¡£ç¼“å­˜ä¸­çš„åº”ç”¨
**ç ”ç©¶é‡ç‚¹ï¼š**
- åˆ†ç‰‡æ–‡æ¡£çš„ç¼“å­˜keyè®¾è®¡
- LRUç¼“å­˜çš„å®¹é‡ç®¡ç†ç­–ç•¥
- ç¼“å­˜å‘½ä¸­ç‡çš„ä¼˜åŒ–æ–¹æ³•
- ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°æœºåˆ¶
- å†…å­˜ä½¿ç”¨é‡çš„ç›‘æ§å’Œæ§åˆ¶

**æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼š**
```python
class ChunkDocumentCache:
    def __init__(self, max_size_mb: int = 256):
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.current_size = 0
        self.cache = OrderedDict()  # LRUç¼“å­˜
        self.hit_count = 0
        self.miss_count = 0
        self.access_patterns = defaultdict(int)
    
    def get_cache_key(self, chunk: CodeChunk) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        content_hash = hashlib.sha256(chunk.content.encode()).hexdigest()[:16]
        return f"{chunk.language}_{chunk.type}_{content_hash}"
    
    def get(self, chunk: CodeChunk) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ–‡æ¡£"""
        key = self.get_cache_key(chunk)
        
        if key in self.cache:
            # LRU: ç§»åŠ¨åˆ°æœ«å°¾
            doc_data = self.cache.pop(key)
            self.cache[key] = doc_data
            self.hit_count += 1
            self.access_patterns[key] += 1
            return doc_data['document']
        
        self.miss_count += 1
        return None
    
    def put(self, chunk: CodeChunk, document: str):
        """å­˜å‚¨æ–‡æ¡£åˆ°ç¼“å­˜"""
        key = self.get_cache_key(chunk)
        doc_size = len(document.encode('utf-8'))
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è…¾å‡ºç©ºé—´
        while (self.current_size + doc_size > self.max_size_bytes and 
               len(self.cache) > 0):
            self._evict_lru()
        
        # å­˜å‚¨æ–‡æ¡£
        self.cache[key] = {
            'document': document,
            'size': doc_size,
            'timestamp': time.time(),
            'access_count': 1
        }
        self.current_size += doc_size
        self.access_patterns[key] = 1
    
    def _evict_lru(self):
        """é©±é€æœ€ä¹…æœªä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if not self.cache:
            return
        
        # FIFOé©±é€ï¼ˆOrderedDictçš„ç¬¬ä¸€ä¸ªæ˜¯æœ€è€çš„ï¼‰
        key, data = self.cache.popitem(last=False)
        self.current_size -= data['size']
        
        # è®°å½•é©±é€ç»Ÿè®¡
        self._record_eviction(key, data)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'cache_size_mb': self.current_size / (1024 * 1024),
            'cached_items': len(self.cache),
            'top_accessed_patterns': dict(
                sorted(self.access_patterns.items(), 
                       key=lambda x: x[1], reverse=True)[:10]
            )
        }
```

**é¢„æœŸäº§å‡ºï¼š** é«˜æ•ˆçš„åˆ†ç‰‡æ–‡æ¡£LRUç¼“å­˜ç³»ç»Ÿ

---

### 19. è®¾è®¡åˆ†ç‰‡çº§åˆ«çš„å¢é‡æ›´æ–°å’Œå·®å¼‚æ£€æµ‹
**ç ”ç©¶é‡ç‚¹ï¼š**
- æ–‡ä»¶å˜æ›´çš„ç²¾ç¡®æ£€æµ‹ç®—æ³•
- åˆ†ç‰‡çº§åˆ«çš„å·®å¼‚è®¡ç®—
- å¢é‡æ›´æ–°çš„æœ€å°åŒ–ç­–ç•¥
- å˜æ›´å½±å“èŒƒå›´çš„åˆ†æ
- æ›´æ–°å†²çªçš„æ£€æµ‹å’Œè§£å†³

**å¢é‡æ›´æ–°ç³»ç»Ÿï¼š**
```python
class IncrementalChunkUpdater:
    def __init__(self, chunk_store: ChunkStore):
        self.chunk_store = chunk_store
        self.file_signatures = {}  # æ–‡ä»¶ç­¾åç¼“å­˜
        self.chunk_dependencies = {}  # åˆ†ç‰‡ä¾èµ–å…³ç³»
    
    def detect_file_changes(self, file_path: str) -> ChangeDetectionResult:
        """æ£€æµ‹æ–‡ä»¶å˜æ›´"""
        current_signature = self._calculate_file_signature(file_path)
        previous_signature = self.file_signatures.get(file_path)
        
        if previous_signature is None:
            return ChangeDetectionResult(ChangeType.NEW_FILE, current_signature)
        
        if current_signature == previous_signature:
            return ChangeDetectionResult(ChangeType.NO_CHANGE, current_signature)
        
        # è®¡ç®—å…·ä½“å˜æ›´
        detailed_changes = self._analyze_detailed_changes(
            file_path, previous_signature, current_signature
        )
        
        return ChangeDetectionResult(ChangeType.MODIFIED, current_signature, detailed_changes)
    
    def _analyze_detailed_changes(self, file_path: str, old_sig: FileSignature, new_sig: FileSignature) -> List[DetailedChange]:
        """åˆ†æè¯¦ç»†å˜æ›´"""
        changes = []
        
        # æŒ‰è¡Œæ¯”è¾ƒå†…å®¹
        old_lines = old_sig.content_lines
        new_lines = new_sig.content_lines
        
        # ä½¿ç”¨difflibè®¡ç®—å·®å¼‚
        differ = difflib.unified_diff(old_lines, new_lines, lineterm='')
        diff_lines = list(differ)
        
        # è§£æå·®å¼‚å¹¶æ˜ å°„åˆ°åˆ†ç‰‡
        chunk_changes = self._map_changes_to_chunks(diff_lines, file_path)
        
        return chunk_changes
    
    def update_affected_chunks(self, file_path: str, changes: List[DetailedChange]) -> UpdateResult:
        """æ›´æ–°å—å½±å“çš„åˆ†ç‰‡"""
        
        affected_chunks = set()
        for change in changes:
            chunk_ids = self._find_chunks_by_line_range(file_path, change.line_range)
            affected_chunks.update(chunk_ids)
        
        # æ‰¾å‡ºä¾èµ–çš„åˆ†ç‰‡
        dependent_chunks = set()
        for chunk_id in affected_chunks:
            deps = self.chunk_dependencies.get(chunk_id, [])
            dependent_chunks.update(deps)
        
        all_chunks_to_update = affected_chunks | dependent_chunks
        
        # æ‰§è¡Œæ›´æ–°
        update_results = []
        for chunk_id in all_chunks_to_update:
            try:
                result = self._update_single_chunk(chunk_id, file_path)
                update_results.append(result)
            except Exception as e:
                update_results.append(ChunkUpdateResult(
                    chunk_id, success=False, error=str(e)
                ))
        
        return UpdateResult(
            total_chunks_updated=len(all_chunks_to_update),
            successful_updates=len([r for r in update_results if r.success]),
            update_details=update_results
        )
    
    def _calculate_file_signature(self, file_path: str) -> FileSignature:
        """è®¡ç®—æ–‡ä»¶ç­¾å"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = content.split('\n')
        
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        
        # è®¡ç®—ç»“æ„å“ˆå¸Œï¼ˆå¿½ç•¥ç©ºç™½å’Œæ³¨é‡Šï¼‰
        normalized_content = self._normalize_content_for_hashing(content)
        structure_hash = hashlib.sha256(normalized_content.encode()).hexdigest()
        
        return FileSignature(
            file_path=file_path,
            content_hash=content_hash,
            structure_hash=structure_hash,
            content_lines=lines,
            line_count=len(lines),
            file_size=len(content.encode()),
            modification_time=os.path.getmtime(file_path)
        )
```

**é¢„æœŸäº§å‡ºï¼š** ç²¾ç¡®çš„å¢é‡æ›´æ–°å’Œå˜æ›´æ£€æµ‹ç³»ç»Ÿ

---

### 20. åˆ†æå¤šçº¿ç¨‹/åç¨‹å¹¶è¡Œå¤„ç†çš„èµ„æºç«äº‰é—®é¢˜
**ç ”ç©¶é‡ç‚¹ï¼š**
- çº¿ç¨‹å®‰å…¨çš„åˆ†ç‰‡å¤„ç†é˜Ÿåˆ—
- å†…å­˜å’ŒCPUèµ„æºçš„åˆç†åˆ†é…
- æ­»é”å’Œç«æ€æ¡ä»¶çš„é¢„é˜²
- å¼‚æ­¥I/Oçš„é«˜æ•ˆåˆ©ç”¨
- é”™è¯¯éš”ç¦»å’Œæ¢å¤æœºåˆ¶

**å¹¶å‘å¤„ç†æ¡†æ¶ï¼š**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from threading import RLock, Semaphore
from queue import Queue, PriorityQueue

class ConcurrentChunkProcessor:
    def __init__(self, max_workers: int = None, memory_limit_mb: int = 1024):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.memory_limit = memory_limit_mb * 1024 * 1024
        self.current_memory_usage = 0
        self.memory_lock = RLock()
        self.processing_semaphore = Semaphore(self.max_workers)
        
        # åˆ†ç¦»CPUå¯†é›†å‹å’ŒI/Oå¯†é›†å‹ä»»åŠ¡
        self.cpu_executor = ProcessPoolExecutor(max_workers=os.cpu_count())
        self.io_executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        # ä¼˜å…ˆçº§é˜Ÿåˆ—ç”¨äºä»»åŠ¡è°ƒåº¦
        self.task_queue = PriorityQueue()
        self.result_queue = Queue()
        
        # èµ„æºç›‘æ§
        self.resource_monitor = ResourceMonitor()
    
    async def process_chunks_concurrent(self, chunks: List[CodeChunk]) -> List[ChunkResult]:
        """å¹¶å‘å¤„ç†åˆ†ç‰‡"""
        
        # 1. ä»»åŠ¡åˆ†ç±»å’Œä¼˜å…ˆçº§æ’åº
        cpu_tasks = []  # ASTè§£æç­‰CPUå¯†é›†å‹ä»»åŠ¡
        io_tasks = []   # æ–‡ä»¶è¯»å†™ç­‰I/Oå¯†é›†å‹ä»»åŠ¡
        
        for i, chunk in enumerate(chunks):
            task_type = self._classify_task_type(chunk)
            priority = self._calculate_task_priority(chunk)
            
            if task_type == TaskType.CPU_INTENSIVE:
                cpu_tasks.append(PrioritizedTask(priority, i, chunk))
            else:
                io_tasks.append(PrioritizedTask(priority, i, chunk))
        
        # 2. å¯åŠ¨èµ„æºç›‘æ§
        monitor_task = asyncio.create_task(self._monitor_resources())
        
        try:
            # 3. å¹¶å‘æ‰§è¡Œä»»åŠ¡
            cpu_results_future = self._process_cpu_tasks(cpu_tasks)
            io_results_future = self._process_io_tasks(io_tasks)
            
            cpu_results, io_results = await asyncio.gather(
                cpu_results_future, 
                io_results_future,
                return_exceptions=True
            )
            
            # 4. åˆå¹¶ç»“æœ
            all_results = self._merge_results(cpu_results, io_results, len(chunks))
            
        finally:
            # 5. æ¸…ç†èµ„æº
            monitor_task.cancel()
            await self._cleanup_executors()
        
        return all_results
    
    async def _process_cpu_tasks(self, tasks: List[PrioritizedTask]) -> List[ChunkResult]:
        """å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡"""
        results = [None] * len(tasks)
        
        # ä½¿ç”¨ProcessPoolExecutoré¿å…GILé™åˆ¶
        loop = asyncio.get_event_loop()
        futures = []
        
        for task in tasks:
            if await self._check_resource_availability():
                future = loop.run_in_executor(
                    self.cpu_executor,
                    self._process_single_chunk_cpu,
                    task.chunk
                )
                futures.append((task.index, future))
            else:
                # èµ„æºä¸è¶³ï¼Œä½¿ç”¨é™çº§å¤„ç†
                result = await self._fallback_processing(task.chunk)
                results[task.index] = result
        
        # ç­‰å¾…æ‰€æœ‰CPUä»»åŠ¡å®Œæˆ
        for index, future in futures:
            try:
                result = await future
                results[index] = result
            except Exception as e:
                results[index] = ChunkResult(
                    chunk_id=tasks[index].chunk.id,
                    success=False,
                    error=str(e)
                )
        
        return results
    
    async def _process_io_tasks(self, tasks: List[PrioritizedTask]) -> List[ChunkResult]:
        """å¤„ç†I/Oå¯†é›†å‹ä»»åŠ¡"""
        results = [None] * len(tasks)
        
        # ä½¿ç”¨ThreadPoolExecutorå¤„ç†I/Oä»»åŠ¡
        loop = asyncio.get_event_loop()
        
        # é™åˆ¶å¹¶å‘æ•°é‡é˜²æ­¢èµ„æºè€—å°½
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def process_single_task(task: PrioritizedTask) -> Tuple[int, ChunkResult]:
            async with semaphore:
                try:
                    # æ£€æŸ¥å†…å­˜ä½¿ç”¨é‡
                    await self._wait_for_memory_availability(task.chunk)
                    
                    result = await loop.run_in_executor(
                        self.io_executor,
                        self._process_single_chunk_io,
                        task.chunk
                    )
                    return task.index, result
                    
                except Exception as e:
                    return task.index, ChunkResult(
                        chunk_id=task.chunk.id,
                        success=False,
                        error=str(e)
                    )
                finally:
                    # é‡Šæ”¾å†…å­˜
                    self._release_chunk_memory(task.chunk)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰I/Oä»»åŠ¡
        task_coroutines = [process_single_task(task) for task in tasks]
        completed_tasks = await asyncio.gather(*task_coroutines)
        
        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        for index, result in completed_tasks:
            results[index] = result
        
        return results
    
    async def _monitor_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨é‡"""
        while True:
            try:
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_info = psutil.virtual_memory()
                
                # è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
                self.resource_monitor.record_usage(
                    cpu_percent=cpu_percent,
                    memory_percent=memory_info.percent,
                    available_memory=memory_info.available
                )
                
                # å¦‚æœèµ„æºä½¿ç”¨è¿‡é«˜ï¼Œè§¦å‘é™åˆ¶
                if cpu_percent > 90:
                    await self._throttle_cpu_tasks()
                
                if memory_info.percent > 85:
                    await self._throttle_memory_usage()
                    
                await asyncio.sleep(2)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.warning(f"Resource monitoring error: {e}")
    
    async def _check_resource_availability(self) -> bool:
        """æ£€æŸ¥èµ„æºæ˜¯å¦å¯ç”¨"""
        memory_info = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent()
        
        return (memory_info.percent < 80 and 
                cpu_percent < 85 and 
                self.current_memory_usage < self.memory_limit)
```

**é¢„æœŸäº§å‡ºï¼š** é«˜æ•ˆå®‰å…¨çš„å¹¶å‘åˆ†ç‰‡å¤„ç†æ¡†æ¶

---

### 21. è®¾è®¡åˆ†ç‰‡å¤„ç†çš„è¯¦ç»†æ€§èƒ½æŒ‡æ ‡å’Œç›‘æ§ä½“ç³»
**ç ”ç©¶é‡ç‚¹ï¼š**
- å…³é”®æ€§èƒ½æŒ‡æ ‡çš„å®šä¹‰å’Œæµ‹é‡
- å®æ—¶æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
- æ€§èƒ½ç“¶é¢ˆçš„è‡ªåŠ¨è¯†åˆ«
- å¤„ç†æ•ˆç‡çš„ä¼˜åŒ–å»ºè®®
- æ€§èƒ½æŠ¥å‘Šçš„ç”Ÿæˆå’Œå¯è§†åŒ–

**æ€§èƒ½ç›‘æ§ç³»ç»Ÿï¼š**
```python
class ChunkingPerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'processing_times': defaultdict(list),  # å„ç±»å‹åˆ†ç‰‡çš„å¤„ç†æ—¶é—´
            'memory_usage': [],                     # å†…å­˜ä½¿ç”¨å†å²
            'cpu_usage': [],                        # CPUä½¿ç”¨å†å²
            'throughput': [],                       # å¤„ç†ååé‡
            'error_rates': defaultdict(int),        # é”™è¯¯ç‡ç»Ÿè®¡
            'cache_performance': {},                # ç¼“å­˜æ€§èƒ½
            'queue_lengths': [],                    # é˜Ÿåˆ—é•¿åº¦å†å²
        }
        self.start_time = time.time()
        self.processed_chunks = 0
        self.failed_chunks = 0
        
    def start_chunk_processing(self, chunk: CodeChunk) -> str:
        """å¼€å§‹å¤„ç†åˆ†ç‰‡ï¼Œè¿”å›å¤„ç†ID"""
        processing_id = f"{chunk.id}_{int(time.time() * 1000000)}"
        
        self.active_processings[processing_id] = {
            'chunk': chunk,
            'start_time': time.time(),
            'start_memory': self._get_current_memory_usage(),
            'start_cpu': psutil.cpu_percent()
        }
        
        return processing_id
    
    def end_chunk_processing(self, processing_id: str, success: bool, result_size: int = 0):
        """ç»“æŸåˆ†ç‰‡å¤„ç†"""
        if processing_id not in self.active_processings:
            return
        
        processing_info = self.active_processings.pop(processing_id)
        end_time = time.time()
        duration = end_time - processing_info['start_time']
        
        chunk = processing_info['chunk']
        
        # è®°å½•å¤„ç†æ—¶é—´
        self.metrics['processing_times'][chunk.type].append(duration)
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        memory_delta = self._get_current_memory_usage() - processing_info['start_memory']
        self.metrics['memory_usage'].append({
            'timestamp': end_time,
            'delta': memory_delta,
            'chunk_type': chunk.type,
            'chunk_size': len(chunk.content)
        })
        
        # æ›´æ–°ç»Ÿè®¡
        if success:
            self.processed_chunks += 1
        else:
            self.failed_chunks += 1
            self.metrics['error_rates'][chunk.type] += 1
        
        # è®¡ç®—ååé‡
        total_time = end_time - self.start_time
        if total_time > 0:
            current_throughput = self.processed_chunks / total_time
            self.metrics['throughput'].append({
                'timestamp': end_time,
                'chunks_per_second': current_throughput,
                'cumulative_chunks': self.processed_chunks
            })
    
    def get_performance_summary(self) -> PerformanceSummary:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        
        # è®¡ç®—å„ç±»å‹åˆ†ç‰‡çš„ç»Ÿè®¡ä¿¡æ¯
        processing_stats = {}
        for chunk_type, times in self.metrics['processing_times'].items():
            if times:
                processing_stats[chunk_type] = {
                    'count': len(times),
                    'avg_time': statistics.mean(times),
                    'median_time': statistics.median(times),
                    'max_time': max(times),
                    'min_time': min(times),
                    'std_dev': statistics.stdev(times) if len(times) > 1 else 0
                }
        
        # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        total_chunks = self.processed_chunks + self.failed_chunks
        success_rate = self.processed_chunks / total_chunks if total_chunks > 0 else 0
        
        current_time = time.time()
        total_duration = current_time - self.start_time
        overall_throughput = self.processed_chunks / total_duration if total_duration > 0 else 0
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_stats = self._analyze_memory_usage()
        
        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        bottlenecks = self._identify_bottlenecks()
        
        return PerformanceSummary(
            total_chunks_processed=self.processed_chunks,
            total_chunks_failed=self.failed_chunks,
            success_rate=success_rate,
            overall_throughput=overall_throughput,
            total_duration=total_duration,
            processing_stats=processing_stats,
            memory_stats=memory_stats,
            bottlenecks=bottlenecks,
            recommendations=self._generate_recommendations(bottlenecks)
        )
    
    def _identify_bottlenecks(self) -> List[PerformanceBottleneck]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # 1. æ£€æŸ¥å¤„ç†æ—¶é—´å¼‚å¸¸
        for chunk_type, times in self.metrics['processing_times'].items():
            if len(times) >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
                avg_time = statistics.mean(times)
                std_dev = statistics.stdev(times)
                
                # å¦‚æœæ ‡å‡†å·®è¿‡å¤§ï¼Œè¯´æ˜å¤„ç†æ—¶é—´ä¸ç¨³å®š
                if std_dev > avg_time * 0.5:
                    bottlenecks.append(PerformanceBottleneck(
                        type="processing_time_variance",
                        affected_component=f"{chunk_type}_processing",
                        severity="medium",
                        description=f"High variance in {chunk_type} processing times",
                        metrics={
                            'avg_time': avg_time,
                            'std_dev': std_dev,
                            'coefficient_of_variation': std_dev / avg_time
                        }
                    ))
        
        # 2. æ£€æŸ¥å†…å­˜ä½¿ç”¨æ¨¡å¼
        if self.metrics['memory_usage']:
            memory_deltas = [m['delta'] for m in self.metrics['memory_usage']]
            if any(delta > 100 * 1024 * 1024 for delta in memory_deltas):  # 100MB
                bottlenecks.append(PerformanceBottleneck(
                    type="memory_spike",
                    affected_component="memory_management",
                    severity="high",
                    description="Large memory spikes detected during processing",
                    metrics={'max_delta_mb': max(memory_deltas) / (1024 * 1024)}
                ))
        
        # 3. æ£€æŸ¥é”™è¯¯ç‡
        total_chunks = sum(len(times) for times in self.metrics['processing_times'].values())
        total_errors = sum(self.metrics['error_rates'].values())
        if total_chunks > 0:
            error_rate = total_errors / total_chunks
            if error_rate > 0.05:  # 5%é”™è¯¯ç‡
                bottlenecks.append(PerformanceBottleneck(
                    type="high_error_rate",
                    affected_component="chunk_processing",
                    severity="high",
                    description=f"High error rate: {error_rate:.2%}",
                    metrics={'error_rate': error_rate, 'total_errors': total_errors}
                ))
        
        return bottlenecks
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        summary = self.get_performance_summary()
        
        report = f"""
# CodeLens åˆ†ç‰‡å¤„ç†æ€§èƒ½æŠ¥å‘Š

## ğŸ“Š æ€»ä½“æ¦‚è§ˆ
- **å¤„ç†æ—¶é—´**: {summary.total_duration:.2f} ç§’
- **å¤„ç†åˆ†ç‰‡æ•°**: {summary.total_chunks_processed}
- **å¤±è´¥åˆ†ç‰‡æ•°**: {summary.total_chunks_failed}  
- **æˆåŠŸç‡**: {summary.success_rate:.2%}
- **å¤„ç†ååé‡**: {summary.overall_throughput:.2f} åˆ†ç‰‡/ç§’

## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡

### æŒ‰ç±»å‹åˆ†ç‰‡æ€§èƒ½
"""
        
        for chunk_type, stats in summary.processing_stats.items():
            report += f"""
#### {chunk_type.title()} ç±»å‹åˆ†ç‰‡
- æ•°é‡: {stats['count']}
- å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_time']:.3f} ç§’
- ä¸­ä½æ•°æ—¶é—´: {stats['median_time']:.3f} ç§’
- æœ€å¤§å¤„ç†æ—¶é—´: {stats['max_time']:.3f} ç§’
- æ ‡å‡†å·®: {stats['std_dev']:.3f} ç§’
"""
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        if summary.memory_stats:
            report += f"""
### ğŸ§  å†…å­˜ä½¿ç”¨åˆ†æ
- å¹³å‡å†…å­˜å¢é‡: {summary.memory_stats.get('avg_delta_mb', 0):.2f} MB
- æœ€å¤§å†…å­˜å¢é‡: {summary.memory_stats.get('max_delta_mb', 0):.2f} MB
- å†…å­˜ä½¿ç”¨æ•ˆç‡: {summary.memory_stats.get('efficiency_score', 0):.2f}
"""
        
        # æ€§èƒ½ç“¶é¢ˆ
        if summary.bottlenecks:
            report += "\n## âš ï¸ è¯†åˆ«çš„æ€§èƒ½ç“¶é¢ˆ\n"
            for bottleneck in summary.bottlenecks:
                report += f"""
### {bottleneck.type}
- **ä¸¥é‡ç¨‹åº¦**: {bottleneck.severity}
- **å½±å“ç»„ä»¶**: {bottleneck.affected_component}  
- **æè¿°**: {bottleneck.description}
- **ç›¸å…³æŒ‡æ ‡**: {bottleneck.metrics}
"""
        
        # ä¼˜åŒ–å»ºè®®
        if summary.recommendations:
            report += "\n## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n"
            for i, rec in enumerate(summary.recommendations, 1):
                report += f"{i}. {rec}\n"
        
        return report
```

**é¢„æœŸäº§å‡ºï¼š** å…¨é¢çš„æ€§èƒ½ç›‘æ§å’Œåˆ†æç³»ç»Ÿ

---

## ğŸ¯ ç³»ç»Ÿé›†æˆä¸æ¶æ„æ·±åº¦åˆ†æ

### 22-30. [ç³»ç»Ÿé›†æˆç›¸å…³çš„æ·±åº¦åˆ†ætodoé¡¹ç›®]

[ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œåˆ—å‡ºå‰©ä½™çš„8ä¸ªç³»ç»Ÿé›†æˆç›¸å…³çš„ç ”ç©¶ç‚¹]

---

## ğŸš€ ç ”ç©¶ä¼˜å…ˆçº§å»ºè®®

### ğŸ”¥ é«˜ä¼˜å…ˆçº§ (æ ¸å¿ƒåŠŸèƒ½)
1. **ASTè§£æè¾¹ç•Œæƒ…å†µ** (#1)
2. **åˆ†ç‰‡ä¾èµ–å…³ç³»æ£€æµ‹** (#2)  
3. **å¤±è´¥é™çº§æœºåˆ¶** (#14)
4. **æ€§èƒ½ç›‘æ§ä½“ç³»** (#21)

### âš¡ ä¸­ä¼˜å…ˆçº§ (ç”¨æˆ·ä½“éªŒ)
5. **è¿›åº¦åé¦ˆå¯è§†åŒ–** (#17)
6. **LRUç¼“å­˜ç­–ç•¥** (#18)
7. **å¢é‡æ›´æ–°æœºåˆ¶** (#19)

### ğŸ”§ ä½ä¼˜å…ˆçº§ (é«˜çº§ç‰¹æ€§)
8. **å¤šè¯­è¨€å¤æ‚ç‰¹æ€§å¤„ç†** (#8-12)
9. **å¹¶å‘èµ„æºç«äº‰** (#20)
10. **ç³»ç»Ÿé›†æˆæ–¹æ¡ˆ** (#22-30)

---

## ğŸ“ ç»“è®º

è¿™ä»½è¯¦ç»†çš„todoåˆ—è¡¨æ¶µç›–äº†CodeLenså¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆçš„å„ä¸ªé‡è¦æ–¹é¢ã€‚æ¯ä¸ªç ”ç©¶ç‚¹éƒ½éœ€è¦æ·±å…¥çš„æŠ€æœ¯åˆ†æå’Œå®ç°è®¾è®¡ï¼Œç¡®ä¿æœ€ç»ˆæ–¹æ¡ˆæ—¢å¼ºå¤§åˆç¨³å®šï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§å¤æ‚çš„å¤§æ–‡ä»¶åœºæ™¯ã€‚

å»ºè®®æŒ‰ç…§ä¼˜å…ˆçº§é€æ­¥ç ”ç©¶å’Œå®ç°ï¼Œå…ˆè§£å†³æ ¸å¿ƒåŠŸèƒ½å’Œç¨³å®šæ€§é—®é¢˜ï¼Œå†æ‰©å±•åˆ°é«˜çº§ç‰¹æ€§å’Œç³»ç»Ÿä¼˜åŒ–ã€‚