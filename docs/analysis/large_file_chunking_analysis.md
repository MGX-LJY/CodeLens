# CodeLenså¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆåˆ†æ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£åˆ†æCodeLensç³»ç»Ÿä¸­å¤§æ–‡ä»¶å¤„ç†çš„ç°çŠ¶ã€æŒ‘æˆ˜å’ŒåŸºäºåˆ†ç‰‡å¤„ç†çš„è§£å†³æ–¹æ¡ˆã€‚é‡ç‚¹å…³æ³¨å¦‚ä½•åœ¨ç°æœ‰ä»»åŠ¡æ‰§è¡Œæµç¨‹ä¸­é›†æˆæ™ºèƒ½åˆ†ç‰‡å¤„ç†ï¼Œä»¥æ”¯æŒå‡ åƒè¡Œç”šè‡³å‡ ä¸‡è¡Œçš„å¤§å‹ä»£ç æ–‡ä»¶ã€‚

---

## ğŸ” ç°æœ‰ç³»ç»Ÿåˆ†æ

### å½“å‰ä»»åŠ¡æ‰§è¡Œæµç¨‹

```mermaid
graph TD
    A[execute_taskè°ƒç”¨] --> B{ä»»åŠ¡ç±»å‹åˆ¤æ–­}
    B -->|scan| C[_execute_scan_task]
    B -->|file_summary| D{æ–‡ä»¶ç±»å‹æ£€æŸ¥}
    
    D --> E{æ˜¯å¦ä¸ºç©ºæ–‡ä»¶?}
    E -->|æ˜¯, â‰¤10å­—ç¬¦| F[_check_and_handle_empty_file]
    E -->|å¦| G[æ ‡è®°ä»»åŠ¡ä¸ºIN_PROGRESS]
    
    F --> H[è‡ªåŠ¨ç”Ÿæˆç®€å•æ–‡æ¡£å¹¶å®Œæˆä»»åŠ¡]
    G --> I[prepare_task_execution]
    I --> J[è¿”å›æ‰§è¡Œä¸Šä¸‹æ–‡å’ŒæŒ‡å¯¼]
    
    C --> K[è‡ªåŠ¨æ‰§è¡Œé¡¹ç›®æ‰«æå¹¶å®Œæˆä»»åŠ¡]
```

### ç°æœ‰æ–‡ä»¶æœåŠ¡é™åˆ¶

**FileService.read_file_safe()** `src/services/file_service.py:89-106`
```python
def read_file_safe(self, file_path: str, max_size: int = 50000) -> Optional[str]:
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = file_path.stat().st_size
    if file_size > max_size:  # é»˜è®¤50KBé™åˆ¶
        print(f"Warning: File {file_path} is too large ({file_size} bytes), skipping")
        return None  # ç›´æ¥è·³è¿‡å¤§æ–‡ä»¶
```

**é—®é¢˜åˆ†æ:**
1. **ç¡¬æ€§50KBé™åˆ¶** - è¶…è¿‡ç›´æ¥è·³è¿‡ï¼Œæ— æ³•å¤„ç†
2. **æ— åˆ†ç‰‡èƒ½åŠ›** - ä¸æ”¯æŒåˆ†å—è¯»å–
3. **æ— ç»“æ„åˆ†æ** - ç¼ºä¹ä»£ç ç»“æ„ç†è§£
4. **å¤„ç†ä¸­æ–­** - å¤§æ–‡ä»¶å¯¼è‡´æ•´ä¸ªæ–‡æ¡£ç”Ÿæˆå¤±è´¥

---

## ğŸ’¡ åˆ†ç‰‡å¤„ç†æ–¹æ¡ˆè®¾è®¡

### æ ¸å¿ƒæ€æƒ³

åŸºäº**åˆ†ç‰‡å¤„ç† + æ™ºèƒ½åˆå¹¶**çš„ç­–ç•¥ï¼ŒæŒ‰ç…§ä»£ç ç»“æ„çš„è¯­ä¹‰è¾¹ç•Œè¿›è¡Œåˆ†ç‰‡ï¼Œè€Œä¸æ˜¯ç®€å•çš„å­—èŠ‚åˆ‡åˆ†ã€‚

### æ•´ä½“æ¶æ„

```mermaid
graph TD
    A[å¤§æ–‡ä»¶æ£€æµ‹] --> B{æ–‡ä»¶å¤§å°>50KB?}
    B -->|å¦| C[å¸¸è§„å¤„ç†æµç¨‹]
    B -->|æ˜¯| D[å¯åŠ¨åˆ†ç‰‡å¤„ç†å™¨]
    
    D --> E[ä»£ç ç»“æ„åˆ†æ]
    E --> F[ç¡®å®šåˆ†ç‰‡ç­–ç•¥]
    F --> G[æŒ‰ç­–ç•¥åˆ†ç‰‡]
    
    G --> H[å¹¶è¡Œå¤„ç†å„ç‰‡æ®µ]
    H --> I[ç”Ÿæˆç‰‡æ®µæ–‡æ¡£]
    I --> J[æ™ºèƒ½åˆå¹¶æ–‡æ¡£]
    J --> K[ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£]
```

---

## ğŸ Pythonæ–‡ä»¶åˆ†ç‰‡ç­–ç•¥

### åˆ†å±‚åˆ†ç‰‡åŸåˆ™

#### **ç¬¬ä¸€å±‚ï¼šæ¨¡å—çº§åˆ†ç‰‡**
```python
# å¤§æ–‡ä»¶ç»“æ„ç¤ºä¾‹
class DataProcessor:      # ç‰‡æ®µ1: DataProcessorç±»
    def __init__(self): pass
    def process(self): pass
    # ... 500è¡Œä»£ç 

class APIHandler:         # ç‰‡æ®µ2: APIHandlerç±»  
    def get(self): pass
    def post(self): pass
    # ... 800è¡Œä»£ç 

def utility_function():   # ç‰‡æ®µ3: æ¨¡å—çº§å‡½æ•°ç»„
    pass

# æ¨¡å—çº§å˜é‡å’Œå¸¸é‡    # ç‰‡æ®µ4: æ¨¡å—çº§å®šä¹‰
CONFIG = {...}
```

**åˆ†ç‰‡ç­–ç•¥:**
- æ¯ä¸ª**ç±»å®šä¹‰**ä½œä¸ºä¸€ä¸ªç‹¬ç«‹ç‰‡æ®µ
- æ‰€æœ‰**æ¨¡å—çº§å‡½æ•°**åˆå¹¶ä¸ºä¸€ä¸ªç‰‡æ®µ  
- **æ¨¡å—çº§å˜é‡/å¸¸é‡**ä½œä¸ºä¸€ä¸ªç‰‡æ®µ
- **å¯¼å…¥è¯­å¥**ä¿ç•™åœ¨æ¯ä¸ªç‰‡æ®µä¸­

#### **ç¬¬äºŒå±‚ï¼šç±»å†…æ–¹æ³•åˆ†ç‰‡**
å½“å•ä¸ªç±»ä»ç„¶è¿‡å¤§ï¼ˆ>30KBï¼‰æ—¶ï¼š
```python
class LargeClass:
    # ç‰‡æ®µA: æ„é€ å’ŒåŸºç¡€æ–¹æ³•
    def __init__(self): pass
    def __repr__(self): pass
    
    # ç‰‡æ®µB: æ ¸å¿ƒä¸šåŠ¡æ–¹æ³•ç»„1
    def process_data(self): pass
    def validate_data(self): pass
    
    # ç‰‡æ®µC: æ ¸å¿ƒä¸šåŠ¡æ–¹æ³•ç»„2  
    def save_results(self): pass
    def export_data(self): pass
    
    # ç‰‡æ®µD: è¾…åŠ©æ–¹æ³•
    def _helper1(self): pass
    def _helper2(self): pass
```

### Pythonåˆ†ç‰‡å®ç°

```python
class PythonChunker:
    def chunk_by_classes(self, content: str) -> List[CodeChunk]:
        """æŒ‰ç±»å®šä¹‰åˆ†ç‰‡"""
        tree = ast.parse(content)
        chunks = []
        
        # æå–å¯¼å…¥è¯­å¥ï¼ˆæ¯ä¸ªç‰‡æ®µéƒ½éœ€è¦ï¼‰
        imports = self._extract_imports(tree)
        
        # æŒ‰ç±»åˆ†ç‰‡
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                chunk_content = imports + self._extract_class_code(node, content)
                chunks.append(CodeChunk(
                    id=f"class_{node.name}",
                    content=chunk_content,
                    type="class",
                    name=node.name,
                    line_range=(node.lineno, node.end_lineno)
                ))
        
        # æ¨¡å—çº§å‡½æ•°å’Œå˜é‡
        module_level = self._extract_module_level_code(tree, content)
        if module_level:
            chunks.append(CodeChunk(
                id="module_level",
                content=imports + module_level,
                type="module",
                name="æ¨¡å—çº§å®šä¹‰"
            ))
            
        return chunks
```

---

## ğŸŒ å…¶ä»–è¯­è¨€åˆ†ç‰‡ç­–ç•¥

### JavaScript/TypeScript
```javascript
// åˆ†ç‰‡ç­–ç•¥
export class ComponentA { }     // ç‰‡æ®µ1: ComponentAç±»
export class ComponentB { }     // ç‰‡æ®µ2: ComponentBç±»

export function utilA() { }     // ç‰‡æ®µ3: å¯¼å‡ºå‡½æ•°ç»„
export function utilB() { }

const config = { };             // ç‰‡æ®µ4: æ¨¡å—çº§å®šä¹‰å’Œç±»å‹
type UserType = { };
```

**ç‰¹ç‚¹:**
- æŒ‰**ç±»/æ¥å£**åˆ†ç‰‡
- **å¯¼å‡ºå‡½æ•°**æŒ‰ç›¸å…³æ€§åˆ†ç»„
- **ç±»å‹å®šä¹‰**å’Œ**å¸¸é‡**åˆå¹¶å¤„ç†

### Java
```java
// Javaé€šå¸¸ä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªä¸»ç±»ï¼Œåˆ†ç‰‡ç­–ç•¥ï¼š
public class MainClass {
    // ç‰‡æ®µ1: é™æ€æˆå‘˜å’Œå¸¸é‡
    static final String CONFIG = "";
    
    // ç‰‡æ®µ2: æ„é€ æ–¹æ³•å’ŒåŸºç¡€æ–¹æ³•
    public MainClass() { }
    
    // ç‰‡æ®µ3: ä¸šåŠ¡æ–¹æ³•ç»„1
    public void businessMethod1() { }
    
    // ç‰‡æ®µ4: ä¸šåŠ¡æ–¹æ³•ç»„2  
    public void businessMethod2() { }
    
    // ç‰‡æ®µ5: å†…éƒ¨ç±»
    static class InnerClass { }
}
```

### C/C++
```cpp
// åˆ†ç‰‡ç­–ç•¥
// ç‰‡æ®µ1: åŒ…å«å’Œå®å®šä¹‰
#include <iostream>
#define MAX_SIZE 1000

// ç‰‡æ®µ2: ç»“æ„ä½“å’Œç±»å‹å®šä¹‰
struct DataStruct { };
typedef int DataType;

// ç‰‡æ®µ3: ç±»å®šä¹‰
class ProcessorClass {
    // å¦‚æœç±»è¿‡å¤§ï¼ŒæŒ‰public/private/protectedåˆ†ç‰‡
};

// ç‰‡æ®µ4: å…¨å±€å‡½æ•°
void globalFunction1() { }
void globalFunction2() { }
```

### Go
```go
// åˆ†ç‰‡ç­–ç•¥
// ç‰‡æ®µ1: åŒ…å£°æ˜å’Œå¯¼å…¥
package main
import "fmt"

// ç‰‡æ®µ2: ç±»å‹å®šä¹‰
type User struct { }
type Handler interface { }

// ç‰‡æ®µ3: æ–¹æ³•ç»„ï¼ˆæŒ‰æ¥æ”¶è€…åˆ†ç»„ï¼‰
func (u *User) GetName() string { }
func (u *User) SetName(name string) { }

// ç‰‡æ®µ4: åŒ…çº§å‡½æ•°
func ProcessData() { }
func ValidateInput() { }
```

---

## ğŸ”§ ä¸FileServiceé›†æˆæ–¹æ¡ˆ

### æ‰©å±•FileService

```python
# åœ¨FileServiceä¸­æ·»åŠ åˆ†ç‰‡è¯»å–èƒ½åŠ›
class FileService:
    def __init__(self):
        # ... ç°æœ‰åˆå§‹åŒ–
        self.chunker_factory = ChunkerFactory()  # æ–°å¢ï¼šåˆ†ç‰‡å¤„ç†å·¥å‚
    
    def read_file_chunked(self, file_path: str, 
                         chunk_strategy: str = "auto") -> List[CodeChunk]:
        """åˆ†ç‰‡è¯»å–å¤§æ–‡ä»¶"""
        if not self.is_large_file(file_path):
            # å°æ–‡ä»¶ç›´æ¥è¿”å›å•ä¸ªchunk
            content = self.read_file_safe(file_path, max_size=float('inf'))
            return [CodeChunk.from_content(content)]
        
        # å¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†
        language = self.detect_language(file_path)
        chunker = self.chunker_factory.get_chunker(language)
        
        # è¯»å–æ–‡ä»¶ï¼ˆåˆ†å—è¯»å–é¿å…å†…å­˜é—®é¢˜ï¼‰
        content = self._read_large_file_streaming(file_path)
        
        # æŒ‰è¯­è¨€ç‰¹å®šç­–ç•¥åˆ†ç‰‡
        chunks = chunker.chunk_file(content, strategy=chunk_strategy)
        
        return chunks
    
    def is_large_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤§æ–‡ä»¶ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰"""
        try:
            path = Path(file_path)
            return path.stat().st_size > 50000  # 50KBé˜ˆå€¼
        except:
            return False
```

### ChunkerFactoryè®¾è®¡

```python
class ChunkerFactory:
    """åˆ†ç‰‡å¤„ç†å™¨å·¥å‚"""
    
    def __init__(self):
        self.chunkers = {
            'python': PythonChunker(),
            'javascript': JavaScriptChunker(),
            'java': JavaChunker(),
            'cpp': CppChunker(),
            'go': GoChunker(),
            'rust': RustChunker()
        }
    
    def get_chunker(self, language: str) -> BaseChunker:
        return self.chunkers.get(language, GenericChunker())

class CodeChunk:
    """ä»£ç ç‰‡æ®µæ•°æ®ç»“æ„"""
    id: str              # ç‰‡æ®µå”¯ä¸€æ ‡è¯†
    content: str         # ç‰‡æ®µå†…å®¹
    type: str           # ç‰‡æ®µç±»å‹ï¼ˆclass/function/moduleç­‰ï¼‰
    name: str           # ç‰‡æ®µåç§°
    line_range: tuple   # åœ¨åŸæ–‡ä»¶ä¸­çš„è¡Œå·èŒƒå›´
    dependencies: List[str]  # å¯¹å…¶ä»–ç‰‡æ®µçš„ä¾èµ–
    metadata: dict      # é¢å¤–å…ƒæ•°æ®
```

---

## ğŸ”„ ä»»åŠ¡æ‰§è¡Œæµç¨‹é›†æˆ

### é›†æˆç‚¹ä½ç½®

åœ¨`TaskExecutor.execute_task()`ä¸­æ·»åŠ å¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†ï¼š

```python
# åœ¨task_execute.py:135-142ä¹‹é—´æ’å…¥
if task.type.value == "file_summary" and task.target_file:
    # ç°æœ‰ï¼šç©ºæ–‡ä»¶æ£€æŸ¥
    empty_file_result = self._check_and_handle_empty_file(task_id)
    if empty_file_result:
        return empty_file_result
    
    # æ–°å¢ï¼šå¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç† 
    large_file_result = self._check_and_handle_large_file_with_chunking(task_id)
    if large_file_result:
        return large_file_result
```

### åˆ†ç‰‡å¤„ç†æ–¹æ³•å®ç°

```python
def _check_and_handle_large_file_with_chunking(self, task_id: str) -> Optional[Dict]:
    """å¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†"""
    task = self.task_manager.get_task(task_id)
    file_path = self.project_path / task.target_file
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§æ–‡ä»¶
    if not self.file_service.is_large_file(str(file_path)):
        return None
    
    self.logger.info(f"æ£€æµ‹åˆ°å¤§æ–‡ä»¶ï¼Œå¯åŠ¨åˆ†ç‰‡å¤„ç†: {task.target_file}")
    
    # 1. åˆ†ç‰‡è¯»å–
    chunks = self.file_service.read_file_chunked(str(file_path))
    
    # 2. åˆ›å»ºå­ä»»åŠ¡æˆ–ç›´æ¥å¤„ç†
    if len(chunks) <= 3:
        # ç‰‡æ®µè¾ƒå°‘ï¼Œç›´æ¥å¤„ç†
        return self._process_chunks_directly(task_id, chunks)
    else:
        # ç‰‡æ®µè¾ƒå¤šï¼Œåˆ›å»ºå­ä»»åŠ¡ç³»ç»Ÿ
        return self._process_chunks_as_subtasks(task_id, chunks)

def _process_chunks_directly(self, task_id: str, chunks: List[CodeChunk]) -> Dict:
    """ç›´æ¥å¤„ç†å°‘é‡ç‰‡æ®µ"""
    task = self.task_manager.get_task(task_id)
    chunk_docs = []
    
    # å¹¶è¡Œå¤„ç†å„ä¸ªç‰‡æ®µ
    for chunk in chunks:
        chunk_doc = self._generate_chunk_documentation(chunk, task)
        chunk_docs.append(chunk_doc)
    
    # åˆå¹¶æ–‡æ¡£
    final_doc = self._merge_chunk_documents(chunk_docs, task.target_file)
    
    # å†™å…¥æœ€ç»ˆæ–‡æ¡£
    output_path = self.project_path / task.output_path
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_doc)
    
    # å®Œæˆä»»åŠ¡
    self.task_manager.update_task_status(task_id, TaskStatus.COMPLETED)
    self.state_tracker.record_task_event("completed", task_id)
    
    return {
        "success": True,
        "message": f"Large file processed with {len(chunks)} chunks",
        "output_file": str(output_path),
        "task_completed": True,
        "processing_method": "chunked_direct",
        "chunk_info": {
            "total_chunks": len(chunks),
            "chunk_types": [c.type for c in chunks]
        }
    }
```

### æ–‡æ¡£åˆå¹¶ç­–ç•¥

```python
def _merge_chunk_documents(self, chunk_docs: List[str], filename: str) -> str:
    """æ™ºèƒ½åˆå¹¶ç‰‡æ®µæ–‡æ¡£"""
    
    # æ–‡æ¡£å¤´éƒ¨
    merged_doc = f"# æ–‡ä»¶åˆ†ææŠ¥å‘Šï¼š{filename}\n\n"
    merged_doc += "## æ–‡ä»¶æ¦‚è¿°\n\n"
    merged_doc += f"è¿™æ˜¯ä¸€ä¸ªå¤§å‹æ–‡ä»¶ï¼Œé‡‡ç”¨åˆ†ç‰‡å¤„ç†æ–¹å¼ç”Ÿæˆæ–‡æ¡£ã€‚å…±{len(chunk_docs)}ä¸ªç‰‡æ®µã€‚\n\n"
    
    # ç›®å½•
    merged_doc += "## å†…å®¹ç›®å½•\n\n"
    for i, chunk_doc in enumerate(chunk_docs, 1):
        title = self._extract_chunk_title(chunk_doc)
        merged_doc += f"- [{title}](#ç‰‡æ®µ{i}-{title})\n"
    merged_doc += "\n"
    
    # åˆå¹¶å„ç‰‡æ®µæ–‡æ¡£
    for i, chunk_doc in enumerate(chunk_docs, 1):
        title = self._extract_chunk_title(chunk_doc)
        merged_doc += f"## ç‰‡æ®µ{i}: {title}\n\n"
        
        # ç§»é™¤ç‰‡æ®µæ–‡æ¡£çš„æ ‡é¢˜ï¼Œæ·»åŠ å†…å®¹
        content = self._clean_chunk_doc_for_merge(chunk_doc)
        merged_doc += content + "\n\n"
    
    # æ–‡æ¡£å°¾éƒ¨
    merged_doc += "---\n\n"
    merged_doc += "*æ­¤æ–‡æ¡£é€šè¿‡CodeLenså¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†æŠ€æœ¯ç”Ÿæˆ*"
    
    return merged_doc
```

---

## ğŸ“Š æ€§èƒ½å’Œèµ„æºä¼˜åŒ–

### å†…å­˜ç®¡ç†

```python
def _read_large_file_streaming(self, file_path: str) -> str:
    """æµå¼è¯»å–å¤§æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    content_parts = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = f.read(8192)  # 8KBå—è¯»å–
            if not chunk:
                break
            content_parts.append(chunk)
    
    return ''.join(content_parts)
```

### å¹¶è¡Œå¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor

def _process_chunks_parallel(self, chunks: List[CodeChunk]) -> List[str]:
    """å¹¶è¡Œå¤„ç†ç‰‡æ®µ"""
    with ThreadPoolExecutor(max_workers=min(4, len(chunks))) as executor:
        chunk_docs = list(executor.map(self._generate_chunk_documentation, chunks))
    return chunk_docs
```

### ç¼“å­˜ç­–ç•¥

```python
def _generate_chunk_documentation(self, chunk: CodeChunk, task: Task) -> str:
    """ç”Ÿæˆç‰‡æ®µæ–‡æ¡£ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
    # è®¡ç®—chunkå†…å®¹hashä½œä¸ºç¼“å­˜key
    chunk_hash = hashlib.md5(chunk.content.encode()).hexdigest()
    cache_key = f"chunk_doc_{chunk_hash}"
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_doc = self._get_from_cache(cache_key)
    if cached_doc:
        return cached_doc
    
    # ç”Ÿæˆæ–°æ–‡æ¡£
    doc = self._do_generate_chunk_documentation(chunk, task)
    
    # å­˜å…¥ç¼“å­˜
    self._save_to_cache(cache_key, doc)
    
    return doc
```

---

## ğŸ¯ é›†æˆä¼˜åŠ¿

### æœ€å°å…¥ä¾µåŸåˆ™
- **ä¿æŒç°æœ‰APIå…¼å®¹** - ä¸ä¿®æ”¹ç°æœ‰æ–¹æ³•ç­¾å
- **æ¸è¿›å¼å¢å¼º** - åœ¨ç°æœ‰æµç¨‹ä¸­æ·»åŠ åˆ†ç‰‡æ£€æŸ¥ç‚¹
- **å‘åå…¼å®¹** - å°æ–‡ä»¶ç»§ç»­ä½¿ç”¨åŸæœ‰å¤„ç†é€»è¾‘

### æ™ºèƒ½å¤„ç†èƒ½åŠ›
- **è¯­ä¹‰åˆ†ç‰‡** - æŒ‰ä»£ç ç»“æ„åˆ†ç‰‡ï¼Œä¸ç ´åè¯­ä¹‰å®Œæ•´æ€§
- **è‡ªé€‚åº”ç­–ç•¥** - æ ¹æ®æ–‡ä»¶å¤§å°å’Œç»“æ„åŠ¨æ€é€‰æ‹©åˆ†ç‰‡ç­–ç•¥  
- **æ™ºèƒ½åˆå¹¶** - ä¿æŒæ–‡æ¡£çš„é€»è¾‘è¿è´¯æ€§å’Œå¯è¯»æ€§

### å¤šè¯­è¨€æ”¯æŒ
- **å¯æ‰©å±•æ¶æ„** - æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€çš„ç‰¹å®šåˆ†ç‰‡ç­–ç•¥
- **é€šç”¨åå¤‡** - å¯¹æœªçŸ¥è¯­è¨€æä¾›é€šç”¨åˆ†ç‰‡æ–¹æ¡ˆ
- **ç»“æ„æ„ŸçŸ¥** - ç†è§£ä¸åŒè¯­è¨€çš„ä»£ç ç»“æ„ç‰¹ç‚¹

---

## ğŸš€ å®æ–½è®¡åˆ’

### é˜¶æ®µ1ï¼šæ ¸å¿ƒåŸºç¡€ (Week 1-2)
1. å®ç°`BaseChunker`å’Œ`ChunkerFactory`
2. å®ç°`PythonChunker`ï¼ˆé‡ç‚¹ï¼‰
3. æ‰©å±•`FileService`æ·»åŠ åˆ†ç‰‡è¯»å–èƒ½åŠ›

### é˜¶æ®µ2ï¼šä»»åŠ¡é›†æˆ (Week 2-3)  
1. åœ¨`TaskExecutor`ä¸­æ·»åŠ å¤§æ–‡ä»¶æ£€æµ‹ç‚¹
2. å®ç°ç›´æ¥åˆ†ç‰‡å¤„ç†æµç¨‹
3. å®ç°æ–‡æ¡£åˆå¹¶é€»è¾‘

### é˜¶æ®µ3ï¼šå¤šè¯­è¨€æ”¯æŒ (Week 3-4)
1. å®ç°JavaScript/TypeScriptåˆ†ç‰‡å™¨
2. å®ç°Javaåˆ†ç‰‡å™¨
3. å®ç°C/C++åˆ†ç‰‡å™¨

### é˜¶æ®µ4ï¼šä¼˜åŒ–å’Œæµ‹è¯• (Week 4-5)
1. æ€§èƒ½ä¼˜åŒ–ï¼ˆå¹¶è¡Œå¤„ç†ã€ç¼“å­˜ï¼‰
2. é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
3. é›†æˆæµ‹è¯•å’Œæ–‡æ¡£

---

## ğŸ“‹ æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡**æ™ºèƒ½åˆ†ç‰‡ + è¯­ä¹‰åˆå¹¶**çš„ç­–ç•¥è§£å†³å¤§æ–‡ä»¶å¤„ç†é—®é¢˜ï¼š

âœ… **ä¿æŒç³»ç»Ÿå…¼å®¹æ€§** - åœ¨ç°æœ‰æ¶æ„ä¸­æœ€å°å…¥ä¾µå¼é›†æˆ  
âœ… **æ”¯æŒè¶…å¤§æ–‡ä»¶** - å¯å¤„ç†å‡ åƒè¡Œåˆ°å‡ ä¸‡è¡Œçš„æ–‡ä»¶  
âœ… **å¤šè¯­è¨€æ”¯æŒ** - é’ˆå¯¹ä¸åŒè¯­è¨€çš„ä¸“é—¨åˆ†ç‰‡ç­–ç•¥  
âœ… **æ™ºèƒ½å¤„ç†** - æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†ç‰‡ï¼Œä¿æŒä»£ç å®Œæ•´æ€§  
âœ… **æ€§èƒ½ä¼˜åŒ–** - å¹¶è¡Œå¤„ç†ã€ç¼“å­˜å’Œæµå¼è¯»å–  

è¯¥æ–¹æ¡ˆå°†å¤§å¹…æå‡CodeLenså¯¹å¤§å‹é¡¹ç›®å’Œå¤æ‚ä»£ç æ–‡ä»¶çš„å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒç°æœ‰åŠŸèƒ½çš„ç¨³å®šæ€§ã€‚