# ğŸš€ å¤§æ–‡ä»¶å¤„ç†è§£å†³æ–¹æ¡ˆ

## é—®é¢˜
å¤„ç†å¤§å‹ä»£ç æ–‡ä»¶ï¼ˆ5000+è¡Œï¼‰æ—¶é¢ä¸´çš„é—®é¢˜ï¼š
- å†…å­˜ä¸è¶³ï¼Œä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶å¯¼è‡´å´©æºƒ
- å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œç”¨æˆ·ä½“éªŒå·®
- ç”Ÿæˆæ–‡æ¡£è´¨é‡å·®ï¼Œç»“æ„æ··ä¹±

## è§£å†³æ–¹æ¡ˆ

### 1. æ™ºèƒ½åˆ†å—
**æ ¸å¿ƒæ€è·¯**: æŒ‰ä»£ç ç»“æ„æ™ºèƒ½åˆ‡åˆ†ï¼Œä¸ç ´åé€»è¾‘

```python
def smart_split_file(content: str, max_lines=1500):
    """æ™ºèƒ½åˆ†å—ï¼šæŒ‰ç±»å’Œå‡½æ•°è¾¹ç•Œåˆ‡åˆ†"""
    chunks = []
    lines = content.splitlines()
    current_chunk = []
    
    for i, line in enumerate(lines):
        current_chunk.append(line)
        
        # æ£€æµ‹åˆ†å—ç‚¹ï¼šç±»å®šä¹‰æˆ–å‡½æ•°å®šä¹‰
        if (line.strip().startswith(('class ', 'def ')) and 
            len(current_chunk) > 500):
            chunks.append('\n'.join(current_chunk[:-1]))
            current_chunk = [line]
        
        # å¼ºåˆ¶åˆ†å—ï¼šé˜²æ­¢å—è¿‡å¤§
        elif len(current_chunk) >= max_lines:
            chunks.append('\n'.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks
```

### 2. ä»»åŠ¡åˆ†è§£
**æ ¸å¿ƒæ€è·¯**: å°†å¤§æ–‡ä»¶åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œåˆ©ç”¨ç°æœ‰ä»»åŠ¡ç³»ç»Ÿ

```python
def create_large_file_tasks(file_path, project_path):
    """ä¸ºå¤§æ–‡ä»¶åˆ›å»ºå­ä»»åŠ¡"""
    
    # 1. è¯»å–æ–‡ä»¶å†…å®¹å¹¶åˆ†å—
    with open(file_path, 'r') as f:
        content = f.read()
    
    chunks = smart_split_file(content)
    
    # 2. ä¸ºæ¯ä¸ªå—åˆ›å»ºæ–‡ä»¶åˆ†æä»»åŠ¡
    chunk_tasks = []
    for i, chunk in enumerate(chunks):
        # ä¿å­˜å—å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
        chunk_file = f"{file_path}.chunk_{i}.tmp"
        with open(chunk_file, 'w') as f:
            f.write(chunk)
        
        # åˆ›å»ºä»»åŠ¡
        task = {
            'id': f"large_file_chunk_{hash(file_path)}_{i}",
            'type': 'file_summary',
            'status': 'pending',
            'file_path': chunk_file,
            'chunk_id': i,
            'total_chunks': len(chunks),
            'original_file': file_path
        }
        chunk_tasks.append(task)
    
    # 3. åˆ›å»ºåˆå¹¶ä»»åŠ¡
    merge_task = {
        'id': f"large_file_merge_{hash(file_path)}",
        'type': 'large_file_merge', 
        'status': 'pending',
        'depends_on': [task['id'] for task in chunk_tasks],
        'original_file': file_path,
        'chunk_tasks': [task['id'] for task in chunk_tasks]
    }
    
    return chunk_tasks + [merge_task]
```

### 3. é¡ºåºå¤„ç†
**æ ¸å¿ƒæ€è·¯**: ClaudeæŒ‰é¡ºåºå¤„ç†æ¯ä¸ªå—ï¼Œæœ€ååˆå¹¶ç»“æœ

```python
def process_large_file_chunk(chunk_task):
    """å¤„ç†å¤§æ–‡ä»¶çš„å•ä¸ªå—"""
    
    # è¯»å–å—å†…å®¹
    with open(chunk_task['file_path'], 'r') as f:
        chunk_content = f.read()
    
    # ç”Ÿæˆå—çš„æ–‡æ¡£
    doc_fragment = analyze_and_document_chunk(
        chunk_content, 
        chunk_task['chunk_id'],
        chunk_task['total_chunks'],
        chunk_task['original_file']
    )
    
    # ä¿å­˜å—æ–‡æ¡£
    chunk_doc_path = f"{chunk_task['original_file']}.chunk_{chunk_task['chunk_id']}.md"
    with open(chunk_doc_path, 'w') as f:
        f.write(doc_fragment)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(chunk_task['file_path'])
    
    return {
        'chunk_id': chunk_task['chunk_id'],
        'doc_path': chunk_doc_path,
        'lines_processed': len(chunk_content.splitlines())
    }

def merge_large_file_docs(merge_task):
    """åˆå¹¶å¤§æ–‡ä»¶çš„æ‰€æœ‰å—æ–‡æ¡£"""
    
    # æ”¶é›†æ‰€æœ‰å—çš„æ–‡æ¡£
    chunk_docs = []
    for chunk_task_id in merge_task['chunk_tasks']:
        # æ‰¾åˆ°å¯¹åº”çš„å—æ–‡æ¡£æ–‡ä»¶
        chunk_id = extract_chunk_id(chunk_task_id)
        chunk_doc_path = f"{merge_task['original_file']}.chunk_{chunk_id}.md"
        
        if os.path.exists(chunk_doc_path):
            with open(chunk_doc_path, 'r') as f:
                chunk_docs.append({
                    'id': chunk_id,
                    'content': f.read()
                })
    
    # æŒ‰é¡ºåºæ’åˆ—å¹¶åˆå¹¶
    chunk_docs.sort(key=lambda x: x['id'])
    
    final_doc = generate_file_header(merge_task['original_file'])
    final_doc += "\n## æ–‡ä»¶åˆ†æ\n\n"
    
    for doc in chunk_docs:
        final_doc += f"### ä»£ç å— {doc['id'] + 1}\n\n"
        final_doc += doc['content']
        final_doc += "\n\n"
    
    # ä¿å­˜æœ€ç»ˆæ–‡æ¡£
    final_doc_path = get_file_doc_path(merge_task['original_file'])
    with open(final_doc_path, 'w') as f:
        f.write(final_doc)
    
    # æ¸…ç†ä¸´æ—¶å—æ–‡æ¡£
    for doc in chunk_docs:
        chunk_doc_path = f"{merge_task['original_file']}.chunk_{doc['id']}.md"
        if os.path.exists(chunk_doc_path):
            os.remove(chunk_doc_path)
    
    return final_doc_path
```

## é›†æˆåˆ°ç°æœ‰ä»»åŠ¡ç³»ç»Ÿ

### ä¿®æ”¹ä»»åŠ¡åˆå§‹åŒ– (task_init.py)
æ£€æµ‹å¤§æ–‡ä»¶æ—¶è‡ªåŠ¨åˆ›å»ºå­ä»»åŠ¡ï¼š

```python
def handle_large_file_in_task_init(file_path, project_path):
    """åœ¨ä»»åŠ¡åˆå§‹åŒ–æ—¶å¤„ç†å¤§æ–‡ä»¶"""
    
    if is_large_file(file_path, threshold=5000):
        # åˆ›å»ºå¤§æ–‡ä»¶å­ä»»åŠ¡
        chunk_tasks = create_large_file_tasks(file_path, project_path) 
        
        # å°†å­ä»»åŠ¡æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
        for task in chunk_tasks:
            task_manager.add_task(task)
        
        print(f"å¤§æ–‡ä»¶ {file_path} å·²åˆ†è§£ä¸º {len(chunk_tasks)-1} ä¸ªå­ä»»åŠ¡")
        return True
    
    return False  # ä¸æ˜¯å¤§æ–‡ä»¶ï¼Œæ­£å¸¸å¤„ç†

def is_large_file(file_path, threshold=5000):
    """æ£€æµ‹æ˜¯å¦ä¸ºå¤§æ–‡ä»¶"""
    try:
        with open(file_path, 'r') as f:
            line_count = sum(1 for _ in f)
        return line_count > threshold
    except:
        return False
```

### ä¿®æ”¹ä»»åŠ¡æ‰§è¡Œ (task_execute.py)
æ·»åŠ å¤§æ–‡ä»¶ä»»åŠ¡ç±»å‹å¤„ç†ï¼š

```python
def execute_task_by_type(task):
    """æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä»»åŠ¡"""
    
    if task['type'] == 'file_summary':
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§æ–‡ä»¶å—ä»»åŠ¡
        if 'chunk_id' in task:
            return process_large_file_chunk(task)
        else:
            return process_normal_file_summary(task)
    
    elif task['type'] == 'large_file_merge':
        return merge_large_file_docs(task)
    
    # ... å…¶ä»–ä»»åŠ¡ç±»å‹
```

### ä¿®æ”¹æ–‡ä»¶æœåŠ¡ (file_service.py)
æ·»åŠ åˆ†å—å™¨åŠŸèƒ½ï¼š

```python
class FileService:
    def __init__(self):
        # ... ç°æœ‰ä»£ç 
        self.large_file_threshold = 5000
    
    def read_file_safe(self, file_path: str, max_size: int = 50000) -> Optional[str]:
        """å¢å¼ºçš„å®‰å…¨æ–‡ä»¶è¯»å–ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                return None

            # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œè¡Œæ•°
            file_size = file_path.stat().st_size
            if file_size > max_size:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§æ–‡ä»¶ï¼ˆæŒ‰è¡Œæ•°ï¼‰
                with open(file_path, 'r', encoding='utf-8') as f:
                    line_count = sum(1 for _ in f)
                
                if line_count > self.large_file_threshold:
                    print(f"æ£€æµ‹åˆ°å¤§æ–‡ä»¶ {file_path} ({line_count} è¡Œ)ï¼Œå»ºè®®ä½¿ç”¨å¤§æ–‡ä»¶å¤„ç†æµç¨‹")
                    return "LARGE_FILE_DETECTED"  # ç‰¹æ®Šæ ‡è®°
                else:
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} è¿‡å¤§ ({file_size} å­—èŠ‚)ï¼Œè·³è¿‡")
                    return None

            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶é”™è¯¯ {file_path}: {e}")
            return None
```

## å®é™…å¤„ç†æµç¨‹

**æ­£å¸¸æ–‡ä»¶ (<5000è¡Œ)**:
```
æ–‡ä»¶æ‰«æ â†’ ç›´æ¥å¤„ç† â†’ ç”Ÿæˆæ–‡æ¡£
```

**å¤§æ–‡ä»¶ (â‰¥5000è¡Œ)**:
```
æ–‡ä»¶æ‰«æ â†’ æ£€æµ‹å¤§æ–‡ä»¶ â†’ æ™ºèƒ½åˆ†å— â†’ åˆ›å»ºå­ä»»åŠ¡
    â†“
å—ä»»åŠ¡1 â†’ Claudeå¤„ç† â†’ å—æ–‡æ¡£1
å—ä»»åŠ¡2 â†’ Claudeå¤„ç† â†’ å—æ–‡æ¡£2  
å—ä»»åŠ¡N â†’ Claudeå¤„ç† â†’ å—æ–‡æ¡£N
    â†“
åˆå¹¶ä»»åŠ¡ â†’ æ•´åˆæ‰€æœ‰å— â†’ æœ€ç»ˆæ–‡æ¡£
```

## å…³é”®ä¼˜åŠ¿

1. **å†…å­˜å®‰å…¨**: åˆ†å—å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
2. **ä»»åŠ¡å…¼å®¹**: å®Œå…¨èå…¥ç°æœ‰ä»»åŠ¡ç³»ç»Ÿ  
3. **ç»“æ„å®Œæ•´**: æ™ºèƒ½åˆ†å—ï¼Œä¿æŒä»£ç é€»è¾‘
4. **æ¸è¿›åé¦ˆ**: æŒ‰å—å¤„ç†ï¼Œå®æ—¶æ˜¾ç¤ºè¿›åº¦

## é€‚é…ç°æœ‰ç³»ç»Ÿ

**æ— éœ€ä¿®æ”¹**:
- æ¨¡æ¿ç³»ç»Ÿç»§ç»­ä½¿ç”¨
- MCPæ¥å£ä¿æŒä¸å˜
- ä»»åŠ¡çŠ¶æ€ç®¡ç†ä¸å˜

**æœ€å°ä¿®æ”¹**:
- `task_init.py`: æ·»åŠ å¤§æ–‡ä»¶æ£€æµ‹é€»è¾‘
- `task_execute.py`: å¢åŠ ä¸¤ç§æ–°ä»»åŠ¡ç±»å‹å¤„ç†
- `file_service.py`: å¢å¼ºæ–‡ä»¶è¯»å–åŠŸèƒ½

## ä¸‹ä¸€æ­¥å®æ–½

1. **ç¬¬ä¸€æ­¥**: åœ¨ `file_service.py` æ·»åŠ  `is_large_file()` æ£€æµ‹
2. **ç¬¬äºŒæ­¥**: åœ¨ `task_init.py` æ·»åŠ å¤§æ–‡ä»¶ä»»åŠ¡åˆ†è§£é€»è¾‘  
3. **ç¬¬ä¸‰æ­¥**: åœ¨ `task_execute.py` æ·»åŠ å—ä»»åŠ¡å’Œåˆå¹¶ä»»åŠ¡å¤„ç†
4. **ç¬¬å››æ­¥**: æµ‹è¯•å¹¶ä¼˜åŒ–åˆ†å—å¤§å°å’Œå¤„ç†é€»è¾‘