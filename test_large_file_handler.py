#!/usr/bin/env python3
"""
æµ‹è¯•å¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†ç³»ç»Ÿ
éªŒè¯ LargeFileHandler å’Œç›¸å…³é›†æˆçš„åŠŸèƒ½
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def create_test_large_file(file_path: str, size_kb: int = 60) -> str:
    """åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„å¤§Pythonæ–‡ä»¶"""
    content = f"""'''
æµ‹è¯•å¤§æ–‡ä»¶ - è‡ªåŠ¨ç”Ÿæˆç”¨äºæµ‹è¯•åˆ†ç‰‡ç³»ç»Ÿ
æ–‡ä»¶å¤§å°çº¦ {size_kb}KB
'''

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

# å…¨å±€å˜é‡
GLOBAL_CONFIG = {{
    "debug": True,
    "version": "1.0.0",
    "max_retries": 3
}}

class DataProcessor:
    \"\"\"æ•°æ®å¤„ç†å™¨ç±» - ç”¨äºæµ‹è¯•ç±»åˆ†ç‰‡\"\"\"
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.processed_count = 0
        self.error_count = 0
    
    def process_data(self, data: List[Dict]) -> List[Dict]:
        \"\"\"å¤„ç†æ•°æ®åˆ—è¡¨\"\"\"
        results = []
        
        for item in data:
            try:
                processed_item = self._process_single_item(item)
                results.append(processed_item)
                self.processed_count += 1
            except Exception as e:
                self.error_count += 1
                print(f"Error processing item: {{e}}")
        
        return results
    
    def _process_single_item(self, item: Dict) -> Dict:
        \"\"\"å¤„ç†å•ä¸ªæ•°æ®é¡¹\"\"\"
        # æ¨¡æ‹Ÿå¤æ‚çš„å¤„ç†é€»è¾‘
        processed = item.copy()
        
        # æ·»åŠ æ—¶é—´æˆ³
        processed['processed_at'] = time.time()
        
        # æ•°æ®éªŒè¯
        if not self._validate_item(processed):
            raise ValueError("Invalid item data")
        
        # æ•°æ®è½¬æ¢
        processed = self._transform_item(processed)
        
        return processed
    
    def _validate_item(self, item: Dict) -> bool:
        \"\"\"éªŒè¯æ•°æ®é¡¹\"\"\"
        required_fields = ['id', 'name', 'type']
        
        for field in required_fields:
            if field not in item:
                return False
        
        return True
    
    def _transform_item(self, item: Dict) -> Dict:
        \"\"\"è½¬æ¢æ•°æ®é¡¹\"\"\"
        # æ¨¡æ‹Ÿå¤æ‚çš„è½¬æ¢é€»è¾‘
        transformed = item.copy()
        
        # æ ‡å‡†åŒ–åç§°
        if 'name' in transformed:
            transformed['name'] = transformed['name'].strip().lower()
        
        # æ·»åŠ è®¡ç®—å­—æ®µ
        transformed['computed_hash'] = hash(str(transformed))
        
        # æ·»åŠ å…ƒæ•°æ®
        transformed['metadata'] = {{
            'processor_version': self.config.get('version', '1.0.0'),
            'processing_time': time.time()
        }}
        
        return transformed

    def get_statistics(self) -> Dict[str, int]:
        \"\"\"è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯\"\"\"
        return {{
            'processed_count': self.processed_count,
            'error_count': self.error_count,
            'total_count': self.processed_count + self.error_count
        }}

    def reset_statistics(self):
        \"\"\"é‡ç½®ç»Ÿè®¡ä¿¡æ¯\"\"\"
        self.processed_count = 0
        self.error_count = 0


class AdvancedDataProcessor(DataProcessor):
    \"\"\"é«˜çº§æ•°æ®å¤„ç†å™¨ - ç»§æ‰¿æµ‹è¯•\"\"\"
    
    def __init__(self, config: Dict[str, Any], advanced_options: Dict[str, Any] = None):
        super().__init__(config)
        self.advanced_options = advanced_options or {{}}
        self.cache = {{}}
    
    def process_data_with_cache(self, data: List[Dict]) -> List[Dict]:
        \"\"\"å¸¦ç¼“å­˜çš„æ•°æ®å¤„ç†\"\"\"
        results = []
        
        for item in data:
            item_key = self._generate_cache_key(item)
            
            if item_key in self.cache:
                # ä»ç¼“å­˜è·å–
                cached_result = self.cache[item_key]
                results.append(cached_result)
            else:
                # å¤„ç†å¹¶ç¼“å­˜
                processed_item = self._process_single_item(item)
                self.cache[item_key] = processed_item
                results.append(processed_item)
        
        return results
    
    def _generate_cache_key(self, item: Dict) -> str:
        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"
        key_fields = ['id', 'name', 'type']
        key_values = []
        
        for field in key_fields:
            if field in item:
                key_values.append(str(item[field]))
        
        return "_".join(key_values)
    
    def clear_cache(self):
        \"\"\"æ¸…ç©ºç¼“å­˜\"\"\"
        self.cache.clear()
    
    def get_cache_size(self) -> int:
        \"\"\"è·å–ç¼“å­˜å¤§å°\"\"\"
        return len(self.cache)


def create_sample_data(count: int = 100) -> List[Dict]:
    \"\"\"åˆ›å»ºç¤ºä¾‹æ•°æ®\"\"\"
    data = []
    
    for i in range(count):
        item = {{
            'id': f"item_{{i:05d}}",
            'name': f"Sample Item {{i}}",
            'type': 'test_data',
            'value': i * 2,
            'category': f"category_{{i % 5}}",
            'tags': [f"tag_{{j}}" for j in range(i % 3)],
            'metadata': {{
                'created_at': time.time() - (i * 3600),
                'priority': i % 10,
                'status': 'active' if i % 2 == 0 else 'inactive'
            }}
        }}
        data.append(item)
    
    return data


def process_batch_data(processor: DataProcessor, batch_size: int = 50):
    \"\"\"æ‰¹é‡å¤„ç†æ•°æ®\"\"\"
    all_data = create_sample_data(200)
    
    print(f"å¤„ç† {{len(all_data)}} æ¡æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {{batch_size}}")
    
    all_results = []
    for i in range(0, len(all_data), batch_size):
        batch = all_data[i:i + batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {{i // batch_size + 1}}: {{len(batch)}} é¡¹")
        
        batch_results = processor.process_data(batch)
        all_results.extend(batch_results)
    
    return all_results


def run_performance_test():
    \"\"\"è¿è¡Œæ€§èƒ½æµ‹è¯•\"\"\"
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    
    config = GLOBAL_CONFIG.copy()
    config.update({{"performance_mode": True}})
    
    # åŸºæœ¬å¤„ç†å™¨æµ‹è¯•
    basic_processor = DataProcessor(config)
    start_time = time.time()
    basic_results = process_batch_data(basic_processor)
    basic_time = time.time() - start_time
    
    print(f"åŸºæœ¬å¤„ç†å™¨ï¼šå¤„ç† {{len(basic_results)}} é¡¹ï¼Œè€—æ—¶ {{basic_time:.2f}} ç§’")
    print(f"åŸºæœ¬å¤„ç†å™¨ç»Ÿè®¡ï¼š{{basic_processor.get_statistics()}}")
    
    # é«˜çº§å¤„ç†å™¨æµ‹è¯•
    advanced_processor = AdvancedDataProcessor(config, {{"cache_enabled": True}})
    start_time = time.time()
    advanced_results = process_batch_data(advanced_processor)
    advanced_time = time.time() - start_time
    
    print(f"é«˜çº§å¤„ç†å™¨ï¼šå¤„ç† {{len(advanced_results)}} é¡¹ï¼Œè€—æ—¶ {{advanced_time:.2f}} ç§’")
    print(f"é«˜çº§å¤„ç†å™¨ç»Ÿè®¡ï¼š{{advanced_processor.get_statistics()}}")
    print(f"ç¼“å­˜å¤§å°ï¼š{{advanced_processor.get_cache_size()}}")
    
    # ç¼“å­˜æµ‹è¯•
    start_time = time.time()
    cached_results = process_batch_data(advanced_processor)
    cached_time = time.time() - start_time
    
    print(f"ç¼“å­˜å¤„ç†å™¨ï¼šå¤„ç† {{len(cached_results)}} é¡¹ï¼Œè€—æ—¶ {{cached_time:.2f}} ç§’")
    
    return {{
        'basic_time': basic_time,
        'advanced_time': advanced_time,
        'cached_time': cached_time,
        'results_count': len(basic_results)
    }}


if __name__ == "__main__":
    print("æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†ç³»ç»Ÿ")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    results = run_performance_test()
    
    print("\\næµ‹è¯•å®Œæˆï¼")
    print(f"æ€§èƒ½ç»“æœï¼š{{results}}")
"""
    
    # æ‰©å±•å†…å®¹ä»¥è¾¾åˆ°æŒ‡å®šå¤§å°
    while len(content) < size_kb * 1024:
        content += f"""

# é¢å¤–å†…å®¹ - è¡Œ {len(content.split())}
def additional_function_{len(content.split())}():
    \"\"\"é¢å¤–çš„å‡½æ•°ç”¨äºå¢åŠ æ–‡ä»¶å¤§å°\"\"\"
    data = [{{
        'index': {len(content.split())},
        'timestamp': time.time(),
        'data': 'x' * 100
    }}]
    return data
"""
    
    # å†™å…¥æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return content


def test_large_file_chunking():
    """æµ‹è¯•å¤§æ–‡ä»¶åˆ†ç‰‡åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•å¤§æ–‡ä»¶åˆ†ç‰‡åŠŸèƒ½")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from src.services.large_file_handler import LargeFileHandler
        from src.services.file_service import FileService
        
        print("âœ… æˆåŠŸå¯¼å…¥å¤§æ–‡ä»¶å¤„ç†æ¨¡å—")
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = "test_large_file.py"
        content = create_test_large_file(test_file, 70)  # 70KB æ–‡ä»¶
        file_size = len(content.encode('utf-8'))
        
        print(f"âœ… åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {test_file} ({file_size} å­—èŠ‚, {file_size/1024:.1f} KB)")
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        handler = LargeFileHandler()
        print(f"âœ… åˆå§‹åŒ– LargeFileHandlerï¼Œæ”¯æŒçš„è¯­è¨€: {list(handler.chunkers.keys())}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦åˆ†ç‰‡
        should_chunk = handler.should_chunk_file(test_file, 50000)  # 50KB é˜ˆå€¼
        print(f"âœ… æ–‡ä»¶æ˜¯å¦éœ€è¦åˆ†ç‰‡: {should_chunk}")
        
        if should_chunk:
            # æ‰§è¡Œåˆ†ç‰‡å¤„ç†
            print("ğŸš€ å¼€å§‹åˆ†ç‰‡å¤„ç†...")
            result = handler.process_large_file(test_file, content)
            
            if result.success:
                print(f"âœ… åˆ†ç‰‡å¤„ç†æˆåŠŸ!")
                print(f"   - æ€»åˆ†ç‰‡æ•°: {result.total_chunks}")
                print(f"   - å¤„ç†æ–¹æ³•: {result.processing_method}")
                print(f"   - å¤„ç†æ—¶é—´: {result.processing_time:.2f} ç§’")
                print(f"   - æ€»å¤§å°: {result.total_size} å­—èŠ‚")
                
                # æ˜¾ç¤ºåˆ†ç‰‡è¯¦æƒ…
                print("\nğŸ“Š åˆ†ç‰‡è¯¦æƒ…:")
                for i, chunk in enumerate(result.chunks[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"   åˆ†ç‰‡ {i+1}: {chunk.chunk_type.value} "
                          f"({chunk.start_line}-{chunk.end_line}è¡Œ, {chunk.size_bytes}å­—èŠ‚)")
                
                if len(result.chunks) > 5:
                    print(f"   ... è¿˜æœ‰ {len(result.chunks) - 5} ä¸ªåˆ†ç‰‡")
                
                # æµ‹è¯• FileService é›†æˆ
                print("\nğŸ”§ æµ‹è¯• FileService é›†æˆ...")
                file_service = FileService(enable_large_file_chunking=True)
                
                processing_info = file_service.get_file_processing_info(test_file, 50000)
                print(f"âœ… æ–‡ä»¶å¤„ç†ä¿¡æ¯: {processing_info}")
                
                # æµ‹è¯•æ–‡ä»¶è¯»å–
                read_result = file_service.read_file_with_chunking(test_file, 50000)
                if isinstance(read_result, type(result)) and read_result.success:
                    print(f"âœ… FileService åˆ†ç‰‡è¯»å–æˆåŠŸ: {read_result.total_chunks} ä¸ªåˆ†ç‰‡")
                else:
                    print(f"âŒ FileService åˆ†ç‰‡è¯»å–å¤±è´¥")
                
            else:
                print(f"âŒ åˆ†ç‰‡å¤„ç†å¤±è´¥: {result.errors}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(test_file):
            os.remove(test_file)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_file}")
        
        print("\nâœ… å¤§æ–‡ä»¶åˆ†ç‰‡æµ‹è¯•å®Œæˆ!")
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_task_executor_integration():
    """æµ‹è¯• TaskExecutor é›†æˆ"""
    print("\nğŸ”§ æµ‹è¯• TaskExecutor é›†æˆ")
    
    try:
        from src.mcp_tools.task_execute import TaskExecutor
        
        # åˆ›å»ºæµ‹è¯•é¡¹ç›®ç›®å½•
        test_project = Path("test_project")
        test_project.mkdir(exist_ok=True)
        
        # åˆ›å»ºå¤§æ–‡ä»¶
        test_file = test_project / "large_module.py"
        content = create_test_large_file(str(test_file), 60)
        
        print(f"âœ… åˆ›å»ºæµ‹è¯•é¡¹ç›®: {test_project}")
        print(f"âœ… åˆ›å»ºå¤§æ–‡ä»¶: {test_file} ({len(content)} å­—ç¬¦)")
        
        # åˆå§‹åŒ– TaskExecutor
        executor = TaskExecutor(str(test_project))
        
        # æ£€æŸ¥åˆ†ç‰‡åŠŸèƒ½æ˜¯å¦å¯ç”¨
        chunking_stats = executor.get_chunking_stats()
        print(f"âœ… TaskExecutor åˆ†ç‰‡ç»Ÿè®¡: {chunking_stats}")
        
        if chunking_stats.get('chunking_enabled'):
            print("âœ… TaskExecutor å¤§æ–‡ä»¶åˆ†ç‰‡åŠŸèƒ½å·²å¯ç”¨")
        else:
            print("âš ï¸ TaskExecutor å¤§æ–‡ä»¶åˆ†ç‰‡åŠŸèƒ½æœªå¯ç”¨")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        if test_project.exists():
            shutil.rmtree(test_project)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•é¡¹ç›®: {test_project}")
        
        print("âœ… TaskExecutor é›†æˆæµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ TaskExecutor é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ CodeLens å¤§æ–‡ä»¶å¤„ç†ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1: æ ¸å¿ƒåˆ†ç‰‡åŠŸèƒ½
    test1_result = test_large_file_chunking()
    
    # æµ‹è¯•2: TaskExecutor é›†æˆ
    test2_result = test_task_executor_integration()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"   - å¤§æ–‡ä»¶åˆ†ç‰‡åŠŸèƒ½: {'âœ… é€šè¿‡' if test1_result else 'âŒ å¤±è´¥'}")
    print(f"   - TaskExecutor é›†æˆ: {'âœ… é€šè¿‡' if test2_result else 'âŒ å¤±è´¥'}")
    
    if test1_result and test2_result:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å¤§æ–‡ä»¶å¤„ç†ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())